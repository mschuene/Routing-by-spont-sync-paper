#+TITLE:Routing by spontaneous synchrony 
#+AUTHOR: Maik Sch√ºnemann
#+email: maikschuenemann@gmail.com
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  de
#+OPTIONS:   H:3 num:t toc:t :nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME:
#+TAGS:  BlowerDoor(b) Suub(s) Uni(u) Home(h) Task(t) Note(n) Info(i) drill(d)
#+TAGS: Changed(c) Project(p) Reading(r) Hobby(f) OpenSource(o) Meta(m)
#+SEQ_TODO: TODO(t) STARTED(s) WAITING(w) APPT(a) NEXT(n) | DONE(d) CANCELLED(c) DEFERRED(f) 
#+STARTUP:showall
#+LaTeX_CLASS: scrartcl



# tangle all files in the beginning and afterwards evaluate this snippet to load
# all neccesary predifined functions
    #+begin_src python :session rbs :results raw drawer 
    %matplotlib inline
    from utils.spectral import * 
    from utils.model import *
    from ana.analytical import *
    #+end_src 

    #+RESULTS:


* Introduction


* Methods 
** Utility functions for spectral analysis

   #+begin_src python :session rbs :results raw drawer :tangle utils/spectral.py
     import matplotlib.pyplot as plt
     import numpy as np
     import scipy.signal as signal

     def wavelet_morlet(sig,dt,freqs,w=6.0,return_coi=True):
         """sig is 1d array, dt step size in seconds, freqs in Hz
            returns complex wavelet coefficients."""
         sig = (sig-sig.mean())/sig.std()
         fs = 1/dt
         widths = w*fs/(2*np.pi*freqs)
         a = signal.cwt(sig,signal.morlet2,widths,w=w)
         if return_coi:
             coi = np.sqrt(2)*widths*dt
             return a,coi
         return a


     def plot_wavelet_with_coi(a,coi,t,freqs,ax=None,**pcm_kwargs):
         if ax is None:
             plt.figure()
             ax = plt.gca()

         im = ax.pcolormesh(t, freqs, np.abs(a), cmap='viridis', shading='gouraud',**pcm_kwargs)
         coil = np.where(coi>t[-1],t[-1],coi)
         coir = t[-1]-coi
         coir = np.where(coir<0,0,coir)
         ax.plot(coil,freqs,'k--')
         ax.plot(coir,freqs,'k--')
         ax.set_xlabel('time [s]')
         ax.set_ylabel('frequency [Hz]')
         return im


     def spectral_coherence(a1,a2,tau_max,ntau,dt,ts=None,te=None):
         """a1 and a2 are (trials,freqs,time) arrays, ts and te time start/end indices
            tau_max is the maximal time delay (dimension time)
            returns C of shape (freqs,taus) where taus is of lenght 2*ntau+1 linearly from -tau_max to +tau_max
             """
         ntrails,nfreqs,ntime = a1.shape
         assert(a2.shape==a1.shape)
         if ts is None:
             ts = 0
         if te is None:
             te = ntime
         #tau_max = 0.6
         #ntau = int(tau_max/dt)
         taus = np.linspace(-tau_max,tau_max,2*ntau+1)
         # for single trial
         c = np.zeros((nfreqs,len(taus)))

         #helper for sum of complex dot products over all trials
         def trial_dot_sum(p,q):
             return np.sum([p[i,:]@np.conj(q[i,:]) for i in range(p.shape[0])])

         for i in range(nfreqs):
             # zero time delay at index ntau
             c[i,ntau] = (np.abs(trial_dot_sum(a1[:,i,ts:te],a2[:,i,ts:te]))**2/
                          (np.sum(np.abs(a1[:,i,ts:te])**2)*np.sum(np.abs(a2[:,i,ts:te])**2)))
             for itau in range(1,ntau+1):
                 tau = taus[ntau+itau] # absolute tau value
                 taui = int(tau/dt) # index shift by tau
                 # shift by +tau
                 c[i,ntau+itau] = (np.abs(trial_dot_sum(a1[:,i,(ts+taui):te],a2[:,i,ts:(te-taui)]))**2/
                                   (np.sum(np.abs(a1[:,i,(ts+taui):te])**2)*np.sum(np.abs(a2[:,i,ts:(te-taui)])**2)))
                 # shift by -tau -> switch t boundaries
                 c[i,ntau-itau] = (np.abs(trial_dot_sum(a1[:,i,ts:(te-taui)],a2[:,i,(ts+taui):te]))**2/
                                   (np.sum(np.abs(a1[:,i,ts:(te-taui)])**2)*np.sum(np.abs(a2[:,i,(ts+taui):te])**2)))            
         # nonlinear normalization    
         #C = 1/(1+np.sqrt(1/c - 1))
         return c,taus

   #+end_src 

   #+RESULTS:

** EHE Model simulation

*** General Weight matrix W
    
    #+begin_src python :session rbs :results raw drawer :tangle utils/model.py
      import itertools 
      import numba
      import pickle

      @numba.njit(cache=True)
      def simulate_model(N,W,p,deltaU,num_steps=None,num_av=None,u0=None):
          u = np.random.random((N,)) if u0 is None else u0
          avc = 0
          step = 0
          neurons = np.arange(N)
          IW = np.eye(N)-W
          avs = []
          avt = []
          pc = np.cumsum(p)
          avu = []
          ks = []
          while avc < num_av if num_av is not None else step < num_steps:
              k = np.searchsorted(pc,np.random.random())
              ks.append(k)
              u[k] += deltaU
              if u[k] > 1:
                  avc+= 1
                  avsize = 0
                  A = u > 1;
                  avinc = np.sum(A)
                  avt.append(step)
                  avunits = []
                  while avinc > 0:
                      avsize += avinc
                      avunits.extend(np.where(A)[0])
                      u = u-IW@A.astype(np.float64)
                      A = u>1
                      avinc = np.sum(A)
                  avs.append(avsize)
                  avu.append(avunits)
              step += 1

          return np.array(avs),np.array(avt),avu,u,ks



    #+end_src 

    #+RESULTS:



*** Block matrix W for subnet simulation
   #+begin_src python :session rbs :results raw drawer :tangle utils/model.py
     import itertools
     import numpy as np
     import numba

     @numba.njit(cache=True)
     def get_avinc(A,Ns,nNs):
         pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
         avinc = np.zeros(len(Ns),dtype=np.int64)
         for k in range(len(Ns)):
             avinc[k]+= np.sum(A[pop_ids(k)])
         return avinc


     @numba.njit(cache=True)
     def simulate_model_subnets(Ns,W,p,deltaU,num_steps=None,num_av=None,u0=None,outdir=None):
         Ns = np.array(Ns,dtype=np.int64)
         N = np.sum(Ns)
         u = np.random.random((N,)) if u0 is None else u0
         avc = 0
         step = 0
         neurons = np.arange(N)
         I = np.eye(len(Ns))
         avs = []
         avt = []
         #pc = np.cumsum(np.concatenate(tuple([np.array([ps/n]*n) for ps,n in zip(p,Ns)])))
         avu = []
         ks = []
         cNs = np.cumsum(Ns)
         nNs = np.concatenate((np.array([0]),cNs))
         pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
         pbig = np.zeros(N)
         for k in range(len(Ns)):
             pbig[pop_ids(k)] = p[k]/Ns[k]
         pc = np.cumsum(pbig)
         #pc = np.cumsum(np.concatenate([[ps/n]*n for ps,n in zip(p,Ns)]))

         while avc < num_av if num_av is not None else step < num_steps:
             k = np.searchsorted(pc,np.random.random())
             ks.append(np.searchsorted(cNs,k+1))
             u[k] += deltaU*(0.5+k/Ns[0])
             if u[k] > 1:
                 avc+= 1
                 avsize = np.zeros(len(Ns),dtype=np.int64)
                 A = u > 1;
                 avinc = get_avinc(A,Ns,nNs) # vector len(Ns)
                 avt.append(step)
                 avunits = []
                 while np.sum(avinc) > 0:
                     avsize += avinc
                     u[A] -= 1
                     rec_int = W@avinc.astype(np.float64)
                     for i in range(len(Ns)):
                         u[pop_ids(i)]+=rec_int[i]/Ns[i]
                     A = u>1
                     avinc = get_avinc(A,Ns,nNs)
                 avs.append(avsize)
                 #avu.append(avunits)
             step += 1
         return avs,np.array(avt),u,ks


     def bin_data(data,bins):
         slices = np.linspace(0, len(data), bins+1, True).astype(np.int)
         counts = np.diff(slices)
         mean = np.add.reduceat(data, slices[:-1]) / counts
         return mean
   #+end_src 

   #+RESULTS:


*** threshold connections to V4 

    #+begin_src python :session rbs :results raw drawer :tangle utils/model.py
      @numba.njit(cache=True)
      def simulate_model_subnets_nonorm_thresholds(Ns,W,p,deltaU,thresh=0,num_steps=None,num_av=None,u0=None,outdir=None):
          th_idx = 4 #TODO Hardcoded index!!!!
          Ns = np.array(Ns,dtype=np.int64)
          N = np.sum(Ns)
          u = np.random.random((N,)) if u0 is None else u0
          avc = 0
          step = 0
          neurons = np.arange(N)
          I = np.eye(len(Ns))
          avs = []
          avt = []
          #pc = np.cumsum(np.concatenate(tuple([np.array([ps/n]*n) for ps,n in zip(p,Ns)])))
          avu = []
          ks = []
          cNs = np.cumsum(Ns)
          nNs = np.concatenate((np.array([0]),cNs))
          pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
          pbig = np.zeros(N)
          for k in range(len(Ns)):
              pbig[pop_ids(k)] = p[k]/Ns[k]
          pc = np.cumsum(pbig)
          u_saved = u.copy()
          #pc = np.cumsum(np.concatenate([[ps/n]*n for ps,n in zip(p,Ns)]))

          while avc < num_av if num_av is not None else step < num_steps:
              k = np.searchsorted(pc,np.random.random())
              k_sub = np.searchsorted(cNs,k+1)
              ks.append(k_sub)
              u_saved = u.copy()
              u[k] += deltaU
              if u[k] > 1:
                  avc+= 1
                  avsize = np.zeros(len(Ns),dtype=np.int64)
                  A = u > 1;
                  avinc = get_avinc(A,Ns,nNs) # vector len(Ns)
                  avt.append(step)
                  avunits = []
                  while np.sum(avinc) > 0:
                      avsize += avinc
                      u[A] -= 1
                      rec_int = W@avinc.astype(np.float64)
                      for i in range(len(Ns)):
                          u[pop_ids(i)]+=rec_int[i]#/Ns[i]
                      A = u>1
                      avinc = get_avinc(A,Ns,nNs)
                  if (k_sub != th_idx) and  (avsize[0]+avsize[2]<thresh): # TODO Hardcoded indices!!!!
                      u[pop_ids(th_idx)] = u_saved[pop_ids(th_idx)]
                      avsize[th_idx] = 0 
                  avs.append(avsize)
                  #avu.append(avunits)
              step += 1
          return avs,np.array(avt),u,ks
    #+end_src 

* Results 

** Flicker representation in V1  
*** Analytical representation in V1   
    #+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py
      import mpmath as m   
      import numpy as np
      def ehe_ana(alpha,N):
          ava = [m.exp(m.log((1/N))+m.log(m.binomial(N,n))+m.log(n*alpha/N)*(n-1)+
                             m.log(1-(n*alpha/N))*(N-n-1)+m.log(((1-alpha)/(1-((N-1)/N)*alpha))))
                  for n in range(1,N+1)]
          if alpha==0:
              ava[0] = 1
          return ava


      nf = 5
      d_levels = np.linspace(-1,1,nf)

      def flicker(levels=None):
          levels = d_levels if levels is None else levels
          return levels[np.random.randint(0,len(levels))]


      def ehe_flicker_V4corrs_flicker_levels_V1_ana(n,alpha,DeltaU,n_levels,c):
          base_p = 0.5
          levels = np.linspace(-1,1,n_levels)
          cfAA = 0#np.zeros((n_levels,n_levels))
          cfBB = 0#np.zeros_like(cfAA)
          EfAA = 0
          EfBB = 0
          EA = 0
          EAA = 0
          EB = 0
          EBB = 0
          EfA = 0
          EfAfA = 0
          EfB = 0
          EfBfB = 0
          for l in levels:
              p = base_p*(1+c*l)
              # Schalte den Flicker mit dazu:
              # n_levels = 5
              # iid realizations
              # gar nicht so einfach
              pavs_a = ehe_ana(alpha,n)
              p_s = p*DeltaU*(1-(n-1)*alpha/n)/(1-alpha)
              # fA ist bernoulli mit success probability pext[0]
              # same for fB. correlations between them are not important for the flicker to v1 correlations!
              # Ich brauche die folgenden Paare
              EfAA += l*p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])/n_levels**2
              EA += p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])/n_levels**2
              EAA += p_s*np.sum([(i+1)**2*p for i,p in enumerate(pavs_a)])/n_levels**2
              EfA += l/n_levels**2
              EfAfA += l**2/n_levels**2

          cfAA = (EfAA-EfA*EA)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(EAA-EA**2))
          return cfAA



      def binned_ehe_flicker_corrs_flicker_levels_V1_ana(n,alpha,DeltaU,n_levels,c,num_binned):
          base_p = 0.5
          levels = np.linspace(-1,1,n_levels)
          cfAA = 0#np.zeros((n_levels,n_levels))
          cfBB = 0#np.zeros_like(cfAA)
          EfAA = 0
          EfBB = 0
          EA = 0
          EAA = 0
          EB = 0
          EBB = 0
          EfA = 0
          EfAfA = 0
          EfB = 0
          EfBfB = 0
          for l in levels:
              p = base_p*(1+c*l)
              # Schalte den Flicker mit dazu:
              # n_levels = 5
              # iid realizations
              # gar nicht so einfach
              pavs_a = ehe_ana(alpha,n)
              p_s = p*DeltaU*(1-(n-1)*alpha/n)/(1-alpha)
              # fA ist bernoulli mit success probability pext[0]
              # same for fB. correlations between them are not important for the flicker to v1 correlations!
              # Ich brauche die folgenden Paare
              EfAA += l*p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])
              EAn = p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])
              EA += EAn
              EAAn = p_s*np.sum([(i+1)**2*p for i,p in enumerate(pavs_a)])
              VarAn = EAAn-EAn**2
              VarA = VarAn/num_binned
              EAAnew = EAn**2+VarA
              EAA += EAAnew
              EfA += l
              EfAfA += l**2
          EfAA = EfAA/n_levels
          EA = EA / n_levels
          EAA = EAA/n_levels
          EfA = EfA/n_levels
          EfAfA = EfAfA/n_levels
          print(EAA-EA**2)
          cfAA = (EfAA-EfA*EA)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(EAA-EA**2))
          return cfAA,cfAA, cfBB, EfAA, EfBB, EA, EAA, EB, EBB, EfA, EfAfA, EfB, EfBfB 
    #+end_src 

    #+RESULTS:

  
#+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py
      import mpmath as m   
      import numpy as np
      def ehe_ana(alpha,N):
          ava = [m.exp(m.log((1/N))+m.log(m.binomial(N,n))+m.log(n*alpha/N)*(n-1)+
                             m.log(1-(n*alpha/N))*(N-n-1)+m.log(((1-alpha)/(1-((N-1)/N)*alpha))))
                  for n in range(1,N+1)]
          if alpha==0:
              ava[0] = 1
          return ava


      nf = 5
      d_levels = np.linspace(-1,1,nf)

      def flicker(levels=None):
          levels = d_levels if levels is None else levels
          return levels[np.random.randint(0,len(levels))]


      def ehe_flicker_V4corrs_flicker_levels_V1_ana(n,alpha,DeltaU,n_levels,c):
          base_p = 0.5
          levels = np.linspace(-1,1,n_levels)
          cfAA = 0#np.zeros((n_levels,n_levels))
          cfBB = 0#np.zeros_like(cfAA)
          EfAA = 0
          EfBB = 0
          EA = 0
          EAA = 0
          EB = 0
          EBB = 0
          EfA = 0
          EfAfA = 0
          EfB = 0
          EfBfB = 0
          for l in levels:
              p = base_p*(1+c*l)
              # Schalte den Flicker mit dazu:
              # n_levels = 5
              # iid realizations
              # gar nicht so einfach
              pavs_a = ehe_ana(alpha,n)
              p_s = p*DeltaU*(1-(n-1)*alpha/n)/(1-alpha)
              # fA ist bernoulli mit success probability pext[0]
              # same for fB. correlations between them are not important for the flicker to v1 correlations!
              # Ich brauche die folgenden Paare
              EfAA += l*p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])/n_levels**2
              EA += p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])/n_levels**2
              EAA += p_s*np.sum([(i+1)**2*p for i,p in enumerate(pavs_a)])/n_levels**2
              EfA += l/n_levels**2
              EfAfA += l**2/n_levels**2

          cfAA = (EfAA-EfA*EA)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(EAA-EA**2))
          return cfAA



      def binned_ehe_flicker_corrs_flicker_levels_V1_ana(n,alpha,DeltaU,n_levels,c,num_binned):
          base_p = 0.5
          levels = np.linspace(-1,1,n_levels)
          cfAA = 0#np.zeros((n_levels,n_levels))
          cfBB = 0#np.zeros_like(cfAA)
          EfAA = 0
          EfBB = 0
          EA = 0
          EAA = 0
          EB = 0
          EBB = 0
          EfA = 0
          EfAfA = 0
          EfB = 0
          EfBfB = 0
          for l in levels:
              p = base_p*(1+c*l)
              # Schalte den Flicker mit dazu:
              # n_levels = 5
              # iid realizations
              # gar nicht so einfach
              pavs_a = ehe_ana(alpha,n)
              p_s = p*DeltaU*(1-(n-1)*alpha/n)/(1-alpha)
              # fA ist bernoulli mit success probability pext[0]
              # same for fB. correlations between them are not important for the flicker to v1 correlations!
              # Ich brauche die folgenden Paare
              EfAA += l*p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])
              EAn = p_s*np.sum([(i+1)*p for i,p in enumerate(pavs_a)])
              EA += EAn
              EAAn = p_s*np.sum([(i+1)**2*p for i,p in enumerate(pavs_a)])
              VarAn = EAAn-EAn**2
              VarA = VarAn/num_binned
              EAAnew = EAn**2+VarA
              EAA += EAAnew
              EfA += l
              EfAfA += l**2
          EfAA = EfAA/n_levels
          EA = EA / n_levels
          EAA = EAA/n_levels
          EfA = EfA/n_levels
          EfAfA = EfAfA/n_levels
          print(EAA-EA**2)
          cfAA = (EfAA-EfA*EA)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(EAA-EA**2))
          return cfAA,cfAA, cfBB, EfAA, EfBB, EA, EAA, EB, EBB, EfA, EfAfA, EfB, EfBfB 
#+end_src

*** Bernstein poster code
    #+begin_src python :session rbs :results raw drawer :async yes
      from tqdm import tqdm
      from utils.model import *
      import numpy as np

      #c1s = np.array([0,0.25,0.5,0.75,0.8,0.85,0.9,0.95,1])
      c1s = np.array([0,0.25,0.5,0.75,1])
      #bin_times = np.linspace(1e-3,100e-3,50)
      bin_times = np.array([0.01/i for i in [1,2,5,10,20,50,100,200,500,1000]])
      # factors = np.arange(1,101)
      # bin_times = 100e-3/factors
      #bin_times = np.array([1e-3,2e-3,4e-3,5e-3,10e-3,20e-3,25e-3,50e-3,100e-3])
      bin_ccs = np.zeros((len(c1s),len(bin_times)))
      ana_bin_ccs = np.zeros((len(c1s),len(bin_times)))
      cc_anas = np.zeros_like(c1s)
      cc_nums = np.zeros_like(c1s)
      rates = np.zeros_like(c1s)
      anas = []
      nums = []
      for saved_rates in [np.array([ 9.932 , 12.8433, 18.0813, 30.5812, 99.52 ]),None]:
                          #[np.array([9.9622,12.7557,17.996,30.6616,35.3456,42.0059,52.3873,69.1412,99.7721]),None]:
          #saved_rates = np.array([9.9622,12.7557,17.996,30.6616,35.3456,42.0059,52.3873,69.1412,99.7721])##None#np.array([ 9.932 , 12.8433, 18.0813, 30.5812, 99.52  ])
          target_rate = 40
          for j,c1 in enumerate(c1s):#c1 = 0.8
              nf = 10000
              n = 100
              c = 0.25
              ac = 1-1/np.sqrt(n)
              alpha = c1*ac
              num_flick = 10000
              base_p = 0.5
              flick_dur = 10e-3
              # an uncoupled neuron fires on average 0.1 times inside single flicker realization
              # just calculate this here analytically -> you know how....
              deltaU = (1-0.9)/(base_p*nf/n)+ np.pi*1e-5
              deltaU = 
              if saved_rates is not None:
                  deltaU *= target_rate/saved_rates[j]
              flicker_levels = 5
              d_levels = np.linspace(-1,1,flicker_levels)
              def flicker(levels=None):
                  levels = d_levels if levels is None else levels
                  return levels[np.random.randint(0,len(levels))]
              flicks = []
              N = n+1
              u = np.random.random((N,))
              avss = []
              avts = []
              kss = []
              Ws = np.array([[alpha,0],[0,0]])
              assert(alpha+deltaU < 1)
              for i in range(num_flick):
                  if i % int(num_flick/100) == 0:
                      print(i,flush=True)
                  flick = flicker()
                  flicks.extend([flick]*nf)
                  p1 = base_p*(1+c*flick)
                  p2 = 1-p1
                  Ns = (n,1)
                  p = np.array([p1,p2])
                  avs,avt,u,ks = simulate_model_subnets(Ns,Ws,p,deltaU,num_steps=nf,u0=u)
                  avs = np.array(avs)
                  kss.append(ks)
                  avss.append(avs)
                  avts.append(np.array(avt)+i*nf)
              flicks = np.array(flicks)
              avs = np.concatenate(avss)
              avt = np.concatenate(avts)
              ks = np.concatenate(kss)
              ns = len(ks)
              spv = np.zeros(ns)
              for at,a in zip(avt,avs):
                  spv[at] = a[0]
              rate = np.sum(spv)/(flick_dur*num_flick*n)
              rates[j] = rate
              print('rate ',rate,flush=True)
              print('subsample to 1ms')
              ccs = np.zeros_like(bin_times)
              ana_ccs = np.zeros_like(bin_times)
              for i,bt in enumerate(bin_times):
                  binned = bin_data(spv,int(flick_dur/bt*len(spv)/nf))
                  binnedf = bin_data(flicks,int(flick_dur/bt *len(spv)/nf))
                  ccs[i] = np.corrcoef(binned,binnedf)[0,1]
                  ana_ccs[i] = binned_ehe_flicker_corrs_flicker_levels_V1_ana(n,alpha,deltaU,flicker_levels,c,round(bt/1e-6))[0]
              bin_ccs[j,:] = ccs
              ana_bin_ccs[j,:] = ana_ccs
              cc_anas[j] = ehe_flicker_corrs_flicker_levels_V1_ana(n,alpha,deltaU,flicker_levels,c)
              cc_nums[j] = np.corrcoef(spv,flicks)[0,1]
          anas.append(cc_anas.copy())
          nums.append(cc_nums.copy())


          plt.figure()
          for j,c1 in enumerate(c1s):
              plt.semilogx(bin_times,bin_ccs[j],label=str(c1)+' rate='+str(np.round(rates[j],2)))
          plt.legend()
          plt.xlabel('bin width')
          plt.ylabel('correlation')
          plt.title(('Ohne' if saved_rates is None else 'Mit')+' Ratenausgleich')
          plt.xscale('log')
          plt.savefig('new_cc_bin_times_'+str(saved_rates is None)+'.png',dpi=200)
          plt.savefig('new_cc_bin_times_'+str(saved_rates is None)+'.svg')


          plt.figure()
          for j,c1 in enumerate(c1s):
              plt.semilogx(bin_times,ana_bin_ccs[j],label='ana '+str(c1)+' rate='+str(np.round(rates[j],2)))
          plt.legend()
          plt.xlabel('bin width')
          plt.ylabel('correlation')
          plt.title(('Ohne' if saved_rates is None else 'Mit')+' Ratenausgleich')
          plt.xscale('log')
          plt.savefig('ana_new_cc_bin_times_'+str(saved_rates is None)+'.png',dpi=200)
          plt.savefig('ana_new_cc_bin_times_'+str(saved_rates is None)+'.svg')
    

      plt.figure()
      plt.plot(c1s,anas[0],label='analytical same rate')
      plt.plot(c1s,nums[0],label='numerical same rate')
      plt.plot(c1s,anas[1],label='analytical scaled rates')
      plt.plot(c1s,nums[1],label='numerical scaled rates')
      plt.legend()
      plt.title('analytical corr. coefs on dt timescale')
      plt.xlabel('c1')
      plt.ylabel('correlation')
      plt.savefig('new_cc_dt_ana_num.png',dpi=200)
      plt.savefig('new_cc_dt_ana_num.svg')

  #+end_src 

  #+RESULTS:
  :results:
  : 0


  #+begin_src python :session rbs :results raw drawer 


    ccs = [binned_ehe_flicker_corrs_flicker_levels_V1_ana(n,alpha,deltaU,flicker_levels,c,int(1/bt))[0] for bt in bin_times]
    plt.semilogx(bin_times,ccs)

       
  #+end_src 



*** New ideas
    - [ ] extract correlation values from the full 3pop grid searches!
    - [ ] check calculations by sampling independent points on the torus.
      - ... or just discretely the avalanche sizes according to the distributions!
    - [ ] Apply mutual information!
    - [ ] Apply unique information!
** Analytical avalanche dist of V1a/V1b to V4 system

  #+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py
    from scipy.special import gammaln

    def lognchoosek(N,k):
        return gammaln(N+1) - gammaln(k+1) - gammaln(N-k+1);

    def ehe_flicker_avs_ana(n,alpha,beta,delta,gamma,deltaU=1,pext=None,pp=False):
        p = np.ones(n)/n if pext is None else pext
        log = np.log
        p_a = np.zeros((n+1,n+1))
        p_b = np.zeros((n+1,n+1))
        p_d = np.zeros((n+1,))
        for a in range(1,n+1):
            if pp: print('a',a)
            for d in range(0,n+1):
                p_a[a,d] = np.exp(log(a*deltaU)+lognchoosek(n,a)+lognchoosek(n,d)+log(alpha/n)
                                  +log(a*gamma/n)+log(a*alpha/n)*(a-2)+log((d*delta+a*gamma)/n)*(d-1)
                                  +log(1-alpha)+log(1-beta)+log(1-delta-a*gamma/n)
                                  +log(1-a*alpha/n)*(n-a-1)+log(1-(a*gamma+d*delta)/n)*(n-d-1))
                b = a
                p_b[b,d] = np.exp(log(b*deltaU)+lognchoosek(n,b)+lognchoosek(n,d)+log(beta/n)
                                  +log(b*gamma/n)+log(b*beta/n)*(b-2)+log((d*delta+b*gamma)/n)*(d-1)
                                  +log(1-beta)+log(1-alpha)+log(1-delta-b*gamma/n)
                                  +log(1-b*beta/n)*(n-b-1)+log(1-(b*gamma+d*delta)/n)*(n-d-1))
        p_a[0,0] = 1-deltaU*n*(1-(n-1)*alpha/n)*(1-beta)*(1-delta)
        p_b[0,0] = 1-deltaU*n*(1-alpha)*(1-(n-1)*beta/n)*(1-delta)

        for d in range(1,n+1):
            p_d[d] = np.exp(log(d*deltaU)+lognchoosek(n,d)+log(delta/n)+log(d*delta/n)*(d-2)
                            +log(1-alpha)+log(1-beta)+log(1-delta)+log(1-d*delta/n)*(n-d-1))
        p_d[0] = 1-deltaU*n*(1-alpha)*(1-beta)*(1-(n-1)*delta/n)


        pavs_a = np.sum(p_a,axis=1)[1:]/(1-p_a[0,0])
        pavs_b = np.sum(p_b,axis=1)[1:]/(1-p_b[0,0])
        pavs_d = p[2]*p_d[1:]+p[0]* np.sum(p_a,axis=0)[1:]+p[1]* np.sum(p_b,axis=0)[1:]
        pavs_d /= np.sum(pavs_d)
        return p_a,p_b,p_d,pavs_a,pavs_b,pavs_d



  #+end_src    

  #+RESULTS:


  #+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py
    from tqdm import tqdm
    import itertools
    from itertools import chain, combinations
    import sympy as sym
    from scipy.special import binom


    def vnonin_mod(ns,Us,W,det_func=np.linalg.det):
        #print(ns,Us)
        Usz_i = np.where(Us==0)[0]
        if len(Usz_i) > 0:
            if np.sum(ns[Usz_i])>0:
                return 0
            Usz_i2 = np.where(Us>0)[0]
            W = W[np.ix_(Usz_i2,Usz_i2)]
            ns = ns[Usz_i2]
            Us = Us[Usz_i2]
        # print('W',W)
        # print('Us',Us,'ns',ns)
        # print('determinant of ',np.diag(Us)-W@np.diag(ns))
        # print('is ',det_func(np.diag(Us)-W@np.diag(ns)))
        # print('times ',Us,(ns-1))
        ret = det_func(np.diag(Us)-W@np.diag(ns))*(Us**(ns-1)).prod()
        return ret if not(np.isnan(ret)) else 0



    class ModularAvalancheSizeDistributionFast():
        def __init__(self,Ns,W,deltaU=0.022,symbolic=False):
            self.Ns = np.array(Ns)
            self.N = np.sum(Ns)
            self.W = np.array(W)
            self.deltaU = deltaU
            self.volumes = {}
            self.detailed_volumes = {}
            self.probs = {}
            self.det_func = np.linalg.det \
                            if not symbolic else lambda a: sym.Matrix(a).det()


        def detailed_volume(self,st,ns):
            #print('hi st ns',st,ns)

            vol = self.detailed_volumes.get((st,tuple(ns)))
            if vol is not None:
                return vol
            num_ass = np.prod([binom(Nsi,nsi) for (Nsi,nsi) in zip(self.Ns,ns)])
            if (ns[st] == 0) or (num_ass==0):
               self.detailed_volumes[(st,tuple(ns))] = 0
               return 0
            ns = np.array(ns)
            # act = (self.W-np.diag(np.diag(self.W)))@ns
            # inds = np.where(ns>0)[0]
            # inds = [i for i in inds if i!=st]
            # if (act[inds] == 0).any():
            #     self.detailed_volumes[(st,tuple(ns))] = 0
            #     return 0
            vol_is = self.deltaU
            Us = self.W@ns
            #print('Us',Us)
            vol = 1
            #print(st,ns)
            if np.sum(ns)>1 and ns[st]>0:
                remu = ns.copy()
                remu[st]-=1
                #vol = ns[st]*(np.prod([U**e for (U,e) in zip(Us,remu)])-vnoninlr_smd(remu,Us,self.smd))
                vol = ns[st]*vnonin_mod(remu,Us,self.W,det_func=self.det_func)
                #((Us**remu).prod()-vnoninlr_smd(remu,Us,self.smd))
            if vol == 0:
                self.detailed_volumes[(st,tuple(ns))] = vol
                return vol
            volc = 1
            if (ns <= self.Ns).all() and np.sum(ns)<np.sum(self.Ns):
                Ucs = 1-Us
                ncs = self.Ns-ns
                #volc = np.prod([U**e for (U,e) in zip(Ucs,ncs)])-vnoninlr_smd(ncs,Ucs,self.smd)
                volc = vnonin_mod(ncs,Ucs,self.W,det_func=self.det_func)
                #(Ucs**ncs).prod()-vnoninlr_smd(ncs,Ucs,self.smd)
            #print('num_ass,vol_is vol volc',num_ass,vol_is,vol,volc)
            vol = num_ass*vol_is*vol*volc
            self.detailed_volumes[(st,tuple(ns))] = vol
            return vol

        def volume_size0_st(self,st):
            vol = 0
            remu = self.Ns.copy()
            remu[st]-=1
            vol = self.Ns[st]*self.deltaU*vnonin_mod(remu,np.ones(self.W.shape[0]),self.W,det_func=self.det_func)
            #(1-vnoninlr_smd(remu,np.ones(self.W.shape[0]),self.smd))
            return vol

        def volume_size0(self):
            return np.sum([self.volume_size0_st(i) for i in range(self.W.shape[0])])

        def volume(self,n):
            vol = self.volumes.get(n)
            if vol is not None:
                return vol
            #print("n",n)    
            if n == 0:
                vol = self.volume_size0()
                self.volumes[n] = vol
                return vol
            vol = 0
            # for ns1 in range(min(self.Ns[0]+1,n+1)):
            #     ns2 = n - ns1
            #     if ns2 <= self.Ns[1]:
            #         num_ass = binom(self.Ns[0],ns1)*binom(self.Ns[1],ns2)                
            #         vol += num_ass*self.detailed_volume(0,(ns1,ns2))
            #         vol += num_ass*self.detailed_volume(1,(ns1,ns2))
            for ns in itertools.product(*[list(range(min(Nsi+1,n+1))) for Nsi in self.Ns]):
                if np.sum(ns)==n:
                    num_ass = 1#np.prod([binom(Nsi,nsi) for (Nsi,nsi) in zip(self.Ns,ns)])
                    for st in range(self.W.shape[0]):
                        vol += num_ass*self.detailed_volume(st,ns)
            self.volumes[n] = vol
            return vol

        def compute_all(self):
            self.volumes = {i:0 for i in range(np.sum(self.Ns)+1)};
            self.volumes[0] = self.volume_size0()
            asses = list(itertools.product(*[range(Nsi+1) for Nsi in self.Ns]))
            for ns in tqdm(asses):
                if np.sum(ns)>0:                 
                    num_ass = 1#np.prod([binom(Nsi,nsi) for (Nsi,nsi) in zip(self.Ns,ns)])
                    vol = 0
                    #print(num_ass)
                    for st in range(self.W.shape[0]):
                            #print("vol ",self.detailed_volume(st,ns))
                            vol += num_ass*self.detailed_volume(st,ns)
                    self.volumes[np.sum(ns)]+=vol

        def prob(self,n):
            prob = self.probs.get(n)
            if prob is not None:
                return prob
            norm_n = self.volume(n)/self.N # 1/N prob to get activation to correct
            norm_0 = self.volume(0)/self.N
            # norm_noninh = self.volume(-1)
            # prob = norm_n/(norm_noninh - norm_0)
            prob = norm_n/norm_0
            self.probs[n] = prob
            return prob

        def avs_dist(self):
            for i in range(1,self.N+1):
                self.prob(i)
            return self.probs    


  #+end_src 

  #+RESULTS:


  Analytical avalanche distibution 3pop model
#+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py 
  def detailed_avs_dist_3pop(c1,c3='auto',cup=0.3,c_control=0.095,n=100,deltaU=1,return_vol=False):
      n2 = int(n/10)
      Ns = (n,n,n2,n)
      ac = 1-1/np.sqrt(n)
      c2 = c1
      if c3=='auto':
          c3 = c2-2*cup
          assert(c3>0)
      Ws = np.zeros((4,4))#one extra unit to supply the overshoot of deltaU external inputs
      Ws[0,0] = c1*ac/n # V1 A
      # Ws[1,:] is inhibited second population of nonattended V1
      Ws[1,1] = c2*ac/n # V1 B 
      Ws[2,2] = (1-np.sqrt(n2)/n2)/n2
      Ws[1,2] = c_control/n2 # input from crit subnet to V1 B # 0.03 f√ºr n=1000
      Ws[3,3] = c3*ac/n # V4 pop
      Ws[3,0] = cup/n # inputs to V4
      Ws[3,1] = cup/n
      avsd = ModularAvalancheSizeDistributionFast(Ns,Ws,deltaU=1)
      p = np.ones(len(Ns))#/len(Ns)
      v_V1aV4 = np.array([[avsd.detailed_volume(0,(i,0,0,j)) for j in range(0,n+1)] for i in range(n+1)])
      p_V1aV4 = p[0]*avsd.volume_size0_st(0)
      v_V1bV4 = np.array([[avsd.detailed_volume(1,(0,i,0,j)) for j in range(0,n+1)] for i in range(n+1)])
      p_V1bV4 = p[1]*avsd.volume_size0_st(1)
      v_V1bcV4 = np.array([[[avsd.detailed_volume(2,(0,j,i,k)) for k in range(0,n+1)]
                            for j in range(0,n+1)] for i in range(n2+1)])
      p_V1bcV4 = p[2]*avsd.volume_size0_st(2)
      v_V4 = np.array([avsd.detailed_volume(3,(0,0,0,i)) for i in range(0,n+1)])
      p_V4 = p[3]*avsd.volume_size0_st(3)
      # explanation: v_... is the unnormalized distribution
      # the corresponding p_... is the probability of a non-empty avalanche
      # therefore, the fraction of v and p give the avalanche size distribution
      # only the p parameters depend on the input statistics!
      if return_vol:
          vol = vnonin_mod(np.array(Ns),np.ones_like(Ns),Ws)
          return ((v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4),vol)
      return (v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4)

  def avs_size_distributions_3pop(detailed_dist,s,V4ext=False,p = None,n=100):
      v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist
      n2 = int(n/10)
      p = np.ones(4)/4 if p is None else p
      p_w = p*np.array([n,n,n2,n])/(3*n+n2)
      # Question: Does p need to sum to 1 how to handle n2 and n parts analytically?
      V1a = p_w[0]*np.array([np.sum(v_V1aV4[i,:]) for i in np.arange(n)+1])/(p_V1aV4*p_w[0])
      #TODO Hier weitermachen!!!
      V1b = (p_w[2]*np.array([np.sum(v_V1bcV4[:,i,:]) for i in np.arange(n)+1])/(p_V1bcV4-np.sum(v_V1bcV4[:,0,:]))
             +p_w[1]*np.array([np.sum(v_V1bV4[i,:]) for i in np.arange(n)+1])/p_V1bV4)/(p_w[1]+p_w[2])
      V4 = ((p_w[3]*v_V4[1:]/p_V4 if V4ext else 0)+
            p_w[0]*np.array([np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])/
            (p_V1aV4-np.sum(v_V1aV4[:,0])-np.sum(v_V1aV4[:s,1:]))+
            p_w[1]*np.array([np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])/
            (p_V1bV4-np.sum(v_V1bV4[:,0])-np.sum(v_V1bV4[:s,1:]))+
            p_w[2]*np.array([np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1])/
            (p_V1bcV4-np.sum(v_V1bcV4[:,:,0])-np.sum(v_V1bcV4[:,:s,1:])))/(p_w[0]+p_w[1]+p_w[2]+(p_w[3] if V4ext else 0))
      return V1a,V1b,V4

#+end_src 

#+RESULTS:

Now with binary V4
#+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py 
  def detailed_avs_dist_3pop_nv4(c1,c3='auto',cup=0.3,c_control=0.095,n=100,nv4=1,deltaU=1,return_vol=False):
      n2 = int(n/10)
      Ns = (n,n,n2,nv4)
      ac = 1-1/np.sqrt(n)
      c2 = c1
      if c3=='auto':
          c3 = c2-2*cup
          assert(c3>0)
      Ws = np.zeros((4,4))#one extra unit to supply the overshoot of deltaU external inputs
      Ws[0,0] = c1*ac/n # V1 A
      # Ws[1,:] is inhibited second population of nonattended V1
      Ws[1,1] = c2*ac/n # V1 B 
      Ws[2,2] = (1-np.sqrt(n2)/n2)/n2
      Ws[1,2] = c_control/n2 # input from crit subnet to V1 B # 0.03 f√ºr n=1000
      Ws[3,3] = c3*ac/nv4 # V4 pop
      Ws[3,0] = cup/n # inputs to V4
      Ws[3,1] = cup/n
      avsd = ModularAvalancheSizeDistributionFast(Ns,Ws,deltaU=1)
      p = np.ones(len(Ns))#/len(Ns)
      v_V1aV4 = np.array([[avsd.detailed_volume(0,(i,0,0,j)) for j in range(0,nv4+1)] for i in range(n+1)])
      p_V1aV4 = p[0]*avsd.volume_size0_st(0)
      v_V1bV4 = np.array([[avsd.detailed_volume(1,(0,i,0,j)) for j in range(0,nv4+1)] for i in range(n+1)])
      p_V1bV4 = p[1]*avsd.volume_size0_st(1)
      v_V1bcV4 = np.array([[[avsd.detailed_volume(2,(0,j,i,k)) for k in range(0,nv4+1)]
                            for j in range(0,n+1)] for i in range(n2+1)])
      p_V1bcV4 = p[2]*avsd.volume_size0_st(2)
      v_V4 = np.array([avsd.detailed_volume(3,(0,0,0,i)) for i in range(0,nv4+1)])
      p_V4 = p[3]*avsd.volume_size0_st(3)
      # explanation: v_... is the unnormalized distribution
      # the corresponding p_... is the probability of a non-empty avalanche
      # therefore, the fraction of v and p give the avalanche size distribution
      # only the p parameters depend on the input statistics!
      if return_vol:
          vol = vnonin_mod(np.array(Ns),np.ones_like(Ns),Ws)
          return ((v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4),vol)
      return (v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4)

  def avs_size_distributions_3pop(detailed_dist,s,V4ext=False,p = None,n=100):
      v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist
      nv4 = v_V4.shape[0]-1
      n2 = int(n/10)
      p = np.ones(4)/4 if p is None else p
      p_w = p*np.array([n,n,n2,nv4])/(3*n+n2)
      # Question: Does p need to sum to 1 how to handle n2 and n parts analytically?
      V1a = p_w[0]*np.array([np.sum(v_V1aV4[i,:]) for i in np.arange(n)+1])/(p_V1aV4*p_w[0])
      #TODO Hier weitermachen!!!
      V1b = (p_w[2]*np.array([np.sum(v_V1bcV4[:,i,:]) for i in np.arange(n)+1])/(p_V1bcV4-np.sum(v_V1bcV4[:,0,:]))
             +p_w[1]*np.array([np.sum(v_V1bV4[i,:]) for i in np.arange(n)+1])/p_V1bV4)/(p_w[1]+p_w[2])
      V4 = ((p_w[3]*v_V4[1:]/p_V4 if V4ext else 0)+
            p_w[0]*np.array([np.sum(v_V1aV4[s:,i]) for i in np.arange(nv4)+1])/
            (p_V1aV4-np.sum(v_V1aV4[:,0])-np.sum(v_V1aV4[:s,1:]))+
            p_w[1]*np.array([np.sum(v_V1bV4[s:,i]) for i in np.arange(nv4)+1])/
            (p_V1bV4-np.sum(v_V1bV4[:,0])-np.sum(v_V1bV4[:s,1:]))+
            p_w[2]*np.array([np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(nv4)+1])/
            (p_V1bcV4-np.sum(v_V1bcV4[:,:,0])-np.sum(v_V1bcV4[:,:s,1:])))/(p_w[0]+p_w[1]+p_w[2]+(p_w[3] if V4ext else 0))
      return V1a,V1b,V4

#+end_src 

#+RESULTS:


  Test the avalanche size distribution function
  #+begin_src python :session rbs :results raw drawer 
    c1 = np.linspace(0.2,1,80)[55]
    c3 = 0.4#'auto'
    cup=0.3;
    c_control = 0.095
    n=100;

    import time
    st = time.time()
    detailed_dist = detailed_avs_dist_3pop(c1,c3,cup,c_control,n)
    print('computing detailed avs took '+str(time.time()-st)+' seconds')



    s=5; # for n=1000 # war vorher threshold
    V1a,V1b,V4 = avs_size_distributions_3pop(detailed_dist,s,V4ext=False,p = None)
    plt.figure(figsize=(3,2),dpi=200)
    plt.loglog(np.arange(n)+1,V1a,label='V1a')
    plt.loglog(np.arange(n)+1,V1b,label='V1b')
    plt.loglog(np.arange(n)+1,V4,label='V4 theta='+str(s))
    plt.loglog(np.arange(n)+1,ehe_ana(0.9,100),label='crit')
    plt.ylim([1e-6,1])
    plt.legend()
    plt.xlabel('avalanche size s')
    plt.ylabel('P(s)')
    plt.title('analytical av. dist.')

  #+end_src 

  #+RESULTS:
  :RESULTS:
  : computing detailed avs took 15.902426958084106 seconds
  : Text(0.5, 1.0, 'analytical av. dist.')
  [[file:./.ob-jupyter/d248bf23c694a9751495091114965fe5b001b616.png]]
  :END:


  Sample from the analytical avalanche distribution
  #+begin_src python :session rbs :results raw drawer 
    c1 = np.linspace(0.2,1,80)[55]
    c3 = 'auto'
    cup=0.3;
    c_control = 0.095
    n=100;

    import time
    st = time.time()
    detailed_dist = detailed_avs_dist_3pop(c1,c3,cup,c_control,n)
    print('computing detailed avs took '+str(time.time()-st)+' seconds')


    v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist


    # flicker etc simulieren....



    # s=5; # for n=1000 # war vorher threshold
    # V1a,V1b,V4 = avs_size_distributions_3pop(detailed_dist,s,V4ext=True,p = None)
    # plt.figure(figsize=(3,2),dpi=200)
    # plt.loglog(np.arange(n)+1,V1a,label='V1a')
    # plt.loglog(np.arange(n)+1,V1b,label='V1b')
    # plt.loglog(np.arange(n)+1,V4,label='V4 theta='+str(s))
    # plt.loglog(np.arange(n)+1,ehe_ana(0.9,100),label='crit')
    # plt.ylim([1e-6,1])
    # plt.legend()
    # plt.xlabel('avalanche size s')
    # plt.ylabel('P(s)')
    # plt.title('analytical av. dist.')

    # '
  #+end_src 



  analytical correlation ratio
  refactored correlation ratio code
  #+begin_src python :session rbs :results raw drawer :tangle ana/analytical.py
    def ana_flicker_v4_correlation(detailed_dist,s,deltaU,n_levels=5,c=0.25):
        v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist
        norm = ((2*n+n2)*(1+c))
        p_base = np.array([n,n,n2,0])/norm
        n_levels = 5
        levels = np.linspace(-1,1,n_levels)
        cfA4 = 0
        cfB4 = 0
        EfA4 = 0
        EfB4 = 0
        E4 = 0
        E44 = 0
        EfA = 0
        EfAfA = 0
        EfB = 0
        EfBfB = 0
        for lA in levels:
            for lB in levels:
                # Adjust input probabilities
                p = p_base.copy()
                p[0] = p[0]*(1+c*lA)
                p[1:3] = p[1:3]*(1+c*lB)
                fA = p[0]
                fB = p[1]+p[2]
                #print(lA,lB)
                #print(p,np.sum(p))
                # Calculate moments
                EfA+=fA
                EfB+=fB
                EfAfA+= fA**2
                EfBfB+= fB**2
                # √úberlegung dazu -> pV1aV4 etc sind unnorm. probs dass eine Lawine gestartet wird
                # daher f√ºr erwartungswert *PV1aV4 rechnen bzw. wegk√ºrzen und p*deltaU davor setzen
                # Ich glaube hier fehlen noch die p_... 
                E4n = deltaU*(#p[3]*np.sum([i*v_V4[i] for i in np.arange(n)+1])+
                              p[0]*np.sum([i*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                              p[1]*np.sum([i*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                              10*p[2]*np.sum([i*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                # print(lA,lB)
                # print(E4n)
                E4+=E4n
                E44n = deltaU*(#p[3]*np.sum([i**2*v_V4[i] for i in np.arange(n)+1])+
                               p[0]*np.sum([i**2*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                               p[1]*np.sum([i**2*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                               10*p[2]*np.sum([i**2*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                E44 += E44n
                EfA4 += fA*E4n
                EfB4 += fB*E4n
        EfA = EfA/n_levels**2
        EfB = EfB/n_levels**2
        EfAfA = EfAfA/n_levels**2
        EfBfB = EfBfB/n_levels**2
        E4 = E4/n_levels**2
        E44 = E44/n_levels**2
        EfA4 = EfA4/n_levels**2
        EfB4 = EfB4/n_levels**2
        cfA4 = (EfA4-EfA*E4)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(E44-E4**2))
        cfB4 = (EfB4-EfB*E4)/(np.sqrt(EfBfB-EfB**2)*np.sqrt(E44-E4**2))
        return cfA4,cfB4,EfA,EfB,EfAfA,EfBfB,E4,E44,EfA4,EfB4



    def binned_ana_flicker_v4_correlation(detailed_dist,s,deltaU,num_binned,n_levels=5,c=0.25):
        v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist
        norm = 3*n*(1+c)
        p_base = np.array([n,n,n,0])/norm
        n_levels = 5
        levels = np.linspace(-1,1,n_levels)
        cfA4 = 0
        cfB4 = 0
        EfA4 = 0
        EfB4 = 0
        E4 = 0
        E44 = 0
        EfA = 0
        EfAfA = 0
        EfB = 0
        EfBfB = 0
        for lA in levels:
            for lB in levels:
                # Adjust input probabilities
                p = p_base.copy()
                p[0] = p[0]*(1+c*lA)
                p[1:3] = p[1:3]*(1+c*lB)
                fA = p[0]
                fB = p[1]+p[2]
                #print(lA,lB)
                #print(p,np.sum(p))
                # Calculate moments
                EfA+=fA
                EfB+=fB
                EfAfA+= fA**2
                EfBfB+= fB**2
                # √úberlegung dazu -> pV1aV4 etc sind unnorm. probs dass eine Lawine gestartet wird
                # daher f√ºr erwartungswert *PV1aV4 rechnen bzw. wegk√ºrzen und p*deltaU davor setzen
                E4n = deltaU*(#p[3]*np.sum([i*v_V4[i] for i in np.arange(n)+1])+
                              p[0]*np.sum([i*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                              p[1]*np.sum([i*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                              p[2]*np.sum([i*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                # print(lA,lB)
                # print(E4n)
                E4+=E4n
                E44n = deltaU*(#p[3]*np.sum([i**2*v_V4[i] for i in np.arange(n)+1])+
                               p[0]*np.sum([i**2*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                               p[1]*np.sum([i**2*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                               p[2]*np.sum([i**2*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                Var44n = (E44n-E4n**2)/num_binned
                E44n = E4**2+Var44n
                E44 += E44n
                EfA4 += fA*E4n
                EfB4 += fB*E4n
        EfA = EfA/n_levels**2
        EfB = EfB/n_levels**2
        EfAfA = EfAfA/n_levels**2
        EfBfB = EfBfB/n_levels**2
        E4 = E4/n_levels**2
        E44 = E44/n_levels**2
        EfA4 = EfA4/n_levels**2
        EfB4 = EfB4/n_levels**2
        cfA4 = (EfA4-EfA*E4)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(E44-E4**2))
        cfB4 = (EfB4-EfB*E4)/(np.sqrt(EfBfB-EfB**2)*np.sqrt(E44-E4**2))
        return cfA4,cfB4,EfA,EfB,EfAfA,EfBfB,E4,E44,EfA4,EfB4


    # TODO something is wrong with this here....
    # Calculate it instead by drive relations
    # or by firing rate considerations!!!!!

    # Firing rate might be the most elegant thing....
    def corr_ratio_ana(detailed_dist,s):
        v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist
        Ia = np.sum([i*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])
        Ib = np.sum([i*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])
        Ic = np.sum([i*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1])
        return (Ib+Ic)/Ia
  #+end_src 

  #+RESULTS:

  apply function to compute analytical correlation ratios
  #+begin_src python :session rbs :results raw drawer 
    ratios = []
    ratios2 = []
    cfA4s = []
    cfB4s = []
    c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
    norm = ((2*n+n2)*(1+c))
    rate1 = 40
    for c1_idx in c1_idxs:
        c1 = np.linspace(0.2,1,80)[c1_idx]
        print(c1_idx)
        c3 = 0.4
        cup=0.3;
        c_control = 0.095
        n=100;
        import time
        st = time.time()
        detailed_dist = detailed_avs_dist_3pop(c1,c3,cup,c_control,n)
        print('computing detailed avs took '+str(time.time()-st)+' seconds'),10000
        s = 5
        ra1 = 1/(n-n*ac*c1) # rate ana 1 step
        deltaU = dt*n*rate1/(ra1*norm)
        print(deltaU)
        cfA4,cfB4,EfA,EfB,EfAfA,EfBfB,E4,E44,EfA4,EfB4 = ana_flicker_v4_correlation(detailed_dist,s,deltaU)
        cfA4s.append(cfA4)
        cfB4s.append(cfB4)
        ratios2.append(corr_ratio_ana(detailed_dist))
        ratios.append(cfB4/cfA4)

    # import pickle
    # pickle.dump([cfA4s,cfB4s,ratios],open('correlation_scans_theta=5.piclke','wb'))
    plt.figure(dpi=200,figsize=(3,2))
    plt.plot([np.linspace(0.2,1,80)[c] for c in c1_idxs],cfA4s,label='noatt')
    plt.plot([np.linspace(0.2,1,80)[c] for c in c1_idxs],cfB4s,label='att')
    plt.legend()
    # c1_idxs = c1_idxs[4:]
    # r = ratios[4:]
    # plt.figure(dpi=200,figsize=(3,2))
    # plt.plot([np.linspace(0.2,1,80)[c1_idx] for c1_idx in c1_idxs],r)

  #+end_src 

  #+begin_src python :session rbs :results raw drawer 
    plt.figure(figsize=(3,2),dpi=200)
    c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
    b = 6
    c1_idxs = c1_idxs[b:]
    for s in [2,5,10]:
        r = np.load(open('sc_collected_ehe_flicker_sync'+('_thresh'+str(s) if s != 5 else '')+'.npz','rb'))
        ccfA4 = r['ccfA4']
        ccfB4 = r['ccfB4']
        ccfA4 = np.nanmean(ccfA4,axis=3)
        ccfB4 = np.nanmean(ccfB4,axis=3)
        plt.plot([np.linspace(0.2,1,80)[c1_idx] for c1_idx in c1_idxs],
                 [ccfB4[c1_idx,0,0]/ccfA4[c1_idx,0,0] for c1_idx in c1_idxs],label='theta='+str(s)+' num')
        ratios,_ = pickle.load(open('ratios_theta'+str(s)+'.pickle','rb'))
        r = ratios[b:]
        plt.plot([np.linspace(0.2,1,80)[c1_idx] for c1_idx in c1_idxs],r,label='theta='+str(s)+' ana',linestyle='dashed')


    plt.xlabel('c')
    plt.ylabel('correlation ratio')
    plt.title('routing selectivity')
    plt.legend()
    plt.savefig('correlation_ratios_num_ana.png',dpi=200)
    plt.savefig('correlation_ratios_num_ana.svg')
  #+end_src 

** Analytical firing rate computation!

   TODO: How to accomodate theta here?
   Rate of V1a/V1b easy to compute by classical means....
   However, to get the input drive one must go to the avalanche
   based version. But then it is possible!!!!!
   Calculate input drive and the V4 rate is the same as if the external input
   would come in single external input steps with adjusted rates!
  
** Correlation analysis 3pop model theta=5

   Correlation values

   #+begin_src python :session rbs :results raw drawer 
     %matplotlib qt

     plt.rc('text', usetex=False)
     plt.rc('font', family='serif')

     import pickle
     c1s = np.linspace(0.2,1,80)
     radvs = np.array([12])
     methods = ['int','ext']

     r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5.npz','rb'))

     ccfA4 = r['c_ccfAV4']
     ccfB4 = r['c_ccfBV4']
     title = 'correlation flicker V4'

     ccfA4 = r['c_ccAV4'] # NOT BINNED!
     ccfB4 = r['c_ccBV4'] # NOT BINNED!
     title = 'correlation V1 V4'
     # ccfA4 = r['c_ccbAV4'] # kaum merklicher unterschied zu  
     # ccfB4 = r['c_ccbBV4'] # binned version! nice :)


     ccfA4 = r['c_ccfAA'] 
     ccfB4 = r['c_ccfBB'] 
     title = 'correlation flicker V1'
     ccfA4 = np.nanmean(ccfA4,axis=3)
     ccfB4 = np.nanmean(ccfB4,axis=3)


     ri = 0

     plt.figure(figsize=(3,2),dpi=200)
     m = max(np.max(ccfA4[:,ri,0]),np.max(ccfB4[:,ri,0]))
     plt.plot(c1s,ccfA4[:,ri,0],label='noatt')
     plt.plot(c1s,ccfB4[:,ri,0],label='att' )
     plt.vlines([c1s[55]],ymin=0,ymax=m,linestyle='dashed',color='k')
     plt.legend()
     plt.title(title)
     plt.xlabel('control parameter c')
     plt.ylabel('correlation')
     plt.savefig('img/'+title+'.png',dpi=200)
     plt.savefig('img/'+title+'.svg')

  #+end_src

  #+RESULTS:

** Spectral Coherence analysis 3pop model theta=5

  OLD: Not used anymore
  Color map!
  #+begin_src python :session rbs :results raw drawer 
    from matplotlib import cm
    from matplotlib.colors import ListedColormap, LinearSegmentedColormap
    import matplotlib.pyplot as plt


    # plt.rcParams['font.size'] = 16
    # plt.rcParams['font.family'] = 'Calibri'
    # plt.rc('text', usetex=True)
    #plt.rc('font', family='serif')
    color_A = '#C00000'
    color_B = '#8080CC'
    color_C = '#000000'

    def WBR(n_cols):
        x = np.arange(0,n_cols)/(n_cols-1);
        x_grid = np.array([0, 0.1, 0.35, 1.0])
        c_grid = np.array([[1, 1, 1],[1,1, 1],[0, 0, 1],[1, 0, 0]]);
        c = np.zeros([n_cols, 3]);
        for rgb in range(3):
            c[:, rgb] = np.interp(x,x_grid, c_grid[:, rgb]);
        return c


    cmap_old = ListedColormap(WBR(256))


    def BWR_balanced(n_cols,min_v,max_v):

        cmax = 1#max_v/max(max_v,np.abs(min_v))
        cmin = np.abs(min_v)/(max_v-min_v)
        print(cmax,cmin)
        x = np.linspace(-cmin,cmax,n_cols)
        x_grid = np.array([-cmin,0,cmax])
        c_grid = np.array([[0,0,1-cmin],[1,1,1],[1, 0, 0]]);
        c = np.zeros([n_cols, 3]);
        for rgb in range(3):
            c[:, rgb] = np.interp(x,x_grid, c_grid[:, rgb]);
        return c


    #cmap = ListedColormap(BWR_balanced(256))
  #+end_src 

  #+RESULTS:

  Spectral coherence values
  OLD: explorative plotting code
   #+begin_src python :session rbs :results raw drawer 
     import pickle
     c1s = np.linspace(0.2,1,80)
     radvs = np.array([12])
     methods = ['int','ext']
     freqs= np.logspace(np.log10(5),np.log10(200),100)
     r = np.load(open('sc_collected_ehe_flicker_sync_new.npz','rb'))


     ccfA4 = r['sc4fA']
     ccfB4 = r['sc4fB']
     fmax = 40
     suptitle = 'SC flicker V4'

     # ccfA4 = r['sc4A']
     # ccfB4 = r['sc4B']
     # fmax=100
     # suptitle = 'SC V1 V4'

     # ccfA4 = r['scfAA']
     # ccfB4 = r['scfBB']
     # fmax=75
     # suptitle = 'SC flicker V1'

     ccfA4 = np.nanmean(ccfA4,axis=3)
     ccfB4 = np.nanmean(ccfB4,axis=3)


     ri = 0
     ar = 0

     # cmax = max(np.max(ccfA4[:,ri,ar,:]),np.max(ccfB4[:,ri,ar,:]))
     # plt.figure(figsize=(15,4),dpi=200)
     # plt.subplot(1,3,1)
     # ca = plt.pcolormesh(c1s,freqs,ccfA4[:,ri,ar,:].T,vmin=0,vmax=cmax,cmap=cmap)
     # plt.title('spectral coherence A')
     # plt.colorbar(ca)
     # plt.vlines(c1s[55],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
     # plt.ylim(freqs[0],fmax)
     # plt.xlabel('c')
     # plt.ylabel('freq')
     # plt.subplot(1,3,2)
     # ca = plt.pcolormesh(c1s,freqs,ccfB4[:,ri,ar,:].T,vmin=0,vmax=cmax,cmap=cmap)
     # plt.colorbar(ca)
     # plt.title('spectral coherence B')
     # plt.ylim(freqs[0],fmax)
     # plt.xlabel('c')
     # plt.ylabel('freq')

     # plt.vlines(c1s[55],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
     # plt.subplot(1,3,3)
     # m = np.max(abs(ccfB4[:,ri,ar,:].T-ccfA4[:,ri,ar,:].T))
     # ca = plt.pcolormesh(c1s,freqs,ccfB4[:,ri,ar,:].T-ccfA4[:,ri,ar,:].T,vmin=-m,vmax=m,cmap='bwr')
     # plt.colorbar(ca)
     # plt.title('spectral coherence B-A')
     # plt.ylim(freqs[0],fmax)
     # plt.xlabel('c')
     # plt.ylabel('freq')
     # plt.vlines(c1s[55],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
     # plt.suptitle(suptitle)
     # # plt.savefig('img/'+suptitle+'.png',dpi=200)
     # # plt.savefig('img/'+suptitle+'.svg')



     plt.figure(figsize=(3,2),dpi=200)
     scdiff = (ccfB4[:,ri,ar,:].T-ccfA4[:,ri,ar,:].T)
     m = np.max(abs(scdiff))
     #m = 1
     ca = plt.pcolormesh(c1s,freqs,scdiff,vmin=-m,vmax=m,cmap='bwr')
     plt.colorbar(ca)
     #plt.title('spectral coherence B-A')
     plt.ylim(freqs[0],fmax)
     #plt.xlabel('c')
     #plt.ylabel('freq')
     plt.vlines(c1s[55],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')




     plt.figure(figsize=(3,2),dpi=200)
     scdiff_norm = np.log2(ccfB4[:,ri,ar,:].T/ccfA4[:,ri,ar,:].T)
     co = 9
     #scdiff_norm[np.abs(scdiff_norm)>co] = 0
     m = co#np.max(abs(scdiff_norm))
     #m = 1
     min_v = np.min(scdiff_norm)
     max_v = np.max(scdiff_norm)
     m = max(min_v,max_v)
     ca = plt.pcolormesh(c1s[40:],freqs,scdiff_norm[:,40:],vmin=-m,vmax=m,
                         cmap="bwr")
     plt.colorbar(ca)
     #plt.title('spectral coherence B-A')
     plt.ylim(freqs[0],fmax)
     #plt.xlabel('c')
     #plt.ylabel('freq')
     plt.vlines(c1s[55],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
     #plt.suptitle(suptitle)
     # plt.savefig('img/'+suptitle+'.png',dpi=200)
     # plt.savefig('img/'+suptitle+'.svg')





     plt.figure(figsize=(3,2),dpi=200);
     plt.title('sum of '+suptitle)
     plt.plot(c1s,np.sum(ccfB4[:,0,0],axis=-1),label='att');
     plt.plot(c1s,np.sum(ccfA4[:,0,0],axis=-1),label='noatt')
     plt.legend()
     m = max(np.max(np.sum(ccfB4[:,0,0],axis=-1)),np.max(np.sum(ccfA4[:,0,0],axis=-1)))
     plt.vlines(c1s[55],ymin=0,ymax=m,color='black',linestyle='dashed')
     plt.xlabel('c')
     plt.ylabel('sum of SC')
     # plt.savefig('img/'+suptitle+'summed_freq.png',dpi=200)
     # plt.savefig('img/'+suptitle+'summed_freq.svg')


     plt.figure(figsize=(3,2),dpi=200);
     plt.title(suptitle+' c='+str(round(c1s[55],2)))
     plt.plot(freqs,ccfB4[55,0,0],label='att');
     plt.plot(freqs,ccfA4[55,0,0],label='noatt')
     plt.xlabel('freq')
     plt.ylabel('SC')
     plt.xlim([freqs[0],fmax])
     plt.legend()
     # plt.savefig('img/'+suptitle+'opt.png',dpi=200)
     # plt.savefig('img/'+suptitle+'opt.svg')




     plt.figure(figsize=(3,2),dpi=200);
     plt.title('ratio of SC att/noatt V1 and V4')
     plt.title(suptitle+' c='+str(round(c1s[55],2)))
     plt.plot(freqs,np.log2(ccfB4[55,0,0]/ccfA4[55,0,0]));
     plt.hlines([-1,0,1,2],xmin=freqs[0],xmax=freqs[-1],colors='k',linestyle='dashed')
     plt.xlabel('freq')
     plt.ylabel('SC')
     plt.xlim([freqs[0],fmax])
     plt.legend()
  #+end_src

  #+RESULTS:
  :RESULTS:
  : <ipython-input-182-6d96306b6cd1>:36: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  :   ca = plt.pcolormesh(c1s,freqs,ccfA4[:,ri,ar,:].T,vmin=0,vmax=cmax,cmap=cmap)
  : <ipython-input-182-6d96306b6cd1>:42: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  :   ca = plt.pcolormesh(c1s,freqs,ccfB4[:,ri,ar,:].T,vmin=0,vmax=cmax,cmap=cmap)
  : <ipython-input-182-6d96306b6cd1>:49: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  :   ca = plt.pcolormesh(c1s,freqs,ccfB4[:,ri,ar,:].T-ccfA4[:,ri,ar,:].T,vmin=-m,vmax=m,cmap='WBR')
  # [goto error]
  :END:


  #+begin_src python :session rbs :results raw drawer 
    plt.figure(figsize=(3,2),dpi=200);
    plt.title('ratio of SC att/noatt V1 and V4')
    plt.title(suptitle+' c='+str(round(c1s[55],2)))
    plt.plot(freqs,np.log2(ccfB4[55,0,0]/ccfA4[55,0,0]));
    plt.hlines([-1,0,1,2],xmin=freqs[0],xmax=freqs[-1],colors='k',linestyle='dashed')
    plt.xlabel('freq')
    plt.ylabel('SC')
    plt.xlim([freqs[0],fmax])
    plt.legend()

  #+end_src 


** Plot correlation results theta=5

     Common constants for plotting

     #+begin_src python :session rbs :results raw drawer 
       import pickle
       plt.rc('text', usetex=False)
       plt.rc('font', family='serif')
       plot_levels=[np.log2(0.25),np.log2(0.5),np.log2(2),np.log2(4)]
       plot_colors=['black','gray','gray','black']
       plot_figsize=(3.2,2)
       ri = 0
       ar = 0
       c_start = 0
       fmax = 100
       c1s = np.linspace(0.2,1,80)
       radvs = np.array([12])
       methods = ['int','ext']
       freqs= np.logspace(np.log10(5),np.log10(200),100)
       fmi = np.searchsorted(freqs,fmax)
       #r = np.load(open('sc_collected_ehe_flicker_sync_new.npz','rb'))
       #r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5_c01.npz','rb'))
       r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5.npz','rb'))
       copt = 55
       copt = 58
       plot_vline=True
     #+end_src 

     #+RESULTS:


*** Correlation Coefficients

**** Flicker to V1

    #+begin_src python :session rbs :results raw drawer 
      ccfA4 = r['c_ccfAA'] 
      ccfB4 = r['c_ccfBB'] 
      title = 'correlation flicker V1'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200)
      m = max(np.max(ccfA4[:,ri,0]),np.max(ccfB4[:,ri,0]))
      plt.plot(c1s,ccfA4[:,ri,0],label='noatt')
      plt.plot(c1s,ccfB4[:,ri,0],label='att' )
      #thresholds,c1s_ana,acs,bcs = pickle.load(open('flicker_v1_corrs_mis_c1_thresholds_c01.pickle','rb'))
      thresholds,c1s_ana,acs,bcs = pickle.load(open('flicker_v1_corrs_mis_c1_thresholds.pickle','rb'))
      plt.plot(c1s_ana,acs[:,0],linestyle='dashed')
      plt.plot(c1s_ana,bcs[:,0],linestyle='dashed')
      plt.vlines([c1s[copt]],ymin=0,ymax=m,linestyle='dashed',color='k')
      plt.legend()
      plt.title(title)
      plt.xlabel('control parameter c')
      plt.ylabel('correlation')
      # plt.savefig('img/'+title+'.png',dpi=200)
      # plt.savefig('img/'+title+'.svg')
    #+end_src 

    #+RESULTS:
    :RESULTS:
    : Text(0, 0.5, 'correlation')
    [[file:./.ob-jupyter/d760e48e954cd92b62c297ddf98a9cd27f59e233.png]]
    :END:

**** V1 to V4


    #+begin_src python :session rbs :results raw drawer 
      ccfA4 = r['c_ccAV4'] 
      ccfB4 = r['c_ccBV4'] 
      title = 'correlation V1 V4'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200)
      m = max(np.max(ccfA4[:,ri,0]),np.max(ccfB4[:,ri,0]))
      plt.plot(c1s,ccfA4[:,ri,0],label='noatt')
      plt.plot(c1s,ccfB4[:,ri,0],label='att' )
      thresholds,c1_anas,acs,bcs = pickle.load(open('v1_v4_corrs_mis_c1_thresholds.pickle','rb'))
      plt.plot(c1_anas,acs[:,0],linestyle='dashed')
      plt.plot(c1_anas,bcs[:,0],linestyle='dashed')
      plt.vlines([c1s[copt]],ymin=0,ymax=m,linestyle='dashed',color='k')
      plt.legend()
      plt.title(title)
      plt.xlabel('control parameter c')
      plt.ylabel('correlation')
      # plt.savefig('img/'+title+'.png',dpi=200)
      # plt.savefig('img/'+title+'.svg')
    #+end_src 

    #+RESULTS:
    :RESULTS:
    # [goto error]
    : 
    : NameErrorTraceback (most recent call last)
    : <ipython-input-1-cfc02032434a> in <module>
    : ----> 1 ccfA4 = r['c_ccAV4']
    :       2 ccfB4 = r['c_ccBV4']
    :       3 title = 'correlation V1 V4'
    :       4 ccfA4 = np.nanmean(ccfA4,axis=3)
    :       5 ccfB4 = np.nanmean(ccfB4,axis=3)
    : 
    : NameError: name 'r' is not defined
    :END:



**** Flicker to V4

    #+begin_src python :session rbs :results raw drawer 
      import pickle
      ccfA4 = r['c_ccfAV4']
      ccfB4 = r['c_ccfBV4']
      # ccfA4 = r['ccfA4']
      # ccfB4 = r['ccfB4']
      title = 'correlation flicker V4'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200)
      m = max(np.max(ccfA4[:,ri,0]),np.max(ccfB4[:,ri,0]))
      plt.plot(c1s,ccfA4[:,ri,0],label='noatt')
      plt.plot(c1s,ccfB4[:,ri,0],label='att' )
      thresholds,c1_anas,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
      plt.plot(c1_anas,acs[:,5],linestyle='dashed') # hier 5 ohne c01 1 mit c01
      plt.plot(c1_anas,bcs[:,5],linestyle='dashed') # hier 5 ohne c01 1 mit c01
      plt.vlines([c1s[copt]],ymin=0,ymax=m,linestyle='dashed',color='k')
      plt.legend()
      plt.title(title)
      plt.xlabel('control parameter c')
      plt.ylabel('correlation')
      # plt.savefig('img/'+title+'.png',dpi=200)
      # plt.savefig('img/'+title+'.svg')
    #+end_src      

    #+RESULTS:
    :RESULTS:
    : Text(0, 0.5, 'correlation')
    [[file:./.ob-jupyter/b8f6d6f4dcd4e2ff14404ba5951b04f585924bf0.png]]
    :END:



*** Spectral coherence ratios



**** Flicker to V1



     #+begin_src python :session rbs :results raw drawer 
       import pickle
       ccfA4 = r['scfAA']
       ccfB4 = r['scfBB']
       suptitle = 'logratio SC flicker V1'
       ccfA4 = np.nanmean(ccfA4,axis=3)
       ccfB4 = np.nanmean(ccfB4,axis=3)
       plt.figure(figsize=plot_figsize,dpi=200)
       scdiff_norm = np.log2(ccfB4[:,ri,ar,:].T/ccfA4[:,ri,ar,:].T)
       min_v = np.min(scdiff_norm)
       max_v = np.max(scdiff_norm)
       m = max(min_v,max_v)
       ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
                           cmap="bwr")
       plt.contour(c1s[c_start:],freqs,scdiff_norm[:,c_start:],levels=plot_levels,colors=plot_colors)
       plt.colorbar(ca)
       plt.title(suptitle)
       plt.ylim(freqs[0],fmax)
       if plot_vline:
           plt.vlines(c1s[copt],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
       # plt.savefig('img/'+suptitle+'.png',dpi=200)
       # plt.savefig('img/'+suptitle+'.svg')
     #+end_src 

     #+RESULTS:
     :RESULTS:
     : <ipython-input-7-d82406e16439>:12: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
     :   ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
     [[file:./.ob-jupyter/f077a9a8aa526d6d123255781b6190429d363689.png]]
     :END:


**** V1 to V4

     #+begin_src python :session rbs :results raw drawer 
       ccfA4 = r['sc4A']
       ccfB4 = r['sc4B']
       suptitle = 'logratio SC V1 V4'
       ccfA4 = np.nanmean(ccfA4,axis=3)
       ccfB4 = np.nanmean(ccfB4,axis=3)
       plt.figure(figsize=plot_figsize,dpi=200)
       scdiff_norm = np.log2(ccfB4[:,ri,ar,:].T/ccfA4[:,ri,ar,:].T)
       min_v = np.min(scdiff_norm)
       max_v = np.max(scdiff_norm)
       m = max(min_v,max_v)
       ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
                           cmap="bwr")
       plt.contour(c1s[c_start:],freqs,scdiff_norm[:,c_start:],levels=plot_levels,colors=plot_colors)
       plt.colorbar(ca)
       plt.title(suptitle)
       plt.ylim(freqs[0],fmax)
       if plot_vline:
           plt.vlines(c1s[copt],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
       # plt.savefig('img/'+suptitle+'.png',dpi=200)
       # plt.savefig('img/'+suptitle+'.svg')


     #+end_src 

     #+RESULTS:
     :RESULTS:
     : <ipython-input-8-9d78889c69d7>:11: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
     :   ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
     [[file:./.ob-jupyter/8ca9688e0d3936f278761b1db4abe10012deb797.png]]
     :END:



**** flicker to V4

     #+begin_src python :session rbs :results raw drawer 
       ccfA4 = r['sc4fA']
       ccfB4 = r['sc4fB']
       suptitle = 'logratio SC flicker V4'
       ccfA4 = np.nanmean(ccfA4,axis=3)
       ccfB4 = np.nanmean(ccfB4,axis=3)
       plt.figure(figsize=plot_figsize,dpi=200)
       scdiff_norm = np.log2(ccfB4[:,ri,ar,:].T/ccfA4[:,ri,ar,:].T)
       min_v = np.min(scdiff_norm)
       max_v = np.max(scdiff_norm)
       m = max(min_v,max_v)
       ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
                           cmap="bwr")
       plt.contour(c1s[c_start:],freqs,scdiff_norm[:,c_start:],levels=plot_levels,colors=plot_colors)
       plt.colorbar(ca)
       plt.title(suptitle)
       plt.ylim(freqs[0],fmax)
       if plot_vline:
           plt.vlines(c1s[copt],ymin=freqs[0],ymax=fmax,color='black',linestyle='dashed')
       # plt.savefig('img/'+suptitle+'.png',dpi=200)
       # plt.savefig('img/'+suptitle+'.svg')


     #+end_src 

     #+RESULTS:
     :RESULTS:
     : <ipython-input-9-28e52335f8f6>:11: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
     :   ca = plt.pcolormesh(c1s[c_start:],freqs,scdiff_norm[:,c_start:],vmin=-m,vmax=m,
     [[file:./.ob-jupyter/ec1055c822bda04168df0ece0835319cfc35a9d6.png]]
     :END:
 

*** Spectral coherence sums


**** Flicker to V1
    #+begin_src python :session rbs :results raw drawer 
      ccfA4 = r['scfAA']
      ccfB4 = r['scfBB']
      suptitle = 'summed freq SC flicker V1'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200);
      plt.title(suptitle)
      plt.plot(c1s,np.mean(ccfB4[:,0,0,:fmi],axis=-1),label='att');
      plt.plot(c1s,np.mean(ccfA4[:,0,0,:fmi],axis=-1),label='noatt')
      plt.legend()
      m = max(np.max(np.mean(ccfB4[:,0,0,:fmi],axis=-1)),np.max(np.mean(ccfA4[:,0,0,:fmi],axis=-1)))
      plt.vlines(c1s[copt],ymin=0,ymax=m,color='black',linestyle='dashed')
      plt.xlabel('c')
      plt.ylabel('sum of SC')
      print('copt att',np.mean(ccfB4[copt,0,0,:fmi],axis=-1))
      print('copt noatt',np.mean(ccfA4[copt,0,0,:fmi],axis=-1))
      # plt.savefig('img/'+suptitle+'summed_freq.png',dpi=200)
      # plt.savefig('img/'+suptitle+'summed_freq.svg')

    #+end_src 

    #+RESULTS:
    :RESULTS:
    : copt att 0.3244476850028815
    : copt noatt 0.42688319116155327
    [[file:./.ob-jupyter/67e12bc6188e25876b60e1ae8967436ecee34048.png]]
    :END:

**** V1 to V4

    #+begin_src python :session rbs :results raw drawer 
      ccfA4 = r['sc4A']
      ccfB4 = r['sc4B']
      suptitle = 'summed freq SC V1 V4'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200);
      plt.title(suptitle)
      plt.plot(c1s,np.sum(ccfB4[:,0,0,:fmi],axis=-1),label='att');
      plt.plot(c1s,np.sum(ccfA4[:,0,0,:fmi],axis=-1),label='noatt')
      plt.legend()
      m = max(np.max(np.sum(ccfB4[:,0,0,:fmi],axis=-1)),np.max(np.sum(ccfA4[:,0,0,:fmi],axis=-1)))
      plt.vlines(c1s[copt],ymin=0,ymax=m,color='black',linestyle='dashed')
      plt.xlabel('c')
      plt.ylabel('sum of SC')
      print('copt att',np.mean(ccfB4[copt,0,0,:fmi],axis=-1))
      print('copt noatt',np.mean(ccfA4[copt,0,0,:fmi],axis=-1))

      plt.savefig('img/'+suptitle+'summed_freq.png',dpi=200)
      plt.savefig('img/'+suptitle+'summed_freq.svg')

    #+end_src 

    #+RESULTS:
    :RESULTS:
    : copt att 0.2599967280919163
    : copt noatt 0.14244087525604973
    [[file:./.ob-jupyter/f444cf40af2e679b5392c1b7a00a8d69a74f7570.png]]
    :END:

**** flicker to V4

         #+begin_src python :session rbs :results raw drawer 
           ccfA4 = r['sc4fA']
           ccfB4 = r['sc4fB']
           suptitle = 'summed freq SC flicker V4'
           ccfA4 = np.nanmean(ccfA4,axis=3)
           ccfB4 = np.nanmean(ccfB4,axis=3)
           plt.figure(figsize=(3,2),dpi=200);
           plt.title(suptitle)
           plt.plot(c1s,np.sum(ccfB4[:,0,0,:fmi],axis=-1),label='att');
           plt.plot(c1s,np.sum(ccfA4[:,0,0,:fmi],axis=-1),label='noatt')
           plt.legend()
           m = max(np.max(np.sum(ccfB4[:,0,0,:fmi],axis=-1)),np.max(np.sum(ccfA4[:,0,0,:fmi],axis=-1)))
           plt.vlines(c1s[copt],ymin=0,ymax=m,color='black',linestyle='dashed')
           plt.xlabel('c')
           plt.ylabel('sum of SC')
           print('copt att',np.mean(ccfB4[copt,0,0,:fmi],axis=-1))
           print('copt noatt',np.mean(ccfA4[copt,0,0,:fmi],axis=-1))
           # plt.savefig('img/'+suptitle+'summed_freq.png',dpi=200)
           # plt.savefig('img/'+suptitle+'summed_freq.svg')

    #+end_src 

    #+RESULTS:
    :RESULTS:
    : copt att 0.03347795967467139
    : copt noatt 0.015853129866536628
    [[file:./.ob-jupyter/13d4199477d273a36457d7c63a9c8347a37f3623.png]]
    :END:


** Locally optimal configuration jointly reproduces physiological signatures of selective signl routing

   Was sollte hier geplotted werden?
   avalanche distribution shift, reates and correlation coefs.
   spectral coherence plots at optimum
   
*** Spectral coherence copt


    #+begin_src python :session rbs :results raw drawer 
      ccfA4 = r['sc4fA']
      ccfB4 = r['sc4fB']
      suptitle = 'copt SC flicker V4'
      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)
      plt.figure(figsize=(3,2),dpi=200);
      plt.title(suptitle+' c='+str(round(c1s[copt],2)))
      plt.plot(freqs,ccfB4[copt,0,0],label='att');
      plt.plot(freqs,ccfA4[copt,0,0],label='noatt')
      plt.xlabel('freq')
      plt.ylabel('SC')
      plt.xlim([freqs[0],35])
      plt.legend()
      # plt.savefig('img/'+suptitle+'opt.png',dpi=200)
      # plt.savefig('img/'+suptitle+'opt.svg')

    #+end_src 

    #+RESULTS:
    [[file:./.ob-jupyter/c87c2f9c2ace3853a7e4373a2ec8f01724394663.png]]

   

    
    


*** TODO Plot V1 to V4 phase coherence
    
    #+begin_src python :session rbs :results raw drawer 
      my_shelf = shelve.open('./kadabuum_workspace_078_all.out')
    #+end_src 

    #+RESULTS:



    #+begin_src python :session rbs :results raw drawer 
      plt.figure(dpi=200,figsize=(3,2))
      plt.plot(my_shelf['freqs'],my_shelf['pc_a4'],label='PC(lA,l4)',color='blue')
      plt.plot(my_shelf['freqs'],my_shelf['pc_ba4'],label='PC(lB,l4)',color='red')
      plt.legend()
      plt.xlabel('frequency')
      plt.ylabel('Phase coherence')
      plt.savefig('pc_plot078.svg')
      plt.savefig('pc_plot078.png',dpi=200)
    #+end_src 

    #+RESULTS:
    [[file:./.ob-jupyter/c04524ac2a78ecb24b6c695f4d2df079197f982c.png]]



*** TODO Plot analytical and numerical avalanche size distributions

  #+begin_src python :session rbs :results raw drawer 
    c1 = np.linspace(0.2,1,80)[58]
    c3 = 0.4#'auto'
    cup=0.3;
    c_control = 0.095
    n=100;

    import time
    st = time.time()
    detailed_dist = detailed_avs_dist_3pop(c1,c3,cup,c_control,n)
    print('computing detailed avs took '+str(time.time()-st)+' seconds')



    s=5; # for n=1000 # war vorher threshold
    V1a,V1b,V4 = avs_size_distributions_3pop(detailed_dist,s,V4ext=False,p = None)
    plt.figure(figsize=(3,2),dpi=200)
    plt.loglog(np.arange(n)+1,V1a,label='V1a',color='blue',linestyle='dashed')
    plt.loglog(np.arange(n)+1,V1b,label='V1b',color='red',linestyle='dashed')
    plt.loglog(np.arange(n)+1,V4,label='V4 theta='+str(s),color='green',linestyle='dashed')
    plt.loglog(np.arange(n)+1,ehe_ana(0.9,100),label='crit',color='black')#,linestyle='dashed')

    #plt.figure(figsize=(6,4),dpi=200)
    avs_V1a = my_shelf['avs'][:,0]
    u,c = np.unique(avs_V1a[avs_V1a>0],return_counts=True)
    c = c / np.sum(c)
    plt.loglog(u,c,color='blue')
    avs_V1b = my_shelf['avs'][:,2]
    u,c = np.unique(avs_V1b[avs_V1b>0],return_counts=True)
    c = c / np.sum(c)
    plt.loglog(u,c,color='red')
    avs_V1b = my_shelf['avs'][:,4]
    u,c = np.unique(avs_V1b[avs_V1b>0],return_counts=True)
    c = c / np.sum(c)
    plt.loglog(u,c,color='green')
    plt.ylim([1e-6,1])
    plt.legend()
    plt.xlabel('avalanche size s')
    plt.ylabel('P(s)')
    plt.title('analytical av. dist.')
    plt.savefig('avs_plot_c078.png',dpi=200)
    plt.savefig('avs_plot_c078.svg')
  #+end_src 

  #+RESULTS:
  :RESULTS:
  : computing detailed avs took 11.037876605987549 seconds
  [[file:./.ob-jupyter/f0dd797e97fdf91a67f0508e4b795c366758f8c3.png]]
  :END:



    
  
** Dependence on model parameters

*** Routing selectivity

    


*** raster plots of c=0.75

**** Avalanche distributions from sim
    #+begin_src python :session rbs :results raw drawer 
      import numpy as np
      import matplotlib.pyplot as plt
      import shelve

      #to load back in
      #my_shelf = shelve.open('configuration075.out')
      my_shelf = shelve.open('./kadabuum_workspace_078_all.out')
    #+end_src 

    #+RESULTS:
   
    #+begin_src python :session rbs :results raw drawer
      #plt.figure(figsize=(6,4),dpi=200)
      plt.figure()
      avs = my_shelf['avs']

      avs_V1a = avs[:,0]
      u,c = np.unique(avs_V1a[avs_V1a>0],return_counts=True)
      c = c / np.sum(c)
      plt.loglog(u,c,label='V1a')


      avs_V1b = avs[:,2]
      u,c = np.unique(avs_V1b[avs_V1b>0],return_counts=True)
      c = c / np.sum(c)
      plt.loglog(u,c,label='V1a')


      avs_V1b = avs[:,4]
      u,c = np.unique(avs_V1b[avs_V1b>0],return_counts=True)
      c = c / np.sum(c)
      plt.loglog(u,c,label='V4')

    #+end_src 

    #+RESULTS:
    :RESULTS:
    | <matplotlib.lines.Line2D | at | 0x7f0080c7a8b0> |
    [[file:./.ob-jupyter/0b1a2f241312769f9f8f3d569512764976ed3a5a.png]]
    :END:


**** Raster plots V1a/V1b
     #+begin_src python :session rbs :results raw drawer 
       n = 100
       c1 = np.linspace(0.2,1,80)[55]
       radv = 12
       method = 'int'
       trial = 0
       #c1,radv,method = 
       rate1=40;
       cup=0.3;
       T=1;
       n2 = int(n/10);
       c=0.25;
       thresh=5; # for n=1000
       ac = 1-1/np.sqrt(n)
       flick_dur = 10e-3
       dt = flick_dur/10000
       n = 100
       n2 = int(n/10)
       N = n+n2
       W = np.zeros((N,N))
       W[:n,:n] = c1*ac/n
       W[n:N,n:N] = (1-np.sqrt(n2)/n2)/n2
       #W[:n,n:N] = 0.095/n2 # hier auskommentieren f√ºr nonatt

       aref = 0.9
       a = c1*ac
       deltaUref = 0.055364
       deltaU = (n*(1-a))*deltaUref/(n*(1-aref))

       p = np.ones(N)/N
       avs,avt,avu,u,ks = simulate_model(N,W,p,deltaU,num_steps=9000)
       #plt.figure(figsize=(6,4),dpi=200)
       raster = np.zeros((N,3000))
       raster2 = np.zeros((N,3000))
       offset = 6000
       for i,u in zip(avt,avu):
           if 0 <= i-offset < 3000:
               if np.max(u)>=100:
                   raster2[u,i-offset] = 1
               else:
                   raster[u,i-offset] = 1

       plt.figure(figsize=[4.59, 2.5 ],dpi=200)        
       plt.spy(raster[:N,:],aspect='auto',markersize=0.5,color='k')
       plt.hlines([100],xmin=0,xmax=3000,color='k',linewidth=0.5)
       # plt.savefig('raster_neu_att'+str(W[0,n]>0)+'.png',dpi=200)
       # plt.savefig('raster_neu_att'+str(W[0,n]>0)+'.svg')

       #plt.figure(figsize=[4.59, 2.5 ],dpi=200)        
       plt.spy(raster2[:N,:],aspect='auto',markersize=0.5,color='red')
       plt.hlines([100],xmin=0,xmax=3000,color='k',linewidth=0.5)
       # plt.savefig('raster2_neu_att'+str(W[0,n]>0)+'.png',dpi=200)
       # plt.savefig('raster2_neu_att'+str(W[0,n]>0)+'.svg')



       print(np.sum(avs))
     #+end_src 

     #+RESULTS:
     :RESULTS:
     : 4992
     [[file:./.ob-jupyter/e44d83732e64428c53de027a35347324da7b97d9.png]]
     [[file:./.ob-jupyter/e0f180bfc454451b206fdb37dedac700c559ef99.png]]
     :END:


**** TODO Raster plots with flicker input!!!    

*** cosyne abstract plots

    #+begin_src python :session rbs :results raw drawer 

      import pickle
      c1s = np.linspace(0.2,1,80)
      radvs = np.array([12])
      methods = ['int','ext']

      r = np.load(open('sc_collected_ehe_flicker_sync_thresh10.npz','rb'))

      ccfA4 = r['ccfA4']
      ccfB4 = r['ccfB4']

      ccfA4 = r['ccV1aV4']
      ccfB4 = r['ccV1bV4']

      ccfA4 = r['scfAA']
      ccfB4 = r['scfBB']

      ccfA4 = r['scV1aV4']
      ccfB4 = r['scV1bV4']


      ccfA4 = r['scfA4']
      ccfB4 = r['scfB4']



      ccfA4 = np.nanmean(ccfA4,axis=3)
      ccfB4 = np.nanmean(ccfB4,axis=3)


      ri = 0

      plt.figure(figsize=(1.2*2.4,1.2*1.6),dpi=200)
      #plt.subplot(1,2,1)
      plt.plot(c1s,ccfA4[:,ri,0],label=f'cc flicker A, V4' )
      plt.plot(c1s,ccfB4[:,ri,0],label=f'cc flicker B, V4' )
      plt.vlines([c1s[np.argmax(ccfB4[:,ri,0])]],ymin=0,ymax=0.06,linestyle='dashed',color='k')
      plt.legend()
      plt.title('Optimal flicker representation')
      plt.xlabel('control parameter c')
      plt.ylabel('correlation coefficient')
      # plt.savefig('img_cosyne/kadabuum_correlation_sync.png',dpi=200)
      # plt.savefig('img_cosyne/kadabuum_correlation_sync.svg')

   #+end_src

   #+RESULTS:
   :RESULTS:
   : Text(0, 0.5, 'correlation coefficient')
   [[file:./.ob-jupyter/559953084c44eb89d669f50f82b952f5777e1ba7.png]]
   :END:

   #+begin_src python :session rbs :results raw drawer
     #plt.figure()
     plt.plot(c1s,ccfA4[:,ri,1],label=f'A4,radv={radvs[ri]},method={methods[1]}' )
     plt.plot(c1s,ccfB4[:,ri,1],label=f'B4,radv={radvs[ri]},method={methods[1]}' )
     plt.title('correlations')
     plt.ylabel('cc')
     plt.xlabel('c1')

     plt.legend()


     plt.subplot(1,2,2)
     plt.hlines([1.3],xmin=0.2,xmax=1,color='k',linestyle='dashed',label='baseline no leak')
     plt.vlines([c1s[np.argmax(ccfB4[:,ri,0])]],ymin=1,ymax=2,linestyle='dashed',color='k')
     plt.title('correlation ratios C_att/C_noatt') 
     plt.ylabel('ratio')
     plt.legend()
     plt.xlabel('c1')
     plt.xlim([0.8*c1s[np.argmax(ccfB4[:,ri,0])],1])
     plt.ylim([1,2])

     plt.plot(c1s,ccfB4[:,ri,0]/ccfA4[:,ri,0],label=f'A4,radv={radvs[ri]},method={methods[0]}' )
     plt.plot(c1s,ccfB4[:,ri,1]/ccfA4[:,ri,1],label=f'A4,radv={radvs[ri]},method={methods[1]}' )
     plt.legend()

     plt.savefig('img_cosyne/kadabuum_correlation_ratios.png',dpi=200)
     plt.savefig('img_cosyne/kadabuum_correlation_ratios.svg')




     [uA,cA,uB,cB,uV4,cV4,avs_noatt,avs_att] = pickle.load(open('avalanches_opt_wp.pickle','rb'))
     thresh = 5
     ua,ca = uB[1:],cB[1:]
     pa = ca/np.sum(ca)
     un,cn = uA[1:],cA[1:]
     pn = cn/np.sum(cn)

     fa = [np.sum([p for p,u in zip(pa,ua) if u <= i]) for i in range(1,101)]
     fn = [np.sum([p for p,u in zip(pn,un) if u <= i]) for i in range(1,101)]

     from mpl_toolkits.axes_grid1.inset_locator import inset_axes
     plt.figure(figsize=(1.2*2.4,1.2*1.6))
     plt.loglog(ua,pa,label='V1a')
     plt.loglog(un,pn,label='V1b')
     plt.loglog(np.arange(100)+1,ehe_ana((1-1/np.sqrt(100)),100),color='k',linestyle='dashed',label='crit')
     plt.xlabel('avalanche size s')
     plt.vlines([5],ymin=1e-6,ymax=0.1,color='k',linestyle='dashed')
     plt.ylabel('p(s)')
     plt.legend()
     axins = inset_axes(plt.gca(),width='40%',height='40%',loc=3)
     axins.set_title(r'$E(s|s\geq t)p(s\geq t)$')
     thresh = 5
     xs = ['att','noatt']
     #ys = [np.sum([u*p for p,u in zip(pa,ua) if u>= thresh]),np.sum([u*p for p,u in zip(pn,un) if u>= thresh])]
     ys = [np.sum(avs_att[avs_att>=thresh])/len(avs_att),(np.sum(avs_noatt[avs_noatt>=thresh])/len(avs_noatt))]
     axins.bar(xs,ys)
     plt.suptitle('p(s) and rate of suprathreshold s. ratio '+str(np.round(ys[0]/ys[1],2)))
     plt.savefig('img_cosyne/kdw_av_illus_opt.png',dpi=200)
     plt.savefig('img_cosyne/kdw_av_illus_opt.svg')





     threshs = np.arange(100)
     att_percent = []
     noatt_percent = []
     ratios = []

     for thresh in threshs:
         tail_rate_att = np.sum(avs_att[avs_att>=thresh])
         tail_rate_noatt = np.sum(avs_noatt[avs_noatt>=thresh])
         print('tail_rate_att,tail_rate_noatt,ratio',tail_rate_att,tail_rate_noatt,tail_rate_att/tail_rate_noatt)
         att_percent.append(tail_rate_att/(tail_rate_att+tail_rate_noatt))
         noatt_percent.append(tail_rate_noatt/(tail_rate_att+tail_rate_noatt))
         ratios.append(tail_rate_att/tail_rate_noatt)

     plt.figure(figsize=(1.2*2.4,1.2*1.6))
     plt.plot(threshs[1:10],ratios[1:10],label='c_att/c_noatt',color='k')
     plt.xlabel('threshold')
     plt.ylabel('c_att/c_noatt')
     plt.legend()
     plt.vlines([5],ymin=1.3,ymax=1.6,color='k',linestyle='dashed')
     plt.title('V4 flicker correlation ratios')
     plt.savefig('img_cosyne/threshold_correlation_ratios.png',dpi=200)
     plt.savefig('img_cosyne/threshold_correlation_ratios.svg')

    #+end_src 

    #+RESULTS:
    :RESULTS:
    #+begin_example
      tail_rate_att,tail_rate_noatt,ratio 260558 199862 1.3036895457865927
      tail_rate_att,tail_rate_noatt,ratio 260558 199862 1.3036895457865927
      tail_rate_att,tail_rate_noatt,ratio 219597 159994 1.3725327199769992
      tail_rate_att,tail_rate_noatt,ratio 190559 132560 1.4375301750150875
      tail_rate_att,tail_rate_noatt,ratio 167960 111773 1.502688484696662
      tail_rate_att,tail_rate_noatt,ratio 149472 95165 1.570661482687963
      tail_rate_att,tail_rate_noatt,ratio 133687 81325 1.6438610513372272
      tail_rate_att,tail_rate_noatt,ratio 120061 69547 1.726328957395718
      tail_rate_att,tail_rate_noatt,ratio 107657 59747 1.8018812660049877
      tail_rate_att,tail_rate_noatt,ratio 96545 51027 1.8920375487487016
      tail_rate_att,tail_rate_noatt,ratio 86834 43791 1.9829188646068827
      tail_rate_att,tail_rate_noatt,ratio 78814 37651 2.0932777349871183
      tail_rate_att,tail_rate_noatt,ratio 71422 31733 2.2507169192953707
      tail_rate_att,tail_rate_noatt,ratio 64702 26801 2.414163650610052
      tail_rate_att,tail_rate_noatt,ratio 58111 23044 2.5217410171845165
      tail_rate_att,tail_rate_noatt,ratio 52875 20132 2.6264156566660044
      tail_rate_att,tail_rate_noatt,ratio 48015 17417 2.756789343744617
      tail_rate_att,tail_rate_noatt,ratio 43311 15065 2.8749419183538003
      tail_rate_att,tail_rate_noatt,ratio 38704 12770 3.030853563038371
      tail_rate_att,tail_rate_noatt,ratio 34672 10682 3.2458341134618984
      tail_rate_att,tail_rate_noatt,ratio 30872 9124 3.383603682595353
      tail_rate_att,tail_rate_noatt,ratio 27592 7444 3.706609349811929
      tail_rate_att,tail_rate_noatt,ratio 24379 6079 4.0103635466359595
      tail_rate_att,tail_rate_noatt,ratio 22135 4847 4.566742314833918
      tail_rate_att,tail_rate_noatt,ratio 19927 4134 4.820270924044509
      tail_rate_att,tail_rate_noatt,ratio 18055 3390 5.325958702064897
      tail_rate_att,tail_rate_noatt,ratio 16005 2665 6.00562851782364
      tail_rate_att,tail_rate_noatt,ratio 14185 2041 6.950024497795199
      tail_rate_att,tail_rate_noatt,ratio 12430 1663 7.4744437763078775
      tail_rate_att,tail_rate_noatt,ratio 10974 1271 8.634146341463415
      tail_rate_att,tail_rate_noatt,ratio 9756 981 9.944954128440367
      tail_rate_att,tail_rate_noatt,ratio 8556 861 9.937282229965156
      tail_rate_att,tail_rate_noatt,ratio 7440 706 10.538243626062323
      tail_rate_att,tail_rate_noatt,ratio 6736 578 11.653979238754324
      tail_rate_att,tail_rate_noatt,ratio 5911 479 12.340292275574113
      tail_rate_att,tail_rate_noatt,ratio 5027 411 12.231143552311435
      tail_rate_att,tail_rate_noatt,ratio 4362 306 14.254901960784315
      tail_rate_att,tail_rate_noatt,ratio 3750 234 16.025641025641026
      tail_rate_att,tail_rate_noatt,ratio 3195 160 19.96875
      tail_rate_att,tail_rate_noatt,ratio 2549 122 20.89344262295082
      tail_rate_att,tail_rate_noatt,ratio 2198 83 26.481927710843372
      tail_rate_att,tail_rate_noatt,ratio 1958 43 45.53488372093023
      tail_rate_att,tail_rate_noatt,ratio 1630 43 37.906976744186046
      tail_rate_att,tail_rate_noatt,ratio 1336 43 31.069767441860463
      tail_rate_att,tail_rate_noatt,ratio 1121 0 inf
      tail_rate_att,tail_rate_noatt,ratio 989 0 inf
      tail_rate_att,tail_rate_noatt,ratio 809 0 inf
      tail_rate_att,tail_rate_noatt,ratio 625 0 inf
      tail_rate_att,tail_rate_noatt,ratio 484 0 inf
      tail_rate_att,tail_rate_noatt,ratio 436 0 inf
      tail_rate_att,tail_rate_noatt,ratio 387 0 inf
      tail_rate_att,tail_rate_noatt,ratio 337 0 inf
      tail_rate_att,tail_rate_noatt,ratio 286 0 inf
      tail_rate_att,tail_rate_noatt,ratio 182 0 inf
      tail_rate_att,tail_rate_noatt,ratio 182 0 inf
      tail_rate_att,tail_rate_noatt,ratio 182 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 127 0 inf
      tail_rate_att,tail_rate_noatt,ratio 64 0 inf
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      tail_rate_att,tail_rate_noatt,ratio 0 0 nan
      <ipython-input-68-5d9bb31f0833>:73: RuntimeWarning: divide by zero encountered in long_scalars
        print('tail_rate_att,tail_rate_noatt,ratio',tail_rate_att,tail_rate_noatt,tail_rate_att/tail_rate_noatt)
      <ipython-input-68-5d9bb31f0833>:76: RuntimeWarning: divide by zero encountered in long_scalars
        ratios.append(tail_rate_att/tail_rate_noatt)
      <ipython-input-68-5d9bb31f0833>:73: RuntimeWarning: invalid value encountered in long_scalars
        print('tail_rate_att,tail_rate_noatt,ratio',tail_rate_att,tail_rate_noatt,tail_rate_att/tail_rate_noatt)
      <ipython-input-68-5d9bb31f0833>:74: RuntimeWarning: invalid value encountered in long_scalars
        att_percent.append(tail_rate_att/(tail_rate_att+tail_rate_noatt))
      <ipython-input-68-5d9bb31f0833>:75: RuntimeWarning: invalid value encountered in long_scalars
        noatt_percent.append(tail_rate_noatt/(tail_rate_att+tail_rate_noatt))
      <ipython-input-68-5d9bb31f0833>:76: RuntimeWarning: invalid value encountered in long_scalars
        ratios.append(tail_rate_att/tail_rate_noatt)
    #+end_example
    [[file:./.ob-jupyter/e5471d508a4e44348172754d5ec249b766294ba2.png]]
    [[file:./.ob-jupyter/3e4fc5e2afa989d85229af2eb513e76c3bf45523.png]]
    [[file:./.ob-jupyter/ea7be57cedfe0e50558309a030b437732cfa7b54.png]]
    :END:

** TODO absolute Correlations flicker V4 are very wrong!!!!!
   - [ ] compute correlations for sampled avalanche data to check analytical calculation
   - [ ] compute correlations without binning in experiments
** TODO What about additive and not multiplicative effects?
   - [ ] run simulation with additive C population and additive ext.
** STARTED Information theoretic analysis of routing
   - use dit to compute mutual informations in addition to correlations
   #+begin_src python :session rbs :results raw drawer 
     import dit

     # Example: Create and distribution
     pmf = np.zeros((2,2,2))
     pmf[0,0,0] = 1/4
     pmf[0,1,0] = 1/4
     pmf[1,0,0] = 1/4
     pmf[1,1,1] = 1/4

     d = dit.Distribution.from_ndarray(pmf)


     xs = np.linspace(-1,1,5)
     ys = np.linspace(-1,1,5)
     zs = np.linspace(-3,3,13)
     # z = 2*x+y
     # -> z can take values from -3 to 3

     pmf = np.zeros((5,5,13))
     for ix,x in enumerate(xs):
         for iy,y in enumerate(ys):
             z = 2*x+y
             iz = list(zs).index(z)
             pmf[ix,iy,iz] = 1/25

     d = dit.Distribution.from_ndarray(pmf)

     xl = []
     yl = []
     zl = []
     for x in (xs):
         for y in (ys):
             xl.append(x)
             yl.append(y)
             zl.append(2*x+y)


     print(np.corrcoef(xl,zl)[0,1])
     print(np.corrcoef(yl,zl)[0,1])
   #+end_src 

   Precompute all detailed dists
   #+begin_src python :session rbs :results raw drawer :eval no
     import pickle
     xs = np.linspace(-1,1,5)
     ys = np.linspace(-1,1,5)
     zs = np.arange(101)
     MI_fA4s = []
     MI_fB4s = []
     corr_fA4s = []
     corr_fB4s = []
     c1_idxs = np.arange(80)#[0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
     dist_vols = []
     for c1_idx in c1_idxs:
         c1 = np.linspace(0.2,1,80)[c1_idx]
         print('c1',c1)
         c3 = 0.4#'auto'
         cup=0.3;
         c_control = 0.095
         n=100;
         n2 = 10
         c=0.25
         dt=1e-6
         rate1=40
         ac = 1-1/np.sqrt(n)
         ra1 = 1/(n-n*ac*c1) # rate ana 1 step
         norm = ((2*n+n2)*(1+c))
         deltaU = dt*n*rate1/(ra1*norm)

         import time
         st = time.time()
         dist_vol = detailed_avs_dist_3pop(c1,c3,cup,c_control,n,return_vol=True)
         print(time.time()-st)
         dist_vols.append(dist_vol)

     pickle.dump(dist_vols,open('dist_vols_n=100.pickle','wb'))

   #+end_src 


   Done:
   construct pmf of flicker to V4
   #+begin_src python :session rbs :results raw drawer 
     xs = np.linspace(-1,1,5)#5)
     ys = np.linspace(-1,1,5)#5)
     zs = np.arange(101)
     MI_fA4s = []
     MI_fB4s = []
     CIs = []
     corr_fA4s = []
     corr_fB4s = []
     bcorr_fA4s = []
     bcorr_fB4s = []
     thresholds = [5]#range(1,21)
     #thresholds = [2,5,10]
     #thresholds = [5]
     #thresholds = [2,5,10]
     c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
     c1_idxs = np.arange(80)
     pmfs = np.zeros((5,5,101,100))
     #c1_idxs = [55]
     dist_vols = pickle.load(open('dist_vols_n=100.pickle','rb'))
     # for c1_idx in c1_idxs:
     #     MI_fA4s.append([])
     #     MI_fB4s.append([])
     #     CIs.append([])
     #     corr_fA4s.append([])
     #     corr_fB4s.append([])
     #     bcorr_fA4s.append([])
     #     bcorr_fB4s.append([])
     #     c1 = np.linspace(0.2,1,80)[c1_idx]
     #     print('c1',c1)
     #     c3 = 0.4#'auto'
     #     cup=0.3;
     #     c_control = 0.095
     #     n=100;
     #     n2 = 10
     #     c=0.1#0.25
     #     dt=1e-6
     #     rate1=40
     #     ac = 1-1/np.sqrt(n)
     #     ra1 = 1/(n-n*ac*c1) # rate ana 1 step
     #     norm = ((2*n+n2)*(1+c))
     #     #deltaU = dt*n*rate1/(ra1*norm)
     #     p0 = 0.266666666 # taken from simulation code!
     #     deltaU = (n*(1-ac*c1))*rate1*dt/p0 # solve for condition V1 has rate1 firing rate
     #     #deltaU = deltaU/100
     #     #deltaU = dt*p0*rate1/ra1
     #     #detailed_dist,vol = detailed_avs_dist_3pop(c1,c3,cup,c_control,n,return_vol=True)
     #     detailed_dist,vol = dist_vols[c1_idx]
     #     v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist

     #     #deltaU = 0.005293874
     #     n = 100
     #     n2 = 10
     #     # das ist jetzt der wichtigste schritt....

     #     #Probability of non-empty avalanche : p*DeltaU*p_Vxxx/vol_hypercube
     #     # vol_hypercube = vnonin_mod(NS,np.ones_like(Ns),W)

     #     # I want to have the whole distribution
     #     # shape (5,5,100,100,10,100)
     #     # This is too much!

     #     # V4=0 √ºber weg A with prob pext_A
     #     #*(1-deltaU*pVxxxx/vol)
     #     # V4=x √ºber ................ pext_A*deltaU*Vxxx/vol !!!!

     #     # hier n2 durch n ersetzt! Faktor steckt schon in den p_ drin!
     #     # und dann m√ºssen alle p_ durch n geteilt werden!!!!!
     #     # equivalent n2 behalten und entsprechende p durch 10 teilen! Das ergibt sogar einen Sinn!!!!!
     #     # beim Aufschreiben einfach vol0_st als Mittelwert und nicht als Summe definieren

     #     # TODO Do also flicker->V1
     #     # AND V1->V4 analytically!!!!!
     #     for s in thresholds:
     #         print('s',s)
     #         norm = ((2*n+n)*(1+c))
     #         p_base = np.array([n,n,n,0])/norm
     #         pmf = np.zeros((len(xs),len(ys),101))
     #         # TODO s einbauen!!!!!
     #         #s=10
     #         #spmf = np.zeros((len(xs),len(ys),100001))
     #         from functools import reduce

     #         for ia,lA in enumerate(xs):
     #             for ib,lB in enumerate(ys):
     #                 print(ia,ib)
     #                 p = p_base.copy()
     #                 p[0] = p[0]*(1+c*lA)
     #                 p[1:3] = p[1:3]*(1+c*lB)
     #                 pmf[ia,ib,0] = 1-np.sum(p) # Prob no unit gets selected! TODO
     #                 # Jetzt √ºber alle Wege im Netzwerk summieren!
     #                 # Weg V1a zu V4
     #                 pmf[ia,ib,0] += p[0]*(1-deltaU*(p_V1aV4-np.sum(v_V1aV4[:s,:]))/vol/n)  
     #                 pmf[ia,ib,:] += p[0]*deltaU*np.sum(v_V1aV4[s:,:],axis=0)/vol/n 
     #                 # Weg V1b zu V4
     #                 pmf[ia,ib,0] += p[1]*(1-deltaU*(p_V1bV4-np.sum(v_V1bV4[:s,:]))/vol/n)  
     #                 pmf[ia,ib,:] += p[1]*deltaU*np.sum(v_V1bV4[s:,:],axis=0)/vol/n 
     #                 # Weg V1bc zu V4
     #                 pmf[ia,ib,0] += p[2]*(1-deltaU*(p_V1bcV4-np.sum(v_V1bcV4[:,:s,:]))/vol/n)  
     #                 pmf[ia,ib,:] += p[2]*deltaU*np.sum(v_V1bcV4[:,s:,:],axis=(0,1))/vol/n

     #                 print('sum pmf',np.sum(pmf[ia,ib,:]))
     #                 #spmf[ia,ib,:] = reduce(lambda x,y: np.convolve(x,y,mode='full'),[pmf[ia,ib,:]]*1000)
     #                 #print('sum spmf',np.sum(spmf[ia,ib,:]))
     #         pmf = pmf/len(xs)**2
     #         #spmf = spmf/len(xs)**2
     #         pmfs[:,:,:,c1_idx] = pmf

     #         # sd_fA4 = np.zeros((5,100001))
     #         # for ia in range(len(xs)):
     #         #     sd_fA4[ia,:] += np.sum(spmf[ia,:,:],axis=0)

     #         # sd_fA = sd_fA4.sum(axis=1)
     #         # sd_4 = sd_fA4.sum(axis=0)

     #         sMI_fA4 = []
     #         for i in range(len(xs)):
     #             for j in range(sd_4.shape[0]):
     #                 sMI_fA4.append(sd_fA4[i,j]*(np.log2(sd_fA4[i,j]) - np.log2(sd_fA[i]) - np.log2(sd_4[j])))
     #         print(np.nansum(np.array(sMI_fA4)))
     #         # compute pmf of 1ms resolved rates
     #         # for ia in range(len(xs)):
     #         #     for ib in range(len(ys)):
     #         #         spmf[ia,ib,:] = reduce(lambda x,y: np.convolve(x,y,mode='full'),[pmf[ia,ib,:]]*1000)

     #         # import dit
     #         # sd = dit.Distribution.from_ndarray(np.log2(spmf),base=2)        
     #         # TODO Compute pid with admUI_numpy!
     #         # generate conditional distributions with dit in base2 and then convert to linear numpy arrays!
     #         import dit
     #         d = dit.Distribution.from_ndarray(np.log2(pmf),base=2)
     #         #print(dit.shannon.mutual_information(d,[0],[2]))
     #         MI_fA4s[-1].append(dit.shannon.mutual_information(d,[0],[2]))
     #         #print(dit.shannon.mutual_information(d,[1],[2]))
     #         MI_fB4s[-1].append(dit.shannon.mutual_information(d,[1],[2]))
     #         #print(dit.shannon.mutual_information(d,[1],[2])/dit.shannon.mutual_information(d,[0],[2]))
     #         CIs[-1].append(dit.shannon.mutual_information(d,[2],[0,1])-
     #                        dit.shannon.mutual_information(d,[0],[2])-
     #                        dit.shannon.mutual_information(d,[1],[2]))
     #         print('synergistic_information',CIs[-1][-1])



     #         # compute correlation coefficient
     #         d_fA4 = d.marginal([0,2])
     #         d_fB4 = d.marginal([1,2])
     #         d_fA = d.marginal([0])
     #         d_fB = d.marginal([1])
     #         d_4 = d.marginal([2])
     #         E_fA = 0#np.sum([xs*np.exp2(d_fA[i,]) for i in range(5)])
     #         E_fB = 0
     #         E_fA2 = np.sum([x**2*np.exp2(d_fA[i,]) for i,x in enumerate(xs)])
     #         E_fB2 = np.sum([x**2*np.exp2(d_fB[i,]) for i,x in enumerate(xs)])
     #         E_4 = np.sum([np.exp2(d_4[i,])*i for i in range(101)])
     #         E_42 = np.sum([np.exp2(d_4[i,])*i**2 for i in range(101)])
     #         E_fA4 = np.sum([np.exp2(d_fA4[i,j])*lA*nV4 for i,lA in enumerate(xs) for j,nV4 in enumerate(range(101))])
     #         E_fB4 = np.sum([np.exp2(d_fB4[i,j])*lB*nV4 for i,lB in enumerate(ys) for j,nV4 in enumerate(range(101))])
     #         corr_fA4 = (E_fA4-E_fA*E_4)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(E_42-E_4**2))
     #         corr_fB4 = (E_fB4-E_fB*E_4)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(E_42-E_4**2))
     #         corr_fA4s[-1].append(corr_fA4)
     #         corr_fB4s[-1].append(corr_fB4)
     #         # print(corr_fA4)
     #         # print(corr_fB4)
     #         # print(corr_fB4/corr_fA4)
     #         num_binned = 1000#100
     #         bvE_4 = (E_42-E_4**2)/num_binned
     #         bE_42 = E_4**2+bvE_4
     #         bcorr_fA4 = (E_fA4-E_fA*E_4)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(bE_42-E_4**2))
     #         bcorr_fB4 = (E_fB4-E_fB*E_4)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(bE_42-E_4**2))
     #         bcorr_fA4s[-1].append(bcorr_fA4)
     #         bcorr_fB4s[-1].append(bcorr_fB4)

     #         p4,pfagv4 = d_fA4.condition_on([1])
     #         p4,pfbgv4 = d_fB4.condition_on([1])
     #         pfafb = d.marginal([0,1])
     #         # Test for vanishing mutual information of fa,fb in Q0
     #         # for fav in range(5):
     #         #     for fbv in range(5):
     #         #         print(np.sum([np.exp2(d_4[nv4,]+pfagv4[nv4][fav,]+pfbgv4[nv4][fbv,])
     #         #                       for nv4 in range(101)]),np.exp2(pfafb[fav,fbv]))
     #         # within numerical precision, works out!
     #         # we have sum_nv p(n4)p(fa|nv),p(fb|nv) = p(fa,fb) = 1/25



     # acs = np.array(bcorr_fA4s)
     # bcs = np.array(bcorr_fB4s)
     # pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_corrs_mis_c1_thresholds_c01.pickle','wb'))
     thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
     # acs = np.array(bcorr_fA4s)
     # bcs = np.array(bcorr_fB4s)
     c1s = np.linspace(0.2,1,80)

     plt.figure(figsize=(6,4),dpi=200)
     plt.title('Effect of synchrony gain')
     plt.plot(c1s,acs[:,2],color='blue',label='C(f_A,r_4) s=2')
     plt.plot(c1s,bcs[:,2],color='blue',label='C(f_B,r_4) s=2')
     plt.plot(c1s,acs[:,5],color='green',label='C(f_A,r_4) s=2')
     plt.plot(c1s,bcs[:,5],color='green',label='C(f_B,r_4) s=2')
     plt.plot(c1s,acs[:,10],color='red',label='C(f_A,r_4) s=2')
     plt.plot(c1s,bcs[:,10],color='red',label='C(f_B,r_4) s=2')
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     #plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
     #plt.savefig('img/ana_synchrony_gain_2510.svg')

     mi = min(acs.min(),bcs.min())
     ma = max(acs.max(),bcs.max())
     plt.figure(dpi=200)
     plt.subplot(2,1,1)
     plt.title('correlation attended flicker and V4')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.subplot(2,1,2)

     plt.title('correlation non-attended flicker and V4')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.tight_layout()
     # plt.savefig('img/correlations_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/correlations_alpha_thresholds_ana.svg')


     acs = np.array(MI_fA4s)
     bcs = np.array(MI_fB4s)
     #pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_mis_c1_thresholds.pickle','wb'))
     mi = -30
     ma = -10
     plt.figure(dpi=200)
     plt.subplot(2,1,1)
     plt.title('Mutual information flicker and V4')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(bcs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.subplot(2,1,2)

     plt.title('Mutual information flicker and V4')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(acs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.tight_layout()
     # plt.savefig('img/MI_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/MI_alpha_thresholds_ana.svg')



     # correlation ratios sind schon in einem extra plot -> dort den grenzfall s=0 mit rein nehmen!

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fB4s))
     # #plt.figure(dpi=200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fA4s)*200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fB4s)*200)

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fB4s))




     # THIS IS VERY NICE!!!!!!!!!!!!!
     # TODO: Add binning to correlation computation and check if numerics match analytics!
     # TODO: refactor to good function
     # TODO: add flicker V1 and V1 V4 analysis
     # TODO: compute unique informations!
     # TODO: check if distribution is really correct -> compute V1 rate and compare to 40/52Hz
     #from scipy.io import savemat
     #savemat('pmfs.mat',{'pmfs':pmfs})


     pmf_q0 = np.zeros_like(pmf)
     for i in range(5):
         for j in range(5):
             for k in range(101):
                 pmf_q0[i,j,k] = d_fA4[i,k]+d_fB4[j,k]-d_4[k,]

     d_q0 = dit.Distribution.from_ndarray(pmf_q0,base=2)
     print('q0indep',dit.shannon.mutual_information(d_q0,[0],[1]))



     pv4,pfafbgv4 = d.condition_on([2])

     idx = 20
     cond = pfafbgv4[idx]
     mfa = cond.marginal([0])
     mfb = cond.marginal([1])

     for i in range(5):
         for j in range(5):
             print(i,j,np.exp2(mfa[i,])*np.exp2(mfb[j,]),np.exp2(cond[i,j]))



     p4,pfagv4 = d_fA4.condition_on([1])
     p4,pfbgv4 = d_fB4.condition_on([1])
     pfafb = d.marginal([0,1])
     for fav in range(5):
         for fbv in range(5):
             print(np.sum([np.exp2(d_4[nv4,]+pfagv4[nv4][fav,]+pfbgv4[nv4][fbv,]) for nv4 in range(101)]),np.exp2(pfafb[fav,fbv]))


     acs = np.array(MI_fA4s)
     bcs = np.array(MI_fB4s)
     cis = np.array(CIs)

     plt.figure(dpi=200)
     plt.subplot(3,1,1)
     plt.plot(np.linspace(0.2,1,80),acs[:,0])
     plt.plot(np.linspace(0.2,1,80),acs[:,1])
     plt.plot(np.linspace(0.2,1,80),acs[:,2])
     plt.subplot(3,1,2)
     plt.plot(np.linspace(0.2,1,80),bcs[:,0])
     plt.plot(np.linspace(0.2,1,80),bcs[:,1])
     plt.plot(np.linspace(0.2,1,80),bcs[:,2])
     plt.figure(dpi=200)
     plt.subplot(3,1,1)
     plt.plot(np.linspace(0.2,1,80),cis[:,0])
     plt.plot(np.linspace(0.2,1,80),(acs[:,0]+bcs[:,0])/300)
     plt.subplot(3,1,2)
     plt.plot(np.linspace(0.2,1,80),cis[:,1])
     plt.plot(np.linspace(0.2,1,80),(acs[:,1]+bcs[:,1])/300)
     plt.subplot(3,1,3)
     plt.plot(np.linspace(0.2,1,80),cis[:,2])
     plt.plot(np.linspace(0.2,1,80),(acs[:,2]+bcs[:,2])/300)
     plt.figure(dpi=200)
     plt.plot(np.linspace(0.2,1,80),(acs[:,0]+bcs[:,0]+cis[:,0]))
     plt.plot(np.linspace(0.2,1,80),(acs[:,1]+bcs[:,1]+cis[:,1]))
     plt.plot(np.linspace(0.2,1,80),(acs[:,2]+bcs[:,2]+cis[:,2]))


     plt.figure(dpi=200)
     sidx=0
     plt.plot(np.linspace(0.2,1,80),(acs[:,sidx]+bcs[:,sidx]+cis[:,sidx]),color='black',label='I(n_4,(f_A,f_B))')
     plt.plot(np.linspace(0.2,1,80),(acs[:,sidx]),color='blue',label='IUNQ(n_4:f_B \ f_A)')
     plt.plot(np.linspace(0.2,1,80),(bcs[:,sidx]),color='red',label='IUNQ(n_4:f_A \ f_B)')
     plt.plot(np.linspace(0.2,1,80),[0]*80,color='brown',label='ISHD(n_4:f_A,f_B)')
     ax2 = plt.gca()#.twinx()
     ax2.plot(np.linspace(0.2,1,80),(200*cis[:,sidx]),color='gray',label='200*ISYN(n_4:f_A,f_B)',linestyle='dashed')
     #ax2.set_ylim([1e-8,6e-8])
     #ax2.set_ylabel('', color='red')
     #ax2.tick_params(axis='y', labelcolor='red')
     plt.legend()
     #plt.tight_layout()
     # plt.savefig('pid_plot_c01.png',dpi=200)
     # plt.savefig('pid_plot_c01.svg')
     # pickle.dump([acs,bcs,cis],open('acs_bcs_cis_c01.pickle','wb'))




   #+end_src 

   flicker to V4 analysis with arbitrary nv4
   #+begin_src python :session rbs :results raw drawer 
     # TODO Hier versteckt sich noch irgendwo ein faktor von 10
     # f√ºr die correlation auf dt timescale
     # timescale angleich funktioniert perfekt!!!
     # deltaU ist korrekt berechnet
     # TODO Check if expected firing rate matches the real one!!!!!!!!!
     # HIER lieght auch wirklich der Unterschied: c1_idx=55 s=5 expected V4 rate
     # np.sum([np.sum([i*s for i,s in enumerate(pmf[i,j,:])]) for i in range(5) for j in range(5)])/100/dt = 2615
     # actual rate in simulation: 26.14
     # dieser Faktor n=100 wird gebraucht!


     # TODO pmf with binary V4 aus dem normalen herleiten. Sollte einfacher sein als komplett neu zu berechnen:
     # einzelnen ehe unit gleichverteilt auf (w_44,1) -> einfachheitshalber gerne auch (0,1)
     # Wahrscheinlichkeit dass V4 feuert ist dann einfach input_drive/(1-w_44)
     xs = np.linspace(-1,1,2)#5)
     ys = np.linspace(-1,1,2)#5)
     zs = np.arange(101)
     MI_fA4s = []
     MI_fB4s = []
     corr_fA4s = []
     corr_fB4s = []
     bcorr_fA4s = []
     bcorr_fB4s = []
     thresholds = range(1,21)
     thresholds = [5]
     #thresholds = [2,5,10]
     c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
     c1_idxs = np.arange(80)
     #c1_idxs = [55]
     c1_idxs = [55]
     #dist_vols = pickle.load(open('dist_vols_n=100.pickle','rb'))
     for c1_idx in c1_idxs:
         MI_fA4s.append([])
         MI_fB4s.append([])
         corr_fA4s.append([])
         corr_fB4s.append([])
         bcorr_fA4s.append([])
         bcorr_fB4s.append([])
         c1 = np.linspace(0.2,1,80)[c1_idx]
         print('c1',c1)
         c3 = 0.4#'auto'
         cup=0.3;
         c_control = 0.095
         n=100;
         n2 = 10
         c=0.25
         dt=1e-6
         rate1=40
         ac = 1-1/np.sqrt(n)
         ra1 = 1/(n-n*ac*c1) # rate ana 1 step
         norm = ((2*n+n2)*(1+c))
         #deltaU = dt*n*rate1/(ra1*norm)
         p0 = 0.266666666 # taken from simulation code!
         deltaU = (n*(1-ac*c1))*rate1*dt/p0 # solve for condition V1 has rate1 firing rate
         #deltaU = deltaU/100
         #deltaU = dt*p0*rate1/ra1
         nv4 = 1
         detailed_dist,vol = detailed_avs_dist_3pop_nv4(c1,c3,cup,c_control,n,nv4=nv4,return_vol=True)
         #etailed_dist,vol = dist_vols[c1_idx]
         v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist

         #deltaU = 0.005293874
         n = 100
         n2 = 10
         # das ist jetzt der wichtigste schritt....

         #Probability of non-empty avalanche : p*DeltaU*p_Vxxx/vol_hypercube
         # vol_hypercube = vnonin_mod(NS,np.ones_like(Ns),W)

         # I want to have the whole distribution
         # shape (5,5,100,100,10,100)
         # This is too much!

         # V4=0 √ºber weg A with prob pext_A
         #*(1-deltaU*pVxxxx/vol)
         # V4=x √ºber ................ pext_A*deltaU*Vxxx/vol !!!!

         # hier n2 durch n ersetzt! Faktor steckt schon in den p_ drin!
         # und dann m√ºssen alle p_ durch n geteilt werden!!!!!
         # equivalent n2 behalten und entsprechende p durch 10 teilen! Das ergibt sogar einen Sinn!!!!!
         # beim Aufschreiben einfach vol0_st als Mittelwert und nicht als Summe definieren

         # TODO Do also flicker->V1
         # AND V1->V4 analytically!!!!!
         for s in thresholds:
             print('s',s)
             norm = ((2*n+n)*(1+c))
             p_base = np.array([n,n,n,0])/norm
             pmf = np.zeros((len(xs),len(ys),nv4+1))
             # TODO s einbauen!!!!!
             #s=10
             for ia,lA in enumerate(xs):
                 for ib,lB in enumerate(ys):
                     p = p_base.copy()
                     p[0] = p[0]*(1+c*lA)
                     p[1:3] = p[1:3]*(1+c*lB)
                     pmf[ia,ib,0] = 1-np.sum(p) # Prob no unit gets selected! TODO
                     # Jetzt √ºber alle Wege im Netzwerk summieren!
                     # Weg V1a zu V4
                     pmf[ia,ib,0] += p[0]*(1-deltaU*(p_V1aV4-np.sum(v_V1aV4[:s,:]))/vol/n)  
                     pmf[ia,ib,:] += p[0]*deltaU*np.sum(v_V1aV4[s:,:],axis=0)/vol/n 
                     # Weg V1b zu V4
                     pmf[ia,ib,0] += p[1]*(1-deltaU*(p_V1bV4-np.sum(v_V1bV4[:s,:]))/vol/n)  
                     pmf[ia,ib,:] += p[1]*deltaU*np.sum(v_V1bV4[s:,:],axis=0)/vol/n 
                     # Weg V1bc zu V4
                     pmf[ia,ib,0] += p[2]*(1-deltaU*(p_V1bcV4-np.sum(v_V1bcV4[:,:s,:]))/vol/n)  
                     pmf[ia,ib,:] += p[2]*deltaU*np.sum(v_V1bcV4[:,s:,:],axis=(0,1))/vol/n 
             pmf = pmf/len(xs)**2

             # TODO Compute pid with admUI_numpy!
             # generate conditional distributions with dit in base2 and then convert to linear numpy arrays!
             import dit
             d = dit.Distribution.from_ndarray(np.log2(pmf),base=2)
             #print(dit.shannon.mutual_information(d,[0],[2]))
             MI_fA4s[-1].append(dit.shannon.mutual_information(d,[0],[2]))
             #print(dit.shannon.mutual_information(d,[1],[2]))
             MI_fB4s[-1].append(dit.shannon.mutual_information(d,[1],[2]))
             #print(dit.shannon.mutual_information(d,[1],[2])/dit.shannon.mutual_information(d,[0],[2]))


             # compute correlation coefficient
             d_fA4 = d.marginal([0,2])
             d_fB4 = d.marginal([1,2])
             d_fA = d.marginal([0])
             d_fB = d.marginal([1])
             d_4 = d.marginal([2])
             E_fA = 0#np.sum([xs*np.exp2(d_fA[i,]) for i in range(5)])
             E_fB = 0
             E_fA2 = np.sum([x**2*np.exp2(d_fA[i,]) for i,x in enumerate(xs)])
             E_fB2 = np.sum([x**2*np.exp2(d_fB[i,]) for i,x in enumerate(xs)])
             E_4 = np.sum([np.exp2(d_4[i,])*i for i in range(nv4+1)])
             E_42 = np.sum([np.exp2(d_4[i,])*i**2 for i in range(nv4+1)])
             E_fA4 = np.sum([np.exp2(d_fA4[i,j])*lA*nV4 for i,lA in enumerate(xs) for j,nV4 in enumerate(range(nv4+1))])
             E_fB4 = np.sum([np.exp2(d_fB4[i,j])*lB*nV4 for i,lB in enumerate(ys) for j,nV4 in enumerate(range(nv4+1))])
             corr_fA4 = (E_fA4-E_fA*E_4)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(E_42-E_4**2))
             corr_fB4 = (E_fB4-E_fB*E_4)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(E_42-E_4**2))
             corr_fA4s[-1].append(corr_fA4)
             corr_fB4s[-1].append(corr_fB4)
             # print(corr_fA4)
             # print(corr_fB4)
             # print(corr_fB4/corr_fA4)
             num_binned = 1000#100
             bvE_4 = (E_42-E_4**2)/num_binned
             bE_42 = E_4**2+bvE_4
             bcorr_fA4 = (E_fA4-E_fA*E_4)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(bE_42-E_4**2))
             bcorr_fB4 = (E_fB4-E_fB*E_4)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(bE_42-E_4**2))
             bcorr_fA4s[-1].append(bcorr_fA4)
             bcorr_fB4s[-1].append(bcorr_fB4)

             p4,pfagv4 = d_fA4.condition_on([1])
             p4,pfbgv4 = d_fB4.condition_on([1])
             pfafb = d.marginal([0,1])
             # Test for vanishing mutual information of fa,fb in Q0
             for fav in range(len(xs)):
                 for fbv in range(len(ys)):
                     print(np.sum([np.exp2(d_4[n4,]+pfagv4[n4][fav,]+pfbgv4[n4][fbv,])
                                   for n4 in range(nv4+1)]),np.exp2(pfafb[fav,fbv]))
             # within numerical precision, works out!
             # we have sum_nv p(n4)p(fa|nv),p(fb|nv) = p(fa,fb) = 1/25

             pv4,pfafbgv4 = d.condition_on([2])
             for idx in [0,1]:
                 cond = pfafbgv4[idx]
                 mfa = cond.marginal([0])
                 mfb = cond.marginal([1])

                 for i in range(len(xs)):
                     for j in range(len(ys)):
                         print(i,j,np.exp2(mfa[i,])*np.exp2(mfb[j,]),np.exp2(cond[i,j]))



     #pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_corrs_mis_c1_thresholds.pickle','wb'))
     #thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
     acs = np.array(bcorr_fA4s)
     bcs = np.array(bcorr_fB4s)

     # plt.figure(figsize=(6,4),dpi=200)
     # plt.title('Effect of synchrony gain')
     # plt.plot(c1s,acs[:,2],color='blue',label='C(f_A,r_4) s=2')
     # plt.plot(c1s,bcs[:,2],color='blue',label='C(f_B,r_4) s=2')
     # plt.plot(c1s,acs[:,5],color='green',label='C(f_A,r_4) s=2')
     # plt.plot(c1s,bcs[:,5],color='green',label='C(f_B,r_4) s=2')
     # plt.plot(c1s,acs[:,10],color='red',label='C(f_A,r_4) s=2')
     # plt.plot(c1s,bcs[:,10],color='red',label='C(f_B,r_4) s=2')
     # plt.legend()
     # plt.xlabel('c1')
     # plt.ylabel('correlation')
     # plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
     # plt.savefig('img/ana_synchrony_gain_2510.svg')

     # mi = min(acs.min(),bcs.min())
     # ma = max(acs.max(),bcs.max())
     # plt.figure(dpi=200)
     # plt.subplot(2,1,1)
     # plt.title('correlation attended flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.subplot(2,1,2)

     # plt.title('correlation non-attended flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.tight_layout()
     # # plt.savefig('img/correlations_alpha_thresholds_ana.png',dpi=200)
     # # plt.savefig('img/correlations_alpha_thresholds_ana.svg')


     acs = np.array(MI_fA4s)
     bcs = np.array(MI_fB4s)
     # #pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_mis_c1_thresholds.pickle','wb'))
     # mi = -30
     # ma = -10
     # plt.figure(dpi=200)
     # plt.subplot(2,1,1)
     # plt.title('Mutual information flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(bcs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.subplot(2,1,2)

     # plt.title('Mutual information flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(acs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.tight_layout()
     # # plt.savefig('img/MI_alpha_thresholds_ana.png',dpi=200)
     # # plt.savefig('img/MI_alpha_thresholds_ana.svg')



     # correlation ratios sind schon in einem extra plot -> dort den grenzfall s=0 mit rein nehmen!

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fB4s))
     # #plt.figure(dpi=200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fA4s)*200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fB4s)*200)

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fB4s))




     # THIS IS VERY NICE!!!!!!!!!!!!!
     # TODO: Add binning to correlation computation and check if numerics match analytics!
     # TODO: refactor to good function
     # TODO: add flicker V1 and V1 V4 analysis
     # TODO: compute unique informations!
     # TODO: check if distribution is really correct -> compute V1 rate and compare to 40/52Hz
   #+end_src 


   Plotting code....
   #+begin_src python :session rbs :results raw drawer 
     thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
     ri = 0
     plt.figure(figsize=1.5*np.array((6,4)),dpi=200)
     plt.suptitle('Effect of synchrony gain')
     plt.subplot(2,2,3)
     plt.title('Signal routing non-attended stimulus')
     plt.plot(c1s,acs[:,2],color='blue',label='C(f_A,r_4) s=2',linestyle='dashed')
     plt.plot(c1s,acs[:,5],color='green',label='C(f_A,r_4) s=5',linestyle='dashed')
     plt.plot(c1s,acs[:,10],color='red',label='C(f_A,r_4) s=10',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     plt.subplot(2,2,1)
     plt.title('Signal routing attended stimulus')
     plt.plot(c1s,bcs[:,2],color='blue',label='C(f_B,r_4) s=2',linestyle='dashed')
     plt.plot(c1s,bcs[:,5],color='green',label='C(f_B,r_4) s=5',linestyle='dashed')
     plt.plot(c1s,bcs[:,10],color='red',label='C(f_B,r_4) s=10',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     for r,col in zip([np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh'+str(s)+'.npz','rb')) for s in [2,5,10]],
                      ['blue','green','red']):
         ccfA4 = r['c_ccfAV4']
         ccfB4 = r['c_ccfBV4']
         ccfA4 = np.nanmean(ccfA4,axis=3)
         ccfB4 = np.nanmean(ccfB4,axis=3)
         plt.subplot(2,2,1)
         plt.plot(c1s,ccfB4[:,ri,0],color=col)
         plt.subplot(2,2,3)
         plt.plot(c1s,ccfA4[:,ri,0],color=col)
     # plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
     # plt.savefig('img/ana_synchrony_gain_2510.svg')


     mi = min(acs.min(),bcs.min())
     ma = max(acs.max(),bcs.max())
     #plt.figure(dpi=200)
     plt.subplot(2,2,2)
     #plt.title('signal routing attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.subplot(2,2,4)

     #plt.title('correlation non-attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.tight_layout()
     # plt.savefig('img/correlations_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/correlations_alpha_thresholds_ana.svg')



     # plt.figure(dpi=200)
     # plt.scatter(thresholds[1:],acs.max(axis=0)[1:],label='max C(f_A,r_C)')
     # plt.scatter(thresholds[1:],bcs.max(axis=0)[1:],label='max C(f_B,r_C)')
   #+end_src 

   #+RESULTS:
   #+begin_example
     <ipython-input-8-ca8fec532128>:29: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,1)
     <ipython-input-8-ca8fec532128>:31: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,3)
     <ipython-input-8-ca8fec532128>:29: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,1)
     <ipython-input-8-ca8fec532128>:31: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,3)
     <ipython-input-8-ca8fec532128>:29: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,1)
     <ipython-input-8-ca8fec532128>:31: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.
       plt.subplot(2,2,3)
   #+end_example


   ... better plotting code
   #+begin_src python :session rbs :results raw drawer 
     thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
     ri = 0
     plt.figure(figsize=1.5*np.array((6,4)),dpi=200)
     plt.suptitle('Effect of synchrony gain')
     plt.subplot(3,2,1)
     plt.title('Signal routing theta=2')
     plt.plot(c1s,acs[:,2],color='red',label='C(f_A,r_C) s=2',linestyle='dashed')
     plt.plot(c1s,bcs[:,2],color='blue',label='C(f_B,r_C) s=2',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     plt.subplot(3,2,3)
     plt.title('Signal routing theta=5')
     plt.plot(c1s,acs[:,5],color='red',label='C(f_A,r_4) s=5',linestyle='dashed')
     plt.plot(c1s,bcs[:,5],color='blue',label='C(f_B,r_4) s=5',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     plt.subplot(3,2,2)
     plt.title('Signal routing theta=10')
     plt.plot(c1s,acs[:,10],color='red',label='C(f_A,r_4) s=10',linestyle='dashed')
     plt.plot(c1s,bcs[:,10],color='blue',label='C(f_B,r_4) s=10',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')


     for r,col,sp in zip([np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh'+str(s)+'.npz','rb')) for s in [2,5,10]],
                         ['blue','green','red'],[1,3,5]):
         ccfA4 = r['c_ccfAV4']
         ccfB4 = r['c_ccfBV4']
         ccfA4 = np.nanmean(ccfA4,axis=3)
         ccfB4 = np.nanmean(ccfB4,axis=3)
         plt.subplot(3,2,sp)
         plt.plot(c1s,ccfB4[:,ri,0],color='blue')
         plt.plot(c1s,ccfA4[:,ri,0],color='red')
     # plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
     # plt.savefig('img/ana_synchrony_gain_2510.svg')


     mi = min(acs.min(),bcs.min())
     ma = max(acs.max(),bcs.max())
     #plt.figure(dpi=200)
     plt.subplot(3,2,5)
     #plt.title('signal routing attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.subplot(3,2,6)

     #plt.title('correlation non-attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     # plt.savefig('img/correlations_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/correlations_alpha_thresholds_ana.svg')



     plt.subplot(3,2,4)
     plt.title('maximum correlation values')
     plt.plot(thresholds[1:],bcs.max(axis=0)[1:],label='max C(f_A,r_C)',color='red')
     plt.plot(thresholds[1:],bcs.max(axis=0)[1:]/np.array([acs[i] for i in zip(bcs.argmax(axis=0),np.arange(acs.shape[1]))])[1:],label='max C(f_B,r_C)',color='blue')
     plt.xlabel('thresholds')
     plt.legend()

     plt.tight_layout()
   #+end_src 


   #+begin_src python :session rbs :results raw drawer 
     thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_mis_c1_thresholds.pickle','rb'))

     plt.figure(figsize=1.5*np.array((6,4)),dpi=200)
     plt.suptitle('Effect of synchrony gain')
     plt.subplot(2,2,3)
     plt.title('Signal routing non-attended stimulus')
     plt.plot(c1s,acs[:,2],color='blue',label='MI(f_A,r_4) s=2',linestyle='dashed')
     plt.plot(c1s,acs[:,5],color='green',label='MI(f_A,r_4) s=5',linestyle='dashed')
     plt.plot(c1s,acs[:,10],color='red',label='MI(f_A,r_4) s=10',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     plt.subplot(2,2,1)
     plt.title('Signal routing attended stimulus')
     plt.plot(c1s,bcs[:,2],color='blue',label='MI(f_B,r_4) s=2',linestyle='dashed')
     plt.plot(c1s,bcs[:,5],color='green',label='MI(f_B,r_4) s=5',linestyle='dashed')
     plt.plot(c1s,bcs[:,10],color='red',label='MI(f_B,r_4) s=10',linestyle='dashed')
     #plt.ylim([0,0.1])
     plt.legend()
     plt.xlabel('c1')
     plt.ylabel('correlation')
     # for r,col in zip([np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh'+str(s)+'.npz','rb')) for s in [2,5,10]],
     #                  ['blue','green','red']):
     #     ccfA4 = r['c_ccfAV4']
     #     ccfB4 = r['c_ccfBV4']
     #     ccfA4 = np.nanmean(ccfA4,axis=3)
     #     ccfB4 = np.nanmean(ccfB4,axis=3)
     #     plt.subplot(2,2,1)
     #     plt.plot(c1s,ccfB4[:,ri,0],color=col)
     #     plt.subplot(2,2,3)
     #     plt.plot(c1s,ccfA4[:,ri,0],color=col)
     # plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
     # plt.savefig('img/ana_synchrony_gain_2510.svg')


     mi = min(acs.min(),bcs.min())
     ma = max(acs.max(),bcs.max())
     #plt.figure(dpi=200)
     plt.subplot(2,2,2)
     #plt.title('signal routing attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.subplot(2,2,4)

     #plt.title('correlation non-attended stimulus')
     plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     plt.colorbar()
     plt.xlabel('coupling strength alpha')
     plt.ylabel('threshold theta')
     plt.ylim([1,20])
     plt.tight_layout()
     plt.savefig('img/MIs_alpha_thresholds_ana.png',dpi=200)
     plt.savefig('img/MIs_alpha_thresholds_ana.svg')

   #+end_src 

   #+RESULTS:


   #+begin_src python :session rbs :results raw drawer 
     from matplotlib.gridspec import GridSpec
     from mpl_toolkits.axes_grid1 import make_axes_locatable
     #pickle.dump([acs,bcs,cis],open('acs_bcs_cis.pickle','wb'))
     acs,bcs,cis = pickle.load(open('acs_bcs_cis.pickle','rb'))
     fig = plt.figure(figsize=3*np.array([3,2]),constrained_layout=True,dpi=200)
     gs = GridSpec(2,2)
     ax1 = fig.add_subplot(gs[:,0])
     sidx=0
     ax1.plot(np.linspace(0.2,1,80),(acs[:,sidx]+bcs[:,sidx]+cis[:,sidx]),color='black',label='I(n_4,(f_A,f_B))')
     ax1.plot(np.linspace(0.2,1,80),(acs[:,sidx]),color='blue',label='IUNQ(n_4:f_B \ f_A)')
     ax1.plot(np.linspace(0.2,1,80),(bcs[:,sidx]),color='red',label='IUNQ(n_4:f_A \ f_B)')
     ax1.plot(np.linspace(0.2,1,80),[0]*80,color='brown',label='ISHD(n_4:f_A,f_B)')
     ax1.plot(np.linspace(0.2,1,80),(200*cis[:,sidx]),color='gray',label='200*ISYN(n_4:f_A,f_B)',linestyle='dashed')
     ax1.set_xlabel('coupling strength alpha')
     ax1.set_ylabel('bits')
     ax1.set_title('PID of signal routing')
     ax1.legend(loc='best')

     # ax2.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,)
     # ax2.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,)
     # ax2 = plt.gca().twinx()
     # ax2.plot(np.linspace(0.2,1,80),bcs
     #          .max(axis=0))
     # ax2.plot(np.linspace(0.2,1,80),acs.max(axis=0))
     ax2 = fig.add_subplot(gs[0,1])
     thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_mis_c1_thresholds.pickle','rb'))
     mi = min(acs.min(),bcs.min())
     ma = max(acs.max(),bcs.max())
     im = ax2.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     ax2.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     divider = make_axes_locatable(ax2)
     cax = divider.append_axes('right', size='5%', pad=0.05)
     fig.colorbar(im, cax=cax, orientation='vertical')
     ax2.set_xlabel('coupling strength alpha')
     ax2.set_ylabel('threshold theta')
     ax2.set_ylim([1,20])
     ax2.set_title('IUNQ(n_4:f_A \ f_B)')
     ax3 = fig.add_subplot(gs[1,1])
     im = ax3.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     ax3.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     divider = make_axes_locatable(ax3)
     cax = divider.append_axes('right', size='5%', pad=0.05)
     fig.colorbar(im, cax=cax, orientation='vertical')
     ax3.set_xlabel('coupling strength alpha')
     ax3.set_ylabel('threshold theta')
     ax3.set_ylim([1,20])
     ax3.set_title('IUNQ(n_4:f_B \ f_A)')
     plt.tight_layout()
     # plt.savefig('img/PID_fig.png',dpi=200)
     # plt.savefig('img/PID_fig.png',svg=200)
   #+end_src 



   construct pmf of flicker to V1
   This works too :) 
   Very nice!
   #+begin_src python :session rbs :results raw drawer 
     xs = np.linspace(-1,1,5)
     ys = np.linspace(-1,1,5)
     zs = np.arange(101)
     MI_fAAs = []
     MI_fBBs = []
     corr_fAAs = []
     corr_fBBs = []
     bcorr_fAAs = []
     bcorr_fBBs = []
     thresholds = [5]#range(1,21)

     c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
     c1_idxs = np.arange(80)
     #c1_idxs = [55]
     dist_vols = pickle.load(open('dist_vols_n=100.pickle','rb'))
     for c1_idx in c1_idxs:
         MI_fAAs.append([])
         MI_fBBs.append([])
         corr_fAAs.append([])
         corr_fBBs.append([])
         bcorr_fAAs.append([])
         bcorr_fBBs.append([])
         c1 = np.linspace(0.2,1,80)[c1_idx]
         print('c1',c1)
         c3 = 0.4#'auto'
         cup=0.3;
         c_control = 0.095
         n=100;
         n2 = 10
         c=0.1 #0.25
         dt=1e-6
         rate1=40
         ac = 1-1/np.sqrt(n)
         ra1 = 1/(n-n*ac*c1) # rate ana 1 step
         norm = ((2*n+n2)*(1+c))
         #deltaU = dt*n*rate1/(ra1*norm)
         p0 = 0.266666666 # taken from simulation code!
         deltaU = (n*(1-ac*c1))*rate1*dt/p0 # solve for condition V1 has rate1 firing rate
         #deltaU = deltaU/100
         #deltaU = dt*p0*rate1/ra1
         #detailed_dist,vol = detailed_avs_dist_3pop(c1,c3,cup,c_control,n,return_vol=True)
         detailed_dist,vol = dist_vols[c1_idx]
         v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist

         #deltaU = 0.005293874
         n = 100
         n2 = 10
         # das ist jetzt der wichtigste schritt....

         #Probability of non-empty avalanche : p*DeltaU*p_Vxxx/vol_hypercube
         # vol_hypercube = vnonin_mod(NS,np.ones_like(Ns),W)

         # I want to have the whole distribution
         # shape (5,5,100,100,10,100)
         # This is too much!

         # V4=0 √ºber weg A with prob pext_A
         #*(1-deltaU*pVxxxx/vol)
         # V4=x √ºber ................ pext_A*deltaU*Vxxx/vol !!!!

         # hier n2 durch n ersetzt! Faktor steckt schon in den p_ drin!
         # und dann m√ºssen alle p_ durch n geteilt werden!!!!!
         # equivalent n2 behalten und entsprechende p durch 10 teilen! Das ergibt sogar einen Sinn!!!!!
         # beim Aufschreiben einfach vol0_st als Mittelwert und nicht als Summe definieren

         # NOW: pmf of flicker->V1
         for s in thresholds:
             print('s',s)
             norm = ((2*n+n)*(1+c))
             p_base = np.array([n,n,n,0])/norm
             pmf_a = np.zeros((5,101))
             pmf_b = np.zeros((5,101))
             # TODO s einbauen!!!!!
             #s=10
             for ia,lA in enumerate(xs):
                 p = p_base.copy()
                 p0 = p[0]*(1+c*lA)
                 pmf_a[ia,0] = 1-p0
                 pmf_a[ia,0] += p0*(1-deltaU*p_V1aV4/vol/n)
                 pmf_a[ia,:] += p0*deltaU*np.sum(v_V1aV4,axis=1)/vol/n
                 pmf_b[ia,0] = 1-2*p0
                 pmf_b[ia,0] += p0*(1-deltaU*p_V1bV4/vol/n)  
                 pmf_b[ia,:] += p0*deltaU*np.sum(v_V1bV4,axis=1)/vol/n 
                 pmf_b[ia,0] += p0*(1-deltaU*p_V1bcV4/vol/n)
                 pmf_b[ia,:] += p0*deltaU*np.sum(v_V1bcV4,axis=(0,2))/vol/n
             pmf_a = pmf_a/5
             pmf_b = pmf_b/5

             # TODO Compute pid with admUI_numpy!
             # generate conditional distributions with dit in base2 and then convert to linear numpy arrays!
             import dit
             da = dit.Distribution.from_ndarray(np.log2(pmf_a),base=2)
             db = dit.Distribution.from_ndarray(np.log2(pmf_b),base=2)
             #print(dit.shannon.mutual_information(d,[0],[2]))
             MI_fAAs[-1].append(dit.shannon.mutual_information(da,[0],[1]))
             #print(dit.shannon.mutual_information(d,[1],[2]))
             MI_fBBs[-1].append(dit.shannon.mutual_information(db,[0],[1]))
             #print(dit.shannon.mutual_information(d,[1],[2])/dit.shannon.mutual_information(d,[0],[2]))
             # compute correlation coefficient
             d_fAA = da
             d_fBB = db
             d_fA = da.marginal([0])
             d_fB = db.marginal([0])
             d_A = da.marginal([1])
             d_B = db.marginal([1])
             E_fA = 0#np.sum([xs*np.exp2(d_fA[i,]) for i in range(5)])
             E_fB = 0
             E_fA2 = np.sum([x**2*np.exp2(d_fA[i,]) for i,x in enumerate(xs)])
             E_fB2 = np.sum([x**2*np.exp2(d_fB[i,]) for i,x in enumerate(xs)])
             E_1A = np.sum([np.exp2(d_A[i,])*i for i in range(101)])
             E_1B = np.sum([np.exp2(d_B[i,])*i for i in range(101)])
             E_1A2 = np.sum([np.exp2(d_A[i,])*i**2 for i in range(101)])
             E_1B2 = np.sum([np.exp2(d_B[i,])*i**2 for i in range(101)])
             E_fAA = np.sum([np.exp2(d_fAA[i,j])*lA*nV1 for i,lA in enumerate(xs) for j,nV1 in enumerate(range(101))])
             E_fBB = np.sum([np.exp2(d_fBB[i,j])*lB*nV1 for i,lB in enumerate(ys) for j,nV1 in enumerate(range(101))])
             corr_fAA = (E_fAA-E_fA*E_1A)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(E_1A2-E_1A**2))
             corr_fBB = (E_fBB-E_fB*E_1B)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(E_1B2-E_1B**2))
             corr_fAAs[-1].append(corr_fAA)
             corr_fBBs[-1].append(corr_fBB)
             # print(corr_fA4)
             # print(corr_fB4)
             # print(corr_fB4/corr_fA4)
             num_binned = 1000#100
             bvE_1A = (E_1A2-E_1A**2)/num_binned
             bvE_1B = (E_1B2-E_1B**2)/num_binned
             bE_1A2 = E_1A**2+bvE_1A
             bE_1B2 = E_1B**2+bvE_1B
             bcorr_fAA = (E_fAA-E_fA*E_1A)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(bE_1A2-E_1A**2))
             bcorr_fBB = (E_fBB-E_fB*E_1B)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(bE_1B2-E_1B**2))
             # bcorr_fA4 = (E_fA4-E_fA*E_4)/(np.sqrt(E_fA2-E_fA**2)*np.sqrt(bE_42-E_4**2))
             # bcorr_fB4 = (E_fB4-E_fB*E_4)/(np.sqrt(E_fB2-E_fB**2)*np.sqrt(bE_42-E_4**2))
             bcorr_fAAs[-1].append(bcorr_fAA)
             bcorr_fBBs[-1].append(bcorr_fBB)



     acs = np.array(bcorr_fAAs)
     bcs = np.array(bcorr_fBBs)
     pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v1_corrs_mis_c1_thresholds_c01.pickle','wb'))

     # acs = np.array(MI_fAAs)
     # bcs = np.array(MI_fBBs)
     # #pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_corrs_mis_c1_thresholds.pickle','wb'))



   #+end_src 


   Now the final link V1 to V4
   Funktioniert auch :) !!!!
   #+begin_src python :session rbs :results raw drawer 
     xs = np.linspace(-1,1,5)
     ys = np.linspace(-1,1,5)
     zs = np.arange(101)
     MI_A4s = []
     MI_B4s = []
     corr_A4s = []
     corr_B4s = []
     bcorr_A4s = []
     bcorr_B4s = []
     thresholds = range(1,21)
     thresholds = [5]
     c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
     c1_idxs = np.arange(80)
     #c1_idxs = [55]
     dist_vols = pickle.load(open('dist_vols_n=100.pickle','rb'))
     for c1_idx in c1_idxs:
         MI_A4s.append([])
         MI_B4s.append([])
         corr_A4s.append([])
         corr_B4s.append([])
         bcorr_A4s.append([])
         bcorr_B4s.append([])
         c1 = np.linspace(0.2,1,80)[c1_idx]
         print('c1',c1)
         c3 = 0.4#'auto'
         cup=0.3;
         c_control = 0.095
         n=100;
         n2 = 10
         c=0.25
         dt=1e-6
         rate1=40
         ac = 1-1/np.sqrt(n)
         ra1 = 1/(n-n*ac*c1) # rate ana 1 step
         norm = ((2*n+n2)*(1+c))
         #deltaU = dt*n*rate1/(ra1*norm)
         p0 = 0.266666666 # taken from simulation code!
         deltaU = (n*(1-ac*c1))*rate1*dt/p0 # solve for condition V1 has rate1 firing rate
         #deltaU = deltaU/100
         #deltaU = dt*p0*rate1/ra1
         #detailed_dist,vol = detailed_avs_dist_3pop(c1,c3,cup,c_control,n,return_vol=True)
         detailed_dist,vol = dist_vols[c1_idx]
         v_V1aV4, p_V1aV4, v_V1bV4, p_V1bV4, v_V1bcV4, p_V1bcV4, v_V4, p_V4 = detailed_dist

         #deltaU = 0.005293874
         n = 100
         n2 = 10
         # das ist jetzt der wichtigste schritt....

         #Probability of non-empty avalanche : p*DeltaU*p_Vxxx/vol_hypercube
         # vol_hypercube = vnonin_mod(NS,np.ones_like(Ns),W)

         # I want to have the whole distribution
         # shape (5,5,100,100,10,100)
         # This is too much!

         # V4=0 √ºber weg A with prob pext_A
         #*(1-deltaU*pVxxxx/vol)
         # V4=x √ºber ................ pext_A*deltaU*Vxxx/vol !!!!

         # hier n2 durch n ersetzt! Faktor steckt schon in den p_ drin!
         # und dann m√ºssen alle p_ durch n geteilt werden!!!!!
         # equivalent n2 behalten und entsprechende p durch 10 teilen! Das ergibt sogar einen Sinn!!!!!
         # beim Aufschreiben einfach vol0_st als Mittelwert und nicht als Summe definieren

         # AND NOW V1->V4 analytically!!!!!
         for s in thresholds:
             print('s',s)
             norm = ((2*n+n)*(1+c))
             p_base = np.array([n,n,n,0])/norm
             pmf_a = np.zeros((101,101))
             pmf_b = np.zeros((101,101))
             # TODO s einbauen!!!!!
             #s=10
             for ia,lA in enumerate(xs):
                 for ib,lB in enumerate(ys):
                     p = p_base.copy()
                     p[0] = p[0]*(1+c*lA)
                     p[1:3] = p[1:3]*(1+c*lB)
                     pmf_a[0,0] += 1-np.sum(p) # Prob no unit gets selected! TODO
                     pmf_b[0,0] += 1-np.sum(p) # Prob no unit gets selected! TODO
                     # Jetzt √ºber alle Wege im Netzwerk summieren!
                     # Kreuzwege sind noch nicht drin!
                     # V4 weg marginalisieren
                     # Weg V1a

                     pmf_a[0,0] += p[0]*(1-deltaU*p_V1aV4/vol/n) # no avalanche in V1
                     pmf_a[:s,0] += p[0]*deltaU*np.sum(v_V1aV4[:s,:],axis=1)/vol/n
                     pmf_a[s:,:] += p[0]*deltaU*v_V1aV4[s:,:]/vol/n
                     # kreuzweg 
                     pmf_b[0,0] += p[0]*(1-deltaU*p_V1aV4/vol/n)
                     pmf_b[0,0] += p[0]*deltaU*np.sum(v_V1aV4[:s,:])/vol/n
                     pmf_b[0,:] += p[0]*deltaU*np.sum(v_V1aV4[s:,:],axis=0)/vol/n
                     # Weg V1b 
                     pmf_b[0,0] += p[1]*(1-deltaU*p_V1bV4/vol/n)  
                     pmf_b[:s,0] += p[1]*deltaU*np.sum(v_V1bV4[:s,:],axis=1)/vol/n
                     pmf_b[s:,:] += p[1]*deltaU*v_V1bV4[s:,:]/vol/n
                     # kreuzweg
                     pmf_a[0,0] += p[1]*(1-deltaU*p_V1bV4/vol/n)
                     pmf_a[0,0] += p[1]*deltaU*np.sum(v_V1bV4[:s,:])/vol/n
                     pmf_a[0,:] += p[1]*deltaU*np.sum(v_V1bV4[s:,:],axis=0)/vol/n
                     # Weg control to V1b
                     pmf_b[0,0] += p[2]*(1-deltaU*p_V1bcV4/vol/n)  
                     pmf_b[:s,0] += p[2]*deltaU*np.sum(v_V1bcV4[:,:s,:],axis=(0,2))/vol/n
                     pmf_b[s:,:] += p[2]*deltaU*np.sum(v_V1bcV4[:,s:,:],axis=0)/vol/n
                     # kreuzweg
                     pmf_a[0,0] += p[2]*(1-deltaU*p_V1bcV4/vol/n)
                     pmf_a[0,0] += p[2]*deltaU*np.sum(v_V1bcV4[:,:s,:])/vol/n
                     pmf_a[0,:] += p[2]*deltaU*np.sum(v_V1bcV4[:,s:,:],axis=(0,1))/vol/n
             pmf_a = pmf_a/25
             pmf_b = pmf_b/25

             # TODO Compute pid with admUI_numpy!
             # generate conditional distributions with dit in base2 and then convert to linear numpy arrays!
             import dit
             da = dit.Distribution.from_ndarray(np.log2(pmf_a),base=2)
             db = dit.Distribution.from_ndarray(np.log2(pmf_b),base=2)
             MI_A4s[-1].append(dit.shannon.mutual_information(da,[0],[1]))
             MI_B4s[-1].append(dit.shannon.mutual_information(db,[0],[1]))

             # compute correlation coefficient
             d_A4 = da
             d_B4 = db
             d_A = da.marginal([0])
             d_B = db.marginal([0])
             d_4 = da.marginal([1]) # same as db.marginal([1])
             E_A = np.sum([np.exp2(d_A[i,])*i for i in range(101)])
             E_B = np.sum([np.exp2(d_B[i,])*i for i in range(101)])
             E_A2 = np.sum([np.exp2(d_A[i,])*i**2 for i in range(101)])
             E_B2 = np.sum([np.exp2(d_B[i,])*i**2 for i in range(101)])
             E_4 = np.sum([np.exp2(d_4[i,])*i for i in range(101)])
             E_42 = np.sum([np.exp2(d_4[i,])*i**2 for i in range(101)])
             E_A4 = np.sum([np.exp2(d_A4[i,j])*i*j for i in range(101) for j in range(101)])
             E_B4 = np.sum([np.exp2(d_B4[i,j])*i*j for i in range(101) for j in range(101)])
             corr_A4 = (E_A4-E_A*E_4)/(np.sqrt(E_A2-E_A**2)*np.sqrt(E_42-E_4**2))
             corr_B4 = (E_B4-E_B*E_4)/(np.sqrt(E_B2-E_B**2)*np.sqrt(E_42-E_4**2))
             corr_A4s[-1].append(corr_A4)
             corr_B4s[-1].append(corr_B4)
             # print(corr_fA4)
             # print(corr_fB4)
             # print(corr_fB4/corr_fA4)
             bcorr_A4s[-1].append(corr_A4)
             bcorr_B4s[-1].append(corr_B4)



     acs = np.array(bcorr_A4s)
     bcs = np.array(bcorr_B4s)
     pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('v1_v4_corrs_mis_c1_thresholds.pickle','wb'))

     # mi = min(acs.min(),bcs.min())
     # ma = max(acs.max(),bcs.max())
     # plt.figure(dpi=200)
     # plt.subplot(2,1,1)
     # plt.title('correlation attended flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.subplot(2,1,2)

     # plt.title('correlation non-attended flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.tight_layout()
     # plt.savefig('img/correlations_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/correlations_alpha_thresholds_ana.svg')


     # acs = np.array(MI_fA4s)
     # bcs = np.array(MI_fB4s)
     # #pickle.dump([thresholds,np.linspace(0.2,1,80),acs,bcs],open('flicker_v4_corrs_mis_c1_thresholds.pickle','wb'))
     # mi = -30
     # ma = -10
     # plt.figure(dpi=200)
     # plt.subplot(2,1,1)
     # plt.title('Mutual information flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(bcs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.subplot(2,1,2)

     # plt.title('Mutual information flicker and V4')
     # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,np.log2(acs).T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
     # plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
     # plt.colorbar()
     # plt.xlabel('coupling strength alpha')
     # plt.ylabel('threshold theta')
     # plt.ylim([1,20])
     # plt.tight_layout()
     # plt.savefig('img/MI_alpha_thresholds_ana.png',dpi=200)
     # plt.savefig('img/MI_alpha_thresholds_ana.svg')



     # correlation ratios sind schon in einem extra plot -> dort den grenzfall s=0 mit rein nehmen!

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(corr_fB4s))
     # #plt.figure(dpi=200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fA4s)*200)
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(MI_fB4s)*200)

     # plt.figure(dpi=200)    
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fA4s))
     # plt.plot(np.linspace(0.2,1,80)[c1_idxs],np.array(bcorr_fB4s))




     # THIS IS VERY NICE!!!!!!!!!!!!!
     # TODO: Add binning to correlation computation and check if numerics match analytics!
     # TODO: refactor to good function
     # TODO: add flicker V1 and V1 V4 analysis
     # TODO: compute unique informations!
     # TODO: check if distribution is really correct -> compute V1 rate and compare to 40/52Hz
   #+end_src 


** TODO Extensions of the model 
*** Lateral connections
*** Hypercolumn model with biased competition
* Discussion


* Old exploration codes

** Correlation ratios

     # TODO Refactor the ugly for loop....
  # Ist refractored-> kann rausgeschmissen werden -> maybe subtree at end...
  #+begin_src python :session rbs :results raw drawer 
    ratios = []
    c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
    for c1_idx in c1_idxs:
        print(c1_idx)
        c1 = np.linspace(0.2,1,80)[c1_idx]
        radv = 12
        method = 'int'
        trial = 0
        #c1,radv,method = 
        rate1=40;
        cup=0.3;
        T=50;
        n=100;
        n2 = int(n/10);
        c=0.25;
        thresh=5; # for n=1000
        fname=None#'./res_kadabuum'
        dtcorr=3
        rate2 = rate1+radv
        Ns = (n,n,n2,n)
        ac = 1-1/np.sqrt(n)
        c2 = c1
        c3 = 0.4#c2-2*cup
        N = 3*n+2*n2


        Ws = np.zeros((4,4))#one extra unit to supply the overshoot of deltaU external inputs
        Ws[0,0] = c1*ac/n # V1 A
        # Ws[1,:] is inhibited second population of nonattended V1
        Ws[1,1] = c2*ac/n # V1 B 
        Ws[2,2] = (1-np.sqrt(n2)/n2)/n2
        if method=='int':
            Ws[1,2] = 0.095/n2 # input from crit subnet to V1 B # 0.03 f√ºr n=1000
        Ws[3,3] = c3*ac/n # V4 pop
        Ws[3,0] = cup/n # inputs to V4
        Ws[3,1] = cup/n



        avsd = ModularAvalancheSizeDistributionFast(Ns,Ws,deltaU=1)
        p = np.ones(len(Ns))#/len(Ns)

        import time
        st = time.time()
        v_V1aV4 = np.array([[avsd.detailed_volume(0,(i,0,0,j)) for j in range(0,n+1)] for i in range(n+1)])
        p_V1aV4 = p[0]*avsd.volume_size0_st(0)

        v_V1bV4 = np.array([[avsd.detailed_volume(1,(0,i,0,j)) for j in range(0,n+1)] for i in range(n+1)])
        p_V1bV4 = p[1]*avsd.volume_size0_st(1)

        v_V1bcV4 = np.array([[[avsd.detailed_volume(2,(0,j,i,k)) for k in range(0,n+1)] for j in range(0,n+1)] for i in range(n2+1)])
        p_V1bcV4 = p[2]*avsd.volume_size0_st(2)

        v_V4 = np.array([avsd.detailed_volume(3,(0,0,0,i)) for i in range(0,n+1)])
        p_V4 = p[3]*avsd.volume_size0_st(3)

        print(time.time()-st)


        c = 0.25
        norm = ((2*n+n2)*(1+c))
        p_base = np.array([n,n,n2,0])/norm
        deltaU = 0.000004775842182531506
        s = 10
        ra1 = 0.005615567911040512
        rate1 = 40
        def ana_flicker_v1_correlation():
            n_levels = 5
            levels = np.linspace(-1,1,n_levels)
            cfA4 = 0
            cfB4 = 0
            EfA4 = 0
            EfB4 = 0
            E4 = 0
            E44 = 0
            EfA = 0
            EfAfA = 0
            EfB = 0
            EfBfB = 0
            for lA in levels:
                for lB in levels:
                    # Adjust input probabilities
                    p = p_base.copy()
                    p[0] = p[0]*(1+c*lA)
                    p[1:3] = p[1:3]*(1+c*lB)
                    fA = p[0]
                    fB = p[1]+p[2]
                    #print(lA,lB)
                    #print(p,np.sum(p))
                    # Calculate moments
                    EfA+=fA
                    EfB+=fB
                    EfAfA+= fA**2
                    EfBfB+= fB**2
                    # √úberlegung dazu -> pV1aV4 etc sind unnorm. probs dass eine Lawine gestartet wird
                    # daher f√ºr erwartungswert *PV1aV4 rechnen bzw. wegk√ºrzen und p*deltaU davor setzen
                    E4n = deltaU*(#p[3]*np.sum([i*v_V4[i] for i in np.arange(n)+1])+
                                  p[0]*np.sum([i*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                                  p[1]*np.sum([i*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                                  10*p[2]*np.sum([i*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                    # print(lA,lB)
                    # print(E4n)
                    E4+=E4n
                    E44n = deltaU*(#p[3]*np.sum([i**2*v_V4[i] for i in np.arange(n)+1])+
                                   p[0]*np.sum([i**2*np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])+ # input to V1a
                                   p[1]*np.sum([i**2*np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])+ # input to V1b
                                   10*p[2]*np.sum([i**2*np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1]))
                    E44 += E44n
                    EfA4 += fA*E4n
                    EfB4 += fB*E4n
            EfA = EfA/n_levels**2
            EfB = EfB/n_levels**2
            EfAfA = EfAfA/n_levels**2
            EfBfB = EfBfB/n_levels**2
            E4 = E4/n_levels**2
            E44 = E44/n_levels**2
            EfA4 = EfA4/n_levels**2
            EfB4 = EfB4/n_levels**2
            cfA4 = (EfA4-EfA*E4)/(np.sqrt(EfAfA-EfA**2)*np.sqrt(E44-E4**2))
            cfB4 = (EfB4-EfB*E4)/(np.sqrt(EfBfB-EfB**2)*np.sqrt(E44-E4**2))
            return cfA4,cfB4,EfA,EfB,EfAfA,EfBfB,E4,E44,EfA4,EfB4

        cfA4,cfB4,EfA,EfB,EfAfA,EfBfB,E4,E44,EfA4,EfB4 = ana_flicker_v1_correlation()

        ratios.append(cfB4/cfA4)



    c1_idxs = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75]
    c1_idxs = c1_idxs[4:]
    r = ratios[4:]
    plt.figure(dpi=200)
    plt.plot([np.linspace(0.2,1,80)[c1_idx] for c1_idx in c1_idxs],r)

    r = np.load(open('sc_collected_ehe_flicker_sync_thresh10.npz','rb'))

    ccfA4 = r['ccfA4']
    ccfB4 = r['ccfB4']

    ccfA4 = np.nanmean(ccfA4,axis=3)
    ccfB4 = np.nanmean(ccfB4,axis=3)

    plt.plot([np.linspace(0.2,1,80)[c1_idx] for c1_idx in c1_idxs],[ccfB4[c1_idx,0,0]/ccfA4[c1_idx,0,0] for c1_idx in c1_idxs])
    plt.xlabel('c')
    plt.ylabel('correlation ratio')
    plt.title('num and ana corr ratio')
    #plt.savefig('num_ana_corr_ratio.png',dpi=200)
  #+end_src 


** Analytical avalanche distribution

Exploration code to compute analytical avalanche distributions

  #+begin_src python :session rbs :results raw drawer 
    c1 = np.linspace(0.2,1,80)[55]
    radv = 12
    method = 'int'
    trial = 0
    #c1,radv,method = 
    rate1=40;
    cup=0.3;
    T=50;
    n=100;
    n2 = int(n/10);
    c=0.25;
    thresh=5; # for n=1000
    fname=None#'./res_kadabuum'
    dtcorr=3
    rate2 = rate1+radv
    Ns = (n,n,n2,n)
    ac = 1-1/np.sqrt(n)
    c2 = c1
    c3 = c2-2*cup
    N = 3*n+2*n2
    Ws = np.zeros((4,4))#one extra unit to supply the overshoot of deltaU external inputs
    Ws[0,0] = c1*ac/n # V1 A
    # Ws[1,:] is inhibited second population of nonattended V1
    Ws[1,1] = c2*ac/n # V1 B 
    Ws[2,2] = (1-np.sqrt(n2)/n2)/n2
    if method=='int':
        Ws[1,2] = 0.095/n2 # input from crit subnet to V1 B # 0.03 f√ºr n=1000
    Ws[3,3] = c3*ac/n # V4 pop
    Ws[3,0] = cup/n # inputs to V4
    Ws[3,1] = cup/n

    avsd = ModularAvalancheSizeDistributionFast(Ns,Ws,deltaU=1)
    p = np.ones(len(Ns))#/len(Ns)
    import time
    st = time.time()
    v_V1aV4 = np.array([[avsd.detailed_volume(0,(i,0,0,j)) for j in range(0,n+1)] for i in range(n+1)])
    p_V1aV4 = p[0]*avsd.volume_size0_st(0)
    v_V1bV4 = np.array([[avsd.detailed_volume(1,(0,i,0,j)) for j in range(0,n+1)] for i in range(n+1)])
    p_V1bV4 = p[1]*avsd.volume_size0_st(1)
    v_V1bcV4 = np.array([[[avsd.detailed_volume(2,(0,j,i,k)) for k in range(0,n+1)] for j in range(0,n+1)] for i in range(n2+1)])
    p_V1bcV4 = p[2]*avsd.volume_size0_st(2)
    v_V4 = np.array([avsd.detailed_volume(3,(0,0,0,i)) for i in range(0,n+1)])
    p_V4 = p[3]*avsd.volume_size0_st(3)
    print(time.time()-st)

    # Jetzt theta analytisch behandeln
    s = 5
    # hier gibt es noch das problem dass V1b auch 0 sein kann...
    plt.figure(figsize=(3,2),dpi=200)
    plt.loglog(np.arange(n)+1,np.array([np.sum(v_V1aV4[i,:]) for i in np.arange(n)+1])/p_V1aV4,label='V1a')

    plt.loglog(np.arange(n)+1,(n2*np.array([np.sum(v_V1bcV4[:,i,:]) for i in np.arange(n)+1])/(p_V1bcV4-np.sum(v_V1bcV4[:,0,:]))
                               +n*np.array([np.sum(v_V1bV4[i,:]) for i in np.arange(n)+1])/p_V1bV4)/(n+n2),label='V1b')
    plt.loglog(np.arange(n)+1,(#n*v_V4[1:]/p_V4+
                               n*np.array([np.sum(v_V1aV4[s:,i]) for i in np.arange(n)+1])/(p_V1aV4-np.sum(v_V1aV4[:,0])-np.sum(v_V1aV4[:s,1:]))+
                               n*np.array([np.sum(v_V1bV4[s:,i]) for i in np.arange(n)+1])/(p_V1bV4-np.sum(v_V1bV4[:,0])-np.sum(v_V1bV4[:s,1:]))+
                               n2*np.array([np.sum(v_V1bcV4[:,s:,i]) for i in np.arange(n)+1])/(p_V1bcV4-np.sum(v_V1bcV4[:,:,0])-np.sum(v_V1bcV4[:,:s,1:])))/(2*n+n2),label='V4 theta='+str(s))

    plt.loglog(np.arange(n)+1,ehe_ana(0.9,100),label='crit')
    plt.ylim([1e-6,1])
    plt.legend()
    plt.xlabel('avalanche size s')
    plt.ylabel('P(s)')
    plt.title('numerical and analytical av. dist.')
    plt.savefig('ana_V4_no_ext_input075c304.png',dpi=200)
    plt.savefig('ana_av_dist_075.png',dpi=200)
    plt.savefig('ana_av_dist_075.svg')
  #+end_src 
  


* Exploration additive model -> no selective routing with additive input!
  
** Correlation analysis 3pop model theta=5

   Correlation values

   #+begin_src python :session rbs :results raw drawer 
     %matplotlib qt

     plt.rc('text', usetex=False)
     plt.rc('font', family='serif')

     import pickle
     c1s = np.linspace(0.2,1,80)
     radvs = np.array([12])
     methods = ['int','ext']

     r = np.load(open('sc_collected_ehe_flicker_sync_additive_correct.npz','rb'))

     ccfA4 = r['c_ccfAV4']
     ccfB4 = r['c_ccfBV4']
     title = 'correlation flicker V4'

     # ccfA4 = r['c_ccAV4'] # NOT BINNED!
     # ccfB4 = r['c_ccBV4'] # NOT BINNED!
     # title = 'correlation V1 V4'
     # # ccfA4 = r['c_ccbAV4'] # kaum merklicher unterschied zu  
     # # ccfB4 = r['c_ccbBV4'] # binned version! nice :)


     # ccfA4 = r['c_ccfAA'] 
     # ccfB4 = r['c_ccfBB'] 
     # title = 'correlation flicker V1'


     ccfA4 = np.nanmean(ccfA4,axis=3)
     ccfB4 = np.nanmean(ccfB4,axis=3)


     ri = 0
     ar = 1

     plt.figure(figsize=(3,2),dpi=200)
     m = max(np.max(ccfA4[:,ri,ar]),np.max(ccfB4[:,ri,ar]))
     plt.plot(c1s,ccfA4[:,ri,ar],label='noatt')
     plt.plot(c1s,ccfB4[:,ri,ar],label='att' )
     plt.vlines([c1s[55]],ymin=0,ymax=m,linestyle='dashed',color='k')
     plt.legend()
     plt.title(title)
     plt.xlabel('control parameter c')
     plt.ylabel('correlation')
     # plt.savefig('img/'+title+'.png',dpi=200)
     # plt.savefig('img/'+title+'.svg')

  #+end_src

  #+RESULTS:
  : Text(0, 0.5, 'correlation')



* Embedded control population!

  simulation code adapted from ehe_flicker_sync_new_c3_thresh1.py

  #+begin_src python :session rbs :results raw drawer 
    from tqdm import tqdm
    import itertools 
    from utils.spectral import *
    import numba
    import pickle
    import mpmath as m   

    def ehe_ana(alpha,N):
        return [m.exp(m.log((1/N))+m.log(m.binomial(N,n))+m.log(n*alpha/N)*(n-1)+
                           m.log(1-(n*alpha/N))*(N-n-1)+m.log(((1-alpha)/(1-((N-1)/N)*alpha))))
                for n in range(1,N+1)]


    # @numba.njit(cache=True)
    # def sim_iaf(ns,dt,ext_inputs,tau = 10e-3,Vthr = 1,dV = 1/20):
    #     V = 0#np.zeros((ns,))
    #     s = []
    #     for i in range(ns-1):
    #         V = V+dt*(-V)/tau
    #         V+= dV*ext_inputs[i]
    #         if V>Vthr:
    #             s.append(i)
    #             V -= 1
    #             #V = 0
    #     return V,s

    # @numba.njit(cache=True)
    # def sim_iaf_network(N,ns,dt,ext_inputs,tau = 10e-3,Vthr = 1,dV = 1/20):
    #     V = np.random.random((N,))
    #     s = []
    #     for i in range(ns-1):
    #         V = V+dt*(-V)/tau
    #         V+= dV*ext_inputs[i]
    #         if V>Vthr:
    #             s.append(i)
    #             V -= 1
    #     return V,s


    # @numba.njit(cache=True)
    # def sim_iaf_discrete(ns,dt,ext_inputs,level_spike,level_leak,num_levels=1000,p_leak=0.0642):
    #     V = 0
    #     s = []
    #     for i in range(ns-1):
    #         if ext_inputs[i] == 0:
    #             V = V-(np.random.rand()<p_leak)*level_leak
    #         V = max(V+level_spike*ext_inputs[i],0)
    #         if V>=num_levels:
    #             s.append(i)
    #             V = 0
    #     return V,s



    # def sc_plot(c4A,c4B,taus,freqs,titleA='spectral coherence v4 A',titleB='spectral coherence v4 B',suptitle=''):
    #     f = plt.figure(figsize=(12,6))
    #     plt.subplot(1,2,1)
    #     dtau =taus[1]-taus[0]
    #     ptaus = np.concatenate(([taus[0]-dtau],taus))+0.5*dtau
    #     #taus = np.array([-0.15,-0.05,0.05,0.15])
    #     vmax = max(np.max(c4A),np.max(c4B))
    #     im = plt.pcolormesh(ptaus,freqs,c4A,cmap='viridis', shading='flat',vmin=0,vmax=vmax)
    #     plt.yscale('log')
    #     plt.xlabel('tau [s]')
    #     plt.ylabel('frequency [Hz]')
    #     plt.title(titleA)
    #     plt.colorbar(im)
    #     plt.subplot(1,2,2)

    #     im = plt.pcolormesh(ptaus,freqs,c4B,cmap='viridis', shading='flat',vmin=0,vmax=vmax)
    #     plt.yscale('log')
    #     plt.xlabel('tau [s]')
    #     plt.ylabel('frequency [Hz]')
    #     plt.title(titleB)
    #     plt.colorbar(im)
    #     plt.suptitle(suptitle+str(round(np.sum(c4A),2))+','+str(round(np.sum(c4B),2)))
    #     return f

    nf = 5
    d_levels = np.linspace(-1,1,nf)
    def flicker(levels=None):
        levels = d_levels if levels is None else levels
        return levels[np.random.randint(0,len(levels))]



    @numba.njit(cache=True)
    def get_avinc(A,Ns,nNs):
        pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
        avinc = np.zeros(len(Ns),dtype=np.int64)
        for k in range(len(Ns)):
            avinc[k]+= np.sum(A[pop_ids(k)])
        return avinc


    @numba.njit(cache=True)
    def simulate_model_subnets(Ns,W,p,deltaU,num_steps=None,num_av=None,u0=None,outdir=None):
        Ns = np.array(Ns,dtype=np.int64)
        N = np.sum(Ns)
        u = np.random.random((N,)) if u0 is None else u0
        avc = 0
        step = 0
        neurons = np.arange(N)
        I = np.eye(len(Ns))
        avs = []
        avt = []
        #pc = np.cumsum(np.concatenate(tuple([np.array([ps/n]*n) for ps,n in zip(p,Ns)])))
        avu = []
        ks = []
        cNs = np.cumsum(Ns)
        nNs = np.concatenate((np.array([0]),cNs))
        pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
        pbig = np.zeros(N)
        for k in range(len(Ns)):
            pbig[pop_ids(k)] = p[k]/Ns[k]
        pc = np.cumsum(pbig)
        #pc = np.cumsum(np.concatenate([[ps/n]*n for ps,n in zip(p,Ns)]))

        while avc < num_av if num_av is not None else step < num_steps:
            k = np.searchsorted(pc,np.random.random())
            ks.append(np.searchsorted(cNs,k+1))
            u[k] += deltaU
            if u[k] > 1:
                avc+= 1
                avsize = np.zeros(len(Ns),dtype=np.int64)
                A = u > 1;
                avinc = get_avinc(A,Ns,nNs) # vector len(Ns)
                avt.append(step)
                avunits = []
                while np.sum(avinc) > 0:
                    avsize += avinc
                    u[A] -= 1
                    rec_int = W@avinc.astype(np.float64)
                    for i in range(len(Ns)):
                        u[pop_ids(i)]+=rec_int[i]/Ns[i]
                    A = u>1
                    avinc = get_avinc(A,Ns,nNs)
                avs.append(avsize)
                #avu.append(avunits)
            step += 1
        return avs,np.array(avt),u,ks

    @numba.njit(cache=True)
    def simulate_model_subnets_thresholds(Ns,W,p,deltaU,thresh=0,num_steps=None,num_av=None,u0=None,outdir=None):
        th_idx = 2
        Ns = np.array(Ns,dtype=np.int64)
        N = np.sum(Ns)
        u = np.random.random((N,)) if u0 is None else u0
        avc = 0
        step = 0
        neurons = np.arange(N)
        I = np.eye(len(Ns))
        avs = []
        avt = []
        #pc = np.cumsum(np.concatenate(tuple([np.array([ps/n]*n) for ps,n in zip(p,Ns)])))
        avu = []
        ks = []
        cNs = np.cumsum(Ns)
        nNs = np.concatenate((np.array([0]),cNs))
        pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
        pbig = np.zeros(N)
        for k in range(len(Ns)):
            pbig[pop_ids(k)] = p[k]/Ns[k]
        pc = np.cumsum(pbig)
        u_saved = u.copy()
        #pc = np.cumsum(np.concatenate([[ps/n]*n for ps,n in zip(p,Ns)]))

        while avc < num_av if num_av is not None else step < num_steps:
            k = np.searchsorted(pc,np.random.random())
            k_sub = np.searchsorted(cNs,k+1)
            ks.append(k_sub)
            u_saved = u.copy()
            u[k] += deltaU
            if u[k] > 1:
                avc+= 1
                avsize = np.zeros(len(Ns),dtype=np.int64)
                A = u > 1;
                avinc = get_avinc(A,Ns,nNs) # vector len(Ns)
                avt.append(step)
                avunits = []
                while np.sum(avinc) > 0:
                    avsize += avinc
                    u[A] -= 1
                    rec_int = W@avinc.astype(np.float64)
                    for i in range(len(Ns)):
                        u[pop_ids(i)]+=rec_int[i]/Ns[i]
                    A = u>1
                    avinc = get_avinc(A,Ns,nNs)
                if (k_sub != th_idx) and  (avsize[0]+avsize[1]<thresh):
                    u[pop_ids(th_idx)] = u_saved[pop_ids(th_idx)]
                    avsize[th_idx] = 0 
                avs.append(avsize)
                #avu.append(avunits)
            step += 1
        return avs,np.array(avt),u,ks


    @numba.njit(cache=True)
    def simulate_model_subnets_nonorm_thresholds(Ns,W,p,deltaU,thresh=0,num_steps=None,num_av=None,u0=None,outdir=None):
        th_idx = 4 #TODO Hardcoded index!!!!
        Ns = np.array(Ns,dtype=np.int64)
        N = np.sum(Ns)
        u = np.random.random((N,)) if u0 is None else u0
        avc = 0
        step = 0
        neurons = np.arange(N)
        I = np.eye(len(Ns))
        avs = []
        avt = []
        #pc = np.cumsum(np.concatenate(tuple([np.array([ps/n]*n) for ps,n in zip(p,Ns)])))
        avu = []
        ks = []
        cNs = np.cumsum(Ns)
        nNs = np.concatenate((np.array([0]),cNs))
        pop_ids = lambda k: slice(nNs[k],nNs[k+1],None)
        pbig = np.zeros(N)
        for k in range(len(Ns)):
            pbig[pop_ids(k)] = p[k]/Ns[k]
        pc = np.cumsum(pbig)
        u_saved = u.copy()
        #pc = np.cumsum(np.concatenate([[ps/n]*n for ps,n in zip(p,Ns)]))

        while avc < num_av if num_av is not None else step < num_steps:
            k = np.searchsorted(pc,np.random.random())
            k_sub = np.searchsorted(cNs,k+1)
            ks.append(k_sub)
            u_saved = u.copy()
            u[k] += deltaU
            if u[k] > 1:
                avc+= 1
                avsize = np.zeros(len(Ns),dtype=np.int64)
                A = u > 1;
                avinc = get_avinc(A,Ns,nNs) # vector len(Ns)
                avt.append(step)
                avunits = []
                while np.sum(avinc) > 0:
                    avsize += avinc
                    u[A] -= 1
                    rec_int = W@avinc.astype(np.float64)
                    for i in range(len(Ns)):
                        u[pop_ids(i)]+=rec_int[i]#/Ns[i]
                    A = u>1
                    avinc = get_avinc(A,Ns,nNs)
                if (k_sub != th_idx) and  (avsize[0]+avsize[2]<thresh): # TODO Hardcoded indices!!!!
                    u[pop_ids(th_idx)] = u_saved[pop_ids(th_idx)]
                    avsize[th_idx] = 0 
                avs.append(avsize)
                #avu.append(avunits)
            step += 1
        return avs,np.array(avt),u,ks




    def bin_data(data,bins):
        slices = np.linspace(0, len(data), bins+1, True).astype(np.int)
        counts = np.diff(slices)
        mean = np.add.reduceat(data, slices[:-1]) / counts
        return mean


    param_array = [(c1,radv,method,trial)
                   for c1 in np.linspace(0.2,1,20)
                   for radv in [12]
                   for method in ['int']#,'ext']
                   for trial in range(5)]

    c1,radv,method,trial = param_array[task_id-1]#[0.8,12,'int',0]#param_array[task_id-1]#
    #c1,radv,method = 

    rate1=40;
    cup=0.3;
    T=250;
    n=100;
    n2 = int(n/10);
    c=0.25;
    thresh=5; # for n=1000
    fname=None#'./res_kadabuum'
    dtcorr=3

    rate2 = rate1+radv
    Ns = (n,n2,n,n2,n,1)
    ac = 1-1/np.sqrt(n)
    c2 = c1
    c3 = 0.4
    N = 3*n+2*n2+1


    Ws = np.zeros((6,6))#one extra unit to supply the overshoot of deltaU external inputs
    Ws[0,0] = c1*ac/n # V1 A
    # Ws[1,:] is inhibited second population of nonattended V1
    Ws[2,2] = c2*ac/n # V1 B 
    Ws[3,3] = c2*ac/n#(1-np.sqrt(n2)/n2)/n2
    if method=='int':
        Ws[2,3] = c2*ac/n#0.095/n2 # input from crit subnet to V1 B # 0.03 f√ºr n=1000
        Ws[3,2] = c2*ac/n # control and V1b together form larger homogeneous network
    Ws[4,4] = c3*ac/n # V4 pop
    Ws[4,0] = cup/n # inputs to V4
    Ws[4,2] = cup/n
    # Ws[5,:] is dummy population

    deltaU = 0.8*(1-(Ws@np.diag(Ns)).sum(axis=1).max())

    assert deltaU>0

    M1 = np.linalg.inv(np.eye(n)-np.ones((n,n))*c1*ac/n)
    ra1 = (M1@(deltaU*np.ones((n,))/n))[0]

    suptitle = ''.join([n+'='+str(round(x,2))+',' for n,x in zip(['c1','c2','rate1','rate2','c3','cup','T','n','c'],
                                                        [c1,c2,rate1,rate2,c3,cup,T,n,c])])+'m='+method


    flick_dur = 10e-3
    dt = flick_dur/10000
    #dt = ra1/(rate1+rate2+(rate1+rate2)/2)
    #dt = dt/dtcorr


    ext_max = 2*(rate1/ra1)*((n+n2)/n)*(1+c)+(rate1/ra1)

    ############# fine tuning of rates without flicker #########################
    # flicka = rate1/ra1
    # flickb = rate2/ra2
    # p1 = flicka/ext_max
    # p2 = flickb/ext_max
    # p3 = 0#((rate1/ra1+rate2/ra2)/2)/ext_max
    # p_dummy = (1-p1-p2-p3)
    # p = np.array([p1,p2,p3,p_dummy])
    p = np.array([1,n2/n,1,n2/n,1,0])*rate1/ra1
    p[5] = ext_max-np.sum(p)
    p = p / np.sum(p)
    T_test = 5
    ns = int(T_test/dt)
    s,_,_,_ = simulate_model_subnets_nonorm_thresholds(Ns,Ws,p,deltaU,num_steps=ns)
    s = np.array(s)
    real_rate1 = np.sum(s[:,0])/(n*T_test)
    real_rate2 = np.sum(s[:,2])/(n*T_test)
    print('real rate1 would be '+str(real_rate1),flush=True)
    print('rate ratio',real_rate2/real_rate1)
    # change dt so that the rates are right
    #dt = dt*(real_rate1/rate1)
    deltaU = deltaU*(rate1/real_rate1)
    print('new deltaU = '+str(deltaU)+' '+str(deltaU + Ws.sum(axis=1).max()))
    assert(deltaU+Ws.sum(axis=1).max()<1)



    ########### now sample with flicker for specified time
    #T = 100
    flicksa = []
    flicksb = []

    u = np.random.random((N,))
    avss = []
    avts = []
    kss = []


    for it in (range(int(T/flick_dur))):
        #print(it,flush=True)
        if it % int((int(T/flick_dur))/10) == 0:
            print(it,flush=True)

        flicka = (rate1/ra1)*(1+c*flicker())
        flickb = (rate1/ra1)*(1+c*flicker())
        if method=='ext':
            flickb = flickb*1.3
        flicksa.extend([flicka]*int(flick_dur/dt))
        flicksb.extend([flickb]*int(flick_dur/dt))
        p1 = flicka
        p2 = flicka*(n2/n)
        p3 = flickb
        p4 = flickb*(n2/n)
        p5 = 0#2*rate1/ra1
        p = np.array([p1,p2,p3,p4,p5,0])
        p[5] = ext_max - np.sum(p)
        p = p/np.sum(p)
        ns = int(flick_dur/dt)
        avs,avt,u,ks = simulate_model_subnets_nonorm_thresholds(Ns,Ws,p,deltaU,thresh=thresh,num_steps=ns,u0=u)
        avs = np.array(avs)
        kss.append(ks)
        avss.append(avs)
        avts.append(np.array(avt)+it*int(flick_dur/dt))

    flicksa = np.array(flicksa)
    flicksb = np.array(flicksb)
    avs = np.concatenate(avss)
    avt = np.concatenate(avts)
    ks = np.concatenate(kss)
    ns = len(ks)
    print(len(avs),flush=True)

    spv = np.zeros((5,ns),dtype=int)
    finpA = np.zeros((ns,),dtype=int)
    finpB = np.zeros_like(finpA)

    finpA[ks==0] = 1
    finpB[ks==1] = 1

    for at,a in zip(avt,avs):
        spv[0,at] = a[0]
        spv[1,at] = a[2]
        spv[2,at] = a[4]

    Vthr = 1
    tau = 1e-2
    dV = 1.2/(n)
    #ext_inputs = np.sum(spv[:2,:],axis=0)
    ext_inputs = np.sum(spv[:2,:],axis=0)
    # V,s = sim_iaf(ns,dt,ext_inputs,tau,Vthr=Vthr,dV=dV)
    # spv[3,s] = 1


    # V,s = sim_iaf(ns,dt,spv[0,:],tau=tau,Vthr=Vthr,dV=dV)
    # spv[4,s] = 1


    # n_levels = 1000
    # dV = 20
    # p_leak = 0.0642
    # #ext_inputs = np.sum(spv[:2,:],axis=0)
    # V,s = sim_iaf_discrete(ns,dt,ext_inputs,dV,1,num_levels =n_levels,p_leak =p_leak)
    # s = np.array(s)
    # spv[4,s] = 1



    print('pop_rates')
    print(np.sum(spv[0,:])/(T*n))
    print(np.sum(spv[1,:])/(T*n))
    print(np.sum(spv[2,:])/(T*n))
    print('rate leaky iaf')
    print(np.sum(spv[3,:])/(T))
    print('rate discrete leaky iaf')
    print(np.sum(spv[4,:])/(T))

    # plt.figure(figsize=(8,8))
    # uA,cA = np.unique(spv[0,:],return_counts=True)
    # plt.loglog(uA[1:],cA[1:]/np.sum(cA[1:]),label='V1A')
    # plt.loglog(np.arange(n)+1,ehe_ana(ac*c1,n),label='V1A analytic')
    # uB,cB = np.unique(spv[1,:],return_counts=True)
    # plt.loglog(uB[1:],cB[1:]/np.sum(cB[1:]),label='V1B')
    # plt.loglog(np.arange(n)+1,ehe_ana(ac*c2,n),label='V1B analytic')
    # uV4,cV4 = np.unique(spv[2,:],return_counts=True)
    # plt.loglog(uV4[1:],cV4[1:]/np.sum(cV4[1:]),label='V4')
    # plt.legend()
    # plt.ylim([1/max(np.sum(cA[1:]),np.sum(cB[:])),1])
    # plt.title('avalanche statistics '+suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_avs.png',dpi=200)


    print('subsample to 1ms')
    print('this corresponds to '+str(int(1e-3/dt))+' dts')

    Ts = len(ks)*dt
    bins = int(Ts/1e-3)

    flick_dur = int(1e-3/dt)*dt
    Bend = int(len(ks)/(flick_dur/dt))#int(T/flick_dur)

    #fA = flicksa[::int(flick_dur/dt)]
    #fB = flicksb[::int(flick_dur/dt)]
    fA = bin_data(flicksa,bins)
    fB = bin_data(flicksb,bins)
    binnedfA = bin_data(finpA,bins)
    binnedfB = bin_data(finpB,bins)
    binnedA = bin_data(spv[0,:],bins)
    binnedB = bin_data(spv[1,:],bins)
    binnedV4 = bin_data(spv[2,:],bins)
    binnedV4_leak = bin_data(spv[3,:],bins)
    binnedV4_leak_dis = bin_data(spv[4,:],bins)

    #dt = int(flick_dur/dt)*dt
    dt = Ts/bins


    pop_act_A = spv[0,:]/n
    pop_act_B = spv[1,:]/n
    pop_act_V4 = spv[2,:]/n
    pop_act_V4_leak = spv[3,:]
    pop_act_V4_leak_dis = spv[4,:]

    print('parameter values',flush=True)
    print(suptitle,flush=True)

    print('eq rates',flush=True)
    print(pop_act_A.sum()/T,flush=True)
    print(pop_act_B.sum()/T,flush=True)
    print(pop_act_V4.sum()/T,flush=True)
    print(pop_act_V4_leak.sum()/T,flush=True)
    print(pop_act_V4_leak_dis.sum()/T,flush=True)

    print('corr coefs pop_act A,V4 and B,V4')
    ccAV4=np.corrcoef(pop_act_A,pop_act_V4)[0,1]
    ccBV4=np.corrcoef(pop_act_B,pop_act_V4)[0,1]
    print(ccAV4,flush=True)
    print(ccBV4,flush=True)

    # print('corr coefs pop_act A,V4 and B,V4 leak')
    # ccAV4_leak=np.corrcoef(pop_act_A,pop_act_V4_leak)[0,1]
    # ccBV4_leak=np.corrcoef(pop_act_B,pop_act_V4_leak)[0,1]
    # print(ccAV4_leak,flush=True)
    # print(ccBV4_leak,flush=True)


    # print('corr coefs pop_act A,V4 and B,V4 leak_dis')
    # ccAV4_leak_dis=np.corrcoef(pop_act_A,pop_act_V4_leak_dis)[0,1]
    # ccBV4_leak_dis=np.corrcoef(pop_act_B,pop_act_V4_leak_dis)[0,1]
    # print(ccAV4_leak_dis,flush=True)
    # print(ccBV4_leak_dis,flush=True)



    print('corr coefs binned pop_act A,V4 and B,V4')
    ccbAV4 = np.corrcoef(binnedA,binnedV4)[0,1]
    ccbBV4 = np.corrcoef(binnedB,binnedV4)[0,1]
    print(ccbAV4,flush=True)
    print(ccbBV4,flush=True)

    # print('corr coefs binned pop_act A,V4 and B,V4 leak')
    # ccbAV4_leak = np.corrcoef(binnedA,binnedV4_leak)[0,1]
    # ccbBV4_leak = np.corrcoef(binnedB,binnedV4_leak)[0,1]
    # print(ccbAV4_leak,flush=True)
    # print(ccbBV4_leak,flush=True)


    # print('corr coefs binned pop_act A,V4 and B,V4 leak_dis')
    # ccbAV4_leak_dis = np.corrcoef(binnedA,binnedV4_leak_dis)[0,1]
    # ccbBV4_leak_dis = np.corrcoef(binnedB,binnedV4_leak_dis)[0,1]
    # print(ccbAV4_leak_dis,flush=True)
    # print(ccbBV4_leak_dis,flush=True)



    print('corr coef flicker input A,V4 B,V4')
    ccfinpAV4 = np.corrcoef(finpA,pop_act_V4)[0,1]
    ccfinpBV4 = np.corrcoef(finpB,pop_act_V4)[0,1]
    print(ccfinpAV4,flush=True)
    print(ccfinpBV4,flush=True)

    # print('corr coef flicker input A,V4 B,V4 leak')
    # ccfinpAV4_leak = np.corrcoef(finpA,pop_act_V4_leak)[0,1]
    # ccfinpBV4_leak = np.corrcoef(finpB,pop_act_V4_leak)[0,1]
    # print(ccfinpAV4_leak,flush=True)
    # print(ccfinpBV4_leak,flush=True)


    # print('corr coef flicker input A,V4 B,V4 leak_dis')
    # ccfinpAV4_leak_dis = np.corrcoef(finpA,pop_act_V4_leak_dis)[0,1]
    # ccfinpBV4_leak_dis = np.corrcoef(finpB,pop_act_V4_leak_dis)[0,1]
    # print(ccfinpAV4_leak_dis,flush=True)
    # print(ccfinpBV4_leak_dis,flush=True)


    print('corr coef flicker binned A,V4 B,V4')
    ccbfAV4 =np.corrcoef(binnedfA,binnedV4)[0,1]
    ccbfBV4 = np.corrcoef(binnedfB,binnedV4)[0,1]
    print(ccbfAV4,flush=True)
    print(ccbfBV4,flush=True)

    # print('corr coef flicker binned A,V4 B,V4 leak')
    # ccbfAV4_leak =np.corrcoef(binnedfA,binnedV4_leak)[0,1]
    # ccbfBV4_leak = np.corrcoef(binnedfB,binnedV4_leak)[0,1]
    # print(ccbfAV4_leak,flush=True)
    # print(ccbfBV4_leak,flush=True)


    # print('corr coef flicker binned A,V4 B,V4 leak_dis')
    # ccbfAV4_leak_dis =np.corrcoef(binnedfA,binnedV4_leak_dis)[0,1]
    # ccbfBV4_leak_dis = np.corrcoef(binnedfB,binnedV4_leak_dis)[0,1]
    # print(ccbfAV4_leak_dis,flush=True)
    # print(ccbfBV4_leak_dis,flush=True)


    print('corr coef real flicker input A,V4 B,V4')
    ccfAV4 =np.corrcoef(fA,binnedV4)[0,1]
    ccfBV4 = np.corrcoef(fB,binnedV4)[0,1]
    print(ccfAV4,flush=True)
    print(ccfBV4,flush=True)


    print('corr coef real flicker input A,V1A B,V1B')
    ccfAA =np.corrcoef(fA,binnedA)[0,1]
    ccfBB = np.corrcoef(fB,binnedB)[0,1]
    print(ccfAA,flush=True)
    print(ccfBB,flush=True)


    # print('corr coef real flicker input A,V4 B,V4 leak')
    # ccfAV4_leak =np.corrcoef(fA,binnedV4_leak)[0,1]
    # ccfBV4_leak = np.corrcoef(fB,binnedV4_leak)[0,1]
    # print(ccfAV4_leak,flush=True)
    # print(ccfBV4_leak,flush=True)


    # print('corr coef real flicker input A,V4 B,V4 leak_dis')
    # ccfAV4_leak_dis =np.corrcoef(fA,binnedV4_leak_dis)[0,1]
    # ccfBV4_leak_dis = np.corrcoef(fB,binnedV4_leak_dis)[0,1]
    # print(ccfAV4_leak_dis,flush=True)
    # print(ccfBV4_leak_dis,flush=True)


    tau_k = 15e-3
    t_k = np.arange(0,4*tau_k,dt)
    Kexp = 1/tau_k*np.exp(-t_k/tau_k);
    lfp = [None,None,None,None]

    freqs= np.logspace(np.log10(5),np.log10(200),100)
    #f0 = 4.84
    #freqs = f0*1.2**np.arange(20)
    num_pops = 3
    lfp[0] = np.convolve(np.array(binnedA)/N,Kexp,mode='same')
    lfp[1] = np.convolve(np.array(binnedB)/N,Kexp,mode='same')
    lfp[2] = np.convolve(np.array(binnedV4)/N,Kexp,mode='same')
    lfp[3] = np.convolve(np.array(binnedV4_leak),Kexp,mode='same')
       #lfp[p] = lfp[p][int(0.1*len(lfp[p])):int(0.9*len(lfp[p]))]


    subsample_fact = 1

    # cwt_V4_leak,_ = wavelet_morlet(lfp[3][::subsample_fact],subsample_fact*dt,freqs)
    cwt_V4,_ = wavelet_morlet(lfp[2][::subsample_fact],subsample_fact*dt,freqs)
    cwt_A,_ =  wavelet_morlet(lfp[0][::subsample_fact],subsample_fact*dt,freqs)
    cwt_B,_ =  wavelet_morlet(lfp[1][::subsample_fact],subsample_fact*dt,freqs)

    cwt_flickA,_ = wavelet_morlet(np.array(binnedfA[::subsample_fact]),subsample_fact*dt,freqs)
    cwt_flickB,coi = wavelet_morlet(np.array(binnedfB[::subsample_fact]),subsample_fact*dt,freqs)

    c4A,taus = spectral_coherence(np.array([cwt_A]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    c4B,taus = spectral_coherence(np.array([cwt_B]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)

    # c4A_leak,taus = spectral_coherence(np.array([cwt_A]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    # c4B_leak,taus = spectral_coherence(np.array([cwt_B]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)


    c4flickA,taus = spectral_coherence(np.array([cwt_flickA]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    c4flickB,taus = spectral_coherence(np.array([cwt_flickB]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)

    # c4flickA_leak,taus = spectral_coherence(np.array([cwt_flickA]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    # c4flickB_leak,taus = spectral_coherence(np.array([cwt_flickB]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)

    cwt_fA,_ = wavelet_morlet(np.array(fA[::subsample_fact]),subsample_fact*dt,freqs)
    cwt_fB,coi = wavelet_morlet(np.array(fB[::subsample_fact]),subsample_fact*dt,freqs)

    c4fA,taus = spectral_coherence(np.array([cwt_fA]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    c4fB,taus = spectral_coherence(np.array([cwt_fB]),np.array([cwt_V4]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)

    # c4fA_leak,taus = spectral_coherence(np.array([cwt_fA]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    # c4fB_leak,taus = spectral_coherence(np.array([cwt_fB]),np.array([cwt_V4_leak]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)


    cfAA,taus = spectral_coherence(np.array([cwt_fA]),np.array([cwt_A]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    cfBB,taus = spectral_coherence(np.array([cwt_fB]),np.array([cwt_B]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)

    cflickAA,taus = spectral_coherence(np.array([cwt_flickA]),np.array([cwt_A]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)
    cflickBB,taus = spectral_coherence(np.array([cwt_flickB]),np.array([cwt_B]),tau_max = 0.2,ntau=4,dt=subsample_fact*dt)



    # plt.figure()
    # plt.semilogy(freqs,np.mean(np.abs(cwt_A),axis=1),label='A')
    # plt.semilogy(freqs,np.mean(np.abs(cwt_B),axis=1),label='B')
    # plt.semilogy(freqs,np.mean(np.abs(cwt_V4),axis=1),label='C')
    # #plt.semilogy(freqs,np.mean(np.abs(cwt_V4_leak),axis=1),label='C_leak')
    # plt.title('mean abs wavelet coefficients')
    # plt.legend()
    # plt.xlabel('freq [Hz]')
    # if fname is not None:
    #     plt.savefig(fname+'_figure_wavelet_spect.png',dpi=200)


    # sc_plot(c4A,c4B,taus,freqs,titleA='sc V1a V4',titleB='sc V1b V4',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scV1V4.png',dpi=200)
    print('sum of sc values V1aV4 vs V1bV4') 
    print(np.sum(c4A),flush=True)
    print(np.sum(c4B),flush=True)

    # sc_plot(c4A_leak,c4B_leak,taus,freqs,titleA='sc V1a V4',titleB='sc V1b V4',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scV1V4_leak.png',dpi=200)
    # print('sum of sc values V1aV4 vs V1bV4') 
    # print(np.sum(c4A_leak),flush=True)
    # print(np.sum(c4B_leak),flush=True)


    # sc_plot(c4flickA,c4flickB,taus,freqs,titleA='sc flick inp A V4 (1ms)',titleB='sc flick inp B V4 (1ms)',suptitle=suptitle )
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflickinpV4.png',dpi=200)
    # print('sum of sc values flicker a,V4 vs flicker b,V4') 
    # print(np.sum(c4flickA),flush=True)
    # print(np.sum(c4flickB),flush=True)


    # sc_plot(c4flickA_leak,c4flickB_leak,taus,freqs,titleA='sc flick inp A V4 (1ms)',titleB='sc flick inp B V4 (1ms)',suptitle=suptitle )
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflickinpV4_leak.png',dpi=200)
    # print('sum of sc values flicker a,V4 vs flicker b,V4') 
    # print(np.sum(c4flickA_leak),flush=True)
    # print(np.sum(c4flickB_leak),flush=True)



    # sc_plot(c4fA,c4fB,taus,freqs,titleA='sc flick A V4 (10ms)',titleB='sc flick B V4 (10ms)',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflickV4.png',dpi=200)
    print('sum of sc values flicker a,V4 vs flicker b,V4 (10ms)') 
    print(np.sum(c4fA),flush=True)
    print(np.sum(c4fB),flush=True)


    # sc_plot(c4fA_leak,c4fB_leak,taus,freqs,titleA='sc flick A V4 (10ms)',titleB='sc flick B V4 (10ms)',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflickV4_leak.png',dpi=200)
    # print('sum of sc values flicker a,V4 vs flicker b,V4 (10ms)') 
    # print(np.sum(c4fA_leak),flush=True)
    # print(np.sum(c4fB_leak),flush=True)



    # sc_plot(cfAA,cfBB,taus,freqs,titleA='sc flick A V1A (10ms)',titleB='sc flick B V1B (10ms)',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflickV1.png',dpi=200)
    print('sum of sc values flicker a,V1A vs flicker b,V1B (10ms)') 
    print(np.sum(cfAA),flush=True)
    print(np.sum(cfBB),flush=True)


    # sc_plot(cflickAA,cflickBB,taus,freqs,titleA='sc flick inp A V1A (1ms)',titleB='sc flick inp B V1B (1ms)',suptitle=suptitle)
    # if fname is not None:
    #     plt.savefig(fname+'_figure_scflicinpV1.png',dpi=200)
    # print('sum of sc values flicker a,V1A vs flicker b,V1b (1ms)') 
    # print(np.sum(cflickAA),flush=True)
    # print(np.sum(cflickBB),flush=True)



    # TODO noch einmal anstellen und auch
    # die nicht gebinnten correlation alle ausgeben

    res = {}
    res['spectral coherences'] = [np.sum(x,axis=-1) for x in [c4A,c4B,c4flickA,c4flickB,c4fA,c4fB,cfAA,cfBB,cflickAA,cflickBB]]
    res['spectral coherences tau=0'] = [x[:,4] for x in [c4A,c4B,c4flickA,c4flickB,c4fA,c4fB,cfAA,cfBB,cflickAA,cflickBB]]
    res['correlations'] = [ccAV4,ccBV4,ccbAV4,ccbBV4,ccfinpAV4,ccfinpBV4,ccbfAV4,ccbfBV4,ccfAV4,ccfBV4,ccfAA,ccfBB]
    res['pop_acts'] = [pop_act_A.sum()/T,pop_act_B.sum()/T,pop_act_V4.sum()/T,pop_act_V4_leak.sum()/T,pop_act_V4_leak_dis.sum()/T]
    res['param_names'] = ['c1','c2','rate1','rate2','c3','cup','T','n','dtcorr','c','method']
    res['param_values'] = [c1,c2,rate1,rate2,c3,cup,T,n,dtcorr,c,method]
    #res['avalanches'] = [uA,cA,uB,cB,uV4,cV4]



    # import shelve
    # my_shelf = shelve.open('./kadabuum_workspace_1000_all.out')
    # for key in dir():
    #     try:
    #         my_shelf[key] = globals()[key]
    #     except:
    #         #
    #         # __builtins__, my_shelf, and imported modules can not be shelved.
    #         #
    #         print('ERROR shelving: {0}'.format(key))

    # my_shelf.close()        



    #to load back in
    #my_shelf = shelve.open('kadabuum_workspace.out')

  #+end_src 


  
  Analysis
     #+begin_src python :session rbs :results raw drawer 
       import pickle
       plt.rc('text', usetex=False)
       plt.rc('font', family='serif')
       plot_levels=[np.log2(0.25),np.log2(0.5),np.log2(2),np.log2(4)]
       plot_colors=['black','gray','gray','black']
       plot_figsize=(3.2,2)
       ri = 0
       ar = 0
       c_start = 0
       fmax = 100
       c1s = np.linspace(0.2,1,20)
       radvs = np.array([12])
       methods = ['int','ext']
       freqs= np.logspace(np.log10(5),np.log10(200),100)
       fmi = np.searchsorted(freqs,fmax)
       #r = np.load(open('sc_collected_ehe_flicker_sync_new.npz','rb'))
       #r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5_c01.npz','rb'))
       r = np.load(open('sc_collected_ehe_flicker_embed_thresh5.npz','rb'))
       #r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5.npz','rb'))
       #copt = 55
       #copt = 58
       #plot_vline=True

       ccsc = 'cc'

       method= 'V1 V4'

       if ccsc=='sc':
           if method=='flicker V4':
               ccfA4 = r['sc4fA']
               ccfB4 = r['sc4fB']
           if method=='flicker V1':
               ccfA4 = r['scfAA']
               ccfB4 = r['scfBB']
           if method=='V1 V4':   
               ccfA4 = r['sc4A']
               ccfB4 = r['sc4B']
       if ccsc=='cc':        
           if method=='flicker V4':
               ccfA4 = r['c_ccfAV4']
               ccfB4 = r['c_ccfBV4']
           if method=='flicker V1':
               ccfA4 = r['c_ccfAA']
               ccfB4 = r['c_ccfBB']
           if method=='V1 V4':
               ccfA4 = r['c_ccAV4']
               ccfB4 = r['c_ccBV4']
               

       # ccfA4 = r['c_ccfAV4']
       # ccfB4 = r['c_ccfBV4']
       # ccfA4 = r['c_ccfAA']
       # ccfB4 = r['c_ccfBB']
       # ccfA4 = r['c_ccAV4']
       # ccfB4 = r['c_ccBV4']


       # ccfA4 = r['scfAA']
       # ccfB4 = r['scfBB']


       # ccfA4 = r['sc4A']
       # ccfB4 = r['sc4B']





       # ccfA4 = r['ccfA4']
       # ccfB4 = r['ccfB4']
       title = method
       ccfA4 = np.nanmean(ccfA4,axis=3)
       ccfB4 = np.nanmean(ccfB4,axis=3)

       if ccsc =='sc':
           ccfA4 = np.sum(ccfA4[:,:,:,:fmi],axis=-1)
           ccfB4 = np.sum(ccfB4[:,:,:,:fmi],axis=-1)

       # r_A = r['r_A']
       # r_B = r['r_B']
       # r_A = np.nanmean(r_A,axis=3)
       # r_B = np.nanmean(r_B,axis=3)

       plt.figure(dpi=200,figsize=plot_figsize)
       plt.title(title)
       # plt.plot(c1s,ccfB4[:,ri,0],label='correlation attended')
       # plt.plot(c1s,ccfA4[:,ri,0],label='correlation non-attended')

       # plt.plot(c1s,r_B[:,ri,0],label='rate attended')
       # plt.plot(c1s,r_A[:,ri,0],label='rate non-attended')
       # plt.ylim([40,80])
       plt.plot(c1s[:-3],ccfB4[:-3,ri,0]/ccfA4[:-3,ri,0],label='correlation ratio',marker='o',color='black')
       plt.plot(c1s[:-3],r_B[:-3,ri,0]/r_A[:-3,ri,0],label='rate ratio',marker='o',color='gray',linestyle='dashed')

       plt.legend()
       plt.hlines([1.3],xmin=0.2,xmax=1,linestyle='dashed',color='black')
       plt.legend()
       #plt.ylim([1,5])
       plt.xlim([0.55,0.9])
       plt.xlabel('alpha')
       #plt.ylabel('ratio')
       plt.savefig(ccsc+'embedded_network_ratios_'+method+'.png',dpi=200)
       plt.savefig(ccsc+'embedded_network_ratios_'+method+'.svg')
       
     #+end_src 

     #+RESULTS:
     [[file:./.ob-jupyter/9367b1a3a9d92ce038d8ea9dfb763a480f00a33a.png]]


  Analysis
     #+begin_src python :session rbs :results raw drawer 
       import pickle
       plt.rc('text', usetex=False)
       plt.rc('font', family='serif')
       plot_levels=[np.log2(0.25),np.log2(0.5),np.log2(2),np.log2(4)]
       plot_colors=['black','gray','gray','black']
       plot_figsize=(3.2,2)
       ri = 0
       ar = 0
       c_start = 0
       fmax = 100
       c1s = np.linspace(0.2,1,20)
       radvs = np.array([12])
       methods = ['int','ext']
       freqs= np.logspace(np.log10(5),np.log10(200),100)
       fmi = np.searchsorted(freqs,fmax)
       #r = np.load(open('sc_collected_ehe_flicker_sync_new.npz','rb'))
       #r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5_c01.npz','rb'))
       r = np.load(open('sc_collected_ehe_flicker_embed_thresh5_adddrive.npz','rb'))
       #r = np.load(open('sc_collected_ehe_flicker_sync_new_c3_thresh5.npz','rb'))
       #copt = 55
       #copt = 58
       #plot_vline=True
       ccfA4 = r['c_ccfAV4']
       ccfB4 = r['c_ccfBV4']
       # ccfA4 = r['c_ccfAA']
       # ccfB4 = r['c_ccfBB']
       # ccfA4 = r['c_ccAV4']
       # ccfB4 = r['c_ccBV4']

       ccfA4 = r['sc4fA']
       ccfB4 = r['sc4fB']

       # ccfA4 = r['scfAA']
       # ccfB4 = r['scfBB']


       # ccfA4 = r['sc4A']
       # ccfB4 = r['sc4B']





       # ccfA4 = r['ccfA4']
       # ccfB4 = r['ccfB4']
       title = 'correlation ratios flicker V4'
       ccfA4 = np.nanmean(ccfA4,axis=3)
       ccfB4 = np.nanmean(ccfB4,axis=3)

       ccfA4 = np.sum(ccfA4[:,:,:,:fmi],axis=-1)
       ccfB4 = np.sum(ccfB4[:,:,:,:fmi],axis=-1)

       # r_A = r['r_A']
       # r_B = r['r_B']
       # r_A = np.nanmean(r_A,axis=3)
       # r_B = np.nanmean(r_B,axis=3)

       plt.figure(dpi=200,figsize=plot_figsize)
       # plt.plot(c1s,ccfB4[:,ri,0],label='correlation attended')
       # plt.plot(c1s,ccfA4[:,ri,0],label='correlation non-attended')

       # plt.plot(c1s,r_B[:,ri,0],label='rate attended')
       # plt.plot(c1s,r_A[:,ri,0],label='rate non-attended')
       # plt.ylim([40,80])
       plt.plot(c1s,ccfB4[:,ri,0]/ccfA4[:,ri,0],label='correlation ratio')
       plt.plot(c1s,r_B[:,ri,0]/r_A[:,ri,0],label='rate ratio')

       plt.legend()
       plt.hlines([1.3],xmin=0.2,xmax=1,linestyle='dashed',color='black')
       plt.legend()
       #plt.ylim([1,2])
     #+end_src 




* Visualization spike raster lfp for udo sfn
  
  #+begin_src python :session rbs :results raw drawer 
    import numpy as np
    import matplotlib.pyplot as plt
    #%matplotlib qt
    n = 100
    for c1 in [0.1,0.6,0.9,0.95,1]:#,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:
        #c1 = [0.3,0.5,0.7,0.9][3]#np.linspace(0.2,1,80)[65]
        radv = 12
        method = 'int'
        trial = 0
        #c1,radv,method = 
        rate1=40;
        cup=0.3;
        T=1;
        n2 = int(n/10);
        c=0.25;
        thresh=5; # for n=1000
        ac = 1-1/np.sqrt(n)
        flick_dur = 10e-3
        dt = flick_dur/10000

        n2 = 10#int(n/10)
        N = n#+n2
        W = np.zeros((N,N))
        W[:n,:n] = c1*ac/n
        #W[n:N,n:N] = (1-np.sqrt(n2)/n2)/n2
        #W[:n,n:N] = 0.095/n2 # hier auskommentieren f√ºr nonatt

        # aref = 0.9
        # a = c1*ac
        # deltaUref = 0.055364
        # deltaU = (n*(1-a))*deltaUref/(n*(1-aref))

        dt = 1e-4
        deltaU  = n*(1-c1*ac)*rate1*dt

        p = np.ones(N)/N
        num_steps = 22000
        avs,avt,avu,u,ks = simulate_model(N,W,p,deltaU,num_steps=num_steps)
        #plt.figure(figsize=(6,4),dpi=200)
        offset = 17000
        raster = np.zeros((N,3000))
        raster2 = np.zeros((N,3000))

        pop_act = np.zeros(num_steps)

        for i,u in zip(avt,avu):
            pop_act[i] = len(u)
            if 0 <= i-offset < 3000:
                if np.max(u)>=100:
                    raster2[u,i-offset] = 1
                else:
                    raster[u,i-offset] = 1

        plt.figure(figsize=[4.59, 4],dpi=200)
        ax1 = plt.subplot(2,1,1)
        x = plt.spy(raster[:N,:],aspect='auto',markersize=0.5,color='k')
        plt.ylabel('neuron')
        #plt.plot(raster.sum(axis=0)[1:])
        # pop_act = np.zeros(len(pop_act))
        # pop_act[offset+1000] = 1
        # pop_act[offset+2000] = 1
        # pop_act[offset+3000] = 1
        # plt.plot(np.arange(3000),pop_act[offset:offset+3000])
        #plt.hlines([100],xmin=0,xmax=3000,color='k',linewidth=0.5)
        # plt.savefig('raster_neu_att'+str(W[0,n]>0)+'.png',dpi=200)
        # plt.savefig('raster_neu_att'+str(W[0,n]>0)+'.svg')

        #plt.figure(figsize=[4.59, 2.5 ],dpi=200)        
        # plt.spy(raster2[:N,:],aspect='auto',markersize=0.5,color='red')
        # plt.hlines([100],xmin=0,xmax=3000,color='k',linewidth=0.5)
        # plt.savefig('raster2_neu_att'+str(W[0,n]>0)+'.png',dpi=200)
        # plt.savefig('raster2_neu_att'+str(W[0,n]>0)+'.svg')



        #print(np.sum(avs))
        #plt.figure(dpi=200)
        dt = 0.1e-3
        tau_k = 15e-3
        t_k = np.arange(0,4*tau_k,dt)
        Kexp = 1/tau_k*np.exp(-t_k/tau_k);
        sigma = 2.5e-3
        t_k2 = np.arange(-30e-3,30e-3,dt)
        Kexp2 = 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*(t_k2/sigma)**2)
        #t_k = np.arange
        plt.subplot(2,1,2,sharex = ax1)
        lfp = np.convolve(pop_act,Kexp2,mode='valid')

        #plt.figure(figsize=[4.59, 2.5 ],dpi=200)
        x = np.arange(len(lfp))+len(Kexp2)/2+1
        y = lfp[offset-299:offset+3000-299]*dt
        plt.plot(np.arange(3000),y)
        #plt.plot(np.arange(3000),y[-3000:])#int(offset-len(Kexp)/2+1):int(offset+3000)]#-len(Kexp)/2+1)])
        plt.xlabel('Time [0.1 ms]')
        plt.suptitle(r'$\alpha='+str(c1)+'$')
        plt.ylim([0,1.5])
        plt.ylabel('LFP')
        plt.tight_layout()
        plt.savefig('raster_lfp_alpha='+str(c1)+'.png',dpi=200)
        #plt.savefig('raster_lfp_alpha='+str(c1)+'.svg',dpi=200)


    #plt.close('all')


    # plt.figure(dpi=200)
    # plt.plot(pop_act)
    # plt.figure(dpi=200)
    # plt.plot(lfp)
  #+end_src 


  #+begin_src python :session rbs :results raw drawer 
    n = 100
    plt.figure(figsize=(4,3),dpi=200)
    for alpha,c in zip([0.1,0.6,1],[[0.6,0.6,0.6],[0.3,0.3,0.3],[0,0,0]]):
        plt.loglog(np.arange(n)+1,ehe_ana(alpha*0.9,100),label=r'$\beta='+str(alpha)+'$',color=c)
    plt.legend()
    plt.ylim([1e-5,1])
    plt.xlabel('avalanche size s')
    plt.ylabel('P(s)')
    plt.title('avalanche distributions')
    plt.tight_layout()
    plt.savefig('new_example_avs_dists.png',dpi=200)
    plt.savefig('new_example_avs_dists.svg',dpi=200)
  #+end_src 



* Plot dependencies of optimal configurations on model parameters

  #+begin_src python :session rbs :results raw drawer 
    thresholds,c1s,acs,bcs = pickle.load(open('flicker_v4_corrs_mis_c1_thresholds.pickle','rb'))
    # acs = np.array(bcorr_fA4s)
    # bcs = np.array(bcorr_fB4s)
    c1s = np.linspace(0.2,1,80)

    opt_idxs = bcs.argmax(axis=0)

    plt.figure(figsize=(5,3),dpi=200)
    plt.plot(thresholds,[bcs[opt_idx,t] for t,opt_idx in enumerate(opt_idxs)],color='black')
    plt.xlim([1,20])
    plt.xlabel(r'$\theta$')
    plt.ylabel(r'Opt. corr. $\max{\,\,C(f_A,r_C | \alpha, \theta)}$')
    plt.twinx()
    plt.plot(thresholds,c1s[bcs.argmax(axis=0)],color='gray')
    plt.ylabel(r'Opt. $\alpha=\argmax{\,\, C(f_A,r_C | \alpha, \theta)}$',color='gray')

    plt.savefig('illus_theta_opt.png',dpi=200)



    plt.figure(figsize=(5,4),dpi=200)
    plt.plot(thresholds,[bcs[opt_idx,t]/acs[opt_idx,t] for t,opt_idx in enumerate(opt_idxs)],color='black')
    plt.xlim([1,20])
    plt.xlabel('Theta')
    plt.ylabel('routing selectivity at opt. point')


    plt.figure(figsize=(5,4),dpi=200)
    plt.plot(c1s,bcs[:,2]/acs[:,2],color='black')
    plt.plot(c1s,bcs[:,2]/acs[:,5],color='black')
    plt.plot(c1s,bcs[:,2]/acs[:,10],color='black')
    plt.xlabel('alpha')
    plt.ylabel('routing selectivity')
    plt.ylim([1,3])




    # Ok this are the new plots for the second part!!!!!!!!!


    # plt.figure(figsize=(6,4),dpi=200)
    # plt.title('Effect of synchrony gain')
    # plt.plot(c1s,acs[:,2],color='blue',label='C(f_A,r_4) s=2')
    # plt.plot(c1s,bcs[:,2],color='blue',label='C(f_B,r_4) s=2')
    # plt.plot(c1s,acs[:,5],color='green',label='C(f_A,r_4) s=2')
    # plt.plot(c1s,bcs[:,5],color='green',label='C(f_B,r_4) s=2')
    # plt.plot(c1s,acs[:,10],color='red',label='C(f_A,r_4) s=2')
    # plt.plot(c1s,bcs[:,10],color='red',label='C(f_B,r_4) s=2')
    # plt.legend()
    # plt.xlabel('c1')
    # plt.ylabel('correlation')
    # #plt.savefig('img/ana_synchrony_gain_2510.png',dpi=200)
    # #plt.savefig('img/ana_synchrony_gain_2510.svg')

    # mi = min(acs.min(),bcs.min())
    # ma = max(acs.max(),bcs.max())
    # plt.figure(dpi=200)
    # plt.subplot(2,1,1)
    # plt.title('correlation attended flicker and V4')
    # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,bcs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
    # plt.plot(np.linspace(0.2,1,80)[bcs.argmax(axis=0)],thresholds,'k--')
    # plt.colorbar()
    # plt.xlabel('coupling strength alpha')
    # plt.ylabel('threshold theta')
    # plt.ylim([1,20])
    # plt.subplot(2,1,2)

    # plt.title('correlation non-attended flicker and V4')
    # plt.pcolormesh(np.linspace(0.2,1,80),thresholds,acs.T,shading='nearest',linewidth=2,vmin=mi,vmax=ma,cmap=cmap_old)
    # plt.plot(np.linspace(0.2,1,80)[acs.argmax(axis=0)],thresholds,'k--')
    # plt.colorbar()
    # plt.xlabel('coupling strength alpha')
    # plt.ylabel('threshold theta')
    # plt.ylim([1,20])
    # plt.tight_layout()

  #+end_src 

  #+RESULTS:
  :RESULTS:
  : Text(0, 0.5, 'routing selectivity at opt. point')
  [[file:./.ob-jupyter/24414db54815beafd086e8f1270eb305c38b7702.png]]
  [[file:./.ob-jupyter/f9e2f3ce61bc4f445b4447646659483e7e557ff1.png]]
  :END:
